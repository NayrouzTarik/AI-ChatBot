{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN   EN \n",
      " \n",
      " \n",
      " EUROPEAN  \n",
      "COMMISSION   \n",
      "Brussels, 21.4.2021  \n",
      "COM(2021) 206 final  \n",
      "2021/0106 (COD)  \n",
      " \n",
      "Proposal for a  \n",
      "REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL  \n",
      "LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE \n",
      "(ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION \n",
      "LEGISLATIVE ACTS  \n",
      "{SEC(2021)  167 final}  - {SWD(2021)  84 final}  - {SWD(2021)  85 final}   EN 1  EN EXPLANATORY MEMORANDUM  \n",
      "1. CONTEXT  OF THE  PROPOSAL  \n",
      "1.1. Reasons for and objectives of the proposal  \n",
      "This explanatory memorandum accompanies the proposal for a Regulation laying down \n",
      "harmonised rules on artificial intelligence (Artificial Intelligence Act). Artificial Intelligence \n",
      "(AI) is a fast evolving family of technologies that can bring a wide array of economic and \n",
      "societal benefits across the entire s pectrum of industries and social activities. By improving \n",
      "prediction, optimising operations and resource allocation, and personalising service delivery, \n",
      "the use of artificial intelligence can support socially and environmentally beneficial outcomes \n",
      "and pro vide key competitive advantages to companies and the European economy. Such \n",
      "action is especially needed in high -impact sectors, including climate change, environment and \n",
      "health, the public sector, finance, mobility, home affairs and agriculture. However, t he same \n",
      "elements and techniques that power the socio -economic benefits of AI can also bring about \n",
      "new risks or negative consequences for individuals or the society. In light of the speed of \n",
      "technological change and possible challenges, the EU is committed to strive for a balanced \n",
      "approach.  It is in the Union interest to preserve the EU’s technological leadership and to \n",
      "ensure that Europeans can benefit from new technologies developed and functioning \n",
      "according to Union values, fundamental rights and principl es. \n",
      "This proposal delivers on the political commitment by President von der Leyen, who \n",
      "announced in her political guidelines for the 2019 -2024 Commission “A Union that strives for \n",
      "more”1, that the Commission would put forward legislation for a coordinated European \n",
      "approach on the human and ethical implications of AI. Following on that announcement, on \n",
      "19 February 2020 the Commission published the White Paper on AI - A European approach \n",
      "to excellence and trust2. The White Paper sets out policy options on how  to achieve the twin \n",
      "objective of promoting the uptake of AI and of addressing the risks associated with certain \n",
      "uses of such technology. This proposal aims to implement the second objective for the \n",
      "development of an ecosystem of trust by proposing a legal  framework for trustworthy AI. The \n",
      "proposal is based on EU values and fundamental rights and aims to give people and other \n",
      "users the confidence to embrace AI -based solutions, while encouraging businesses to develop \n",
      "them. AI should be a tool for people and be a force for good in society with the ultimate aim \n",
      "of increasing human well -being. Rules for AI available in the Union market or otherwise \n",
      "affecting people in the Union should therefore be human centric, so that people can trust that \n",
      "the technology is us ed in a way that is safe and compliant with the law, including the respect \n",
      "of fundamental rights. Following the publication of the White Paper, the Commission \n",
      "launched a broad stakeholder consultation,  which was met with a great interest by a large \n",
      "number of stakeholders who were largely supportive of regulatory intervention to address the \n",
      "challenges and concerns raised by the increasing use of AI.  \n",
      "The proposal also responds to explicit requests from the European Parliament (EP) and the \n",
      "European  Council, w hich have repeatedly expressed calls for legislative action to ensure a \n",
      "well-functioning internal market for artificial intelligence systems (‘AI systems’) where both \n",
      "benefits and risks of AI are adequately addressed at Union level.  It supports the objecti ve of \n",
      "the Union being a global leader in the development of secure, trustworthy and ethical artificial \n",
      "                                                 \n",
      "1 https://ec.europa.eu/commission/sites/beta -political/files/political -guidelines -next-commission_en.pdf  \n",
      "2 European Commission, White Paper on Artificial Intelligence - A European approach to excellence and \n",
      "trust, COM(2020) 65 final, 2020.  EN 2  EN intelligence as stated by the European Council3 and ensures the protection of ethical principles \n",
      "as specifically requested by the European Parliament4.  \n",
      "In 2017, the European Council called for a ‘sense of urgency to address emerging trends’ \n",
      "including ‘issues such as artificial intelligence …, while at the same time ensuring a high \n",
      "level of data protection, digital rights and ethical standards’5. In its 20 19 Conclusions on the \n",
      "Coordinated Plan on the development and use of artificial intelligence Made in Europe6, the \n",
      "Council further highlighted the importance of ensuring that European citizens’ rights are fully \n",
      "respected and called for a review of the exist ing relevant legislation to make it fit for purpose \n",
      "for the new opportunities and challenges raised by AI. The European Council has also called \n",
      "for a clear determination of the AI applications that should be considered high -risk7.  \n",
      "The most recent Conclusi ons from 21 October 2020  further called for addressing the opacity, \n",
      "complexity, bias, a certain degree of unpredictability and partially autonomous behaviour of \n",
      "certain AI systems, to ensure their compatibility with fundamental rights and to facilitate the  \n",
      "enforcement of legal rules8. \n",
      "The European Parliament has also undertaken a considerable amount of work in the area of \n",
      "AI. In October 2020, it adopted a number of resolutions related to AI, including on ethics9, \n",
      "liability10 and copyright11. In 2021, those were followed by resolutions on AI in criminal \n",
      "matters12 and in education, culture and the audio -visual sector13. The EP Resolution on a \n",
      "Framework of Ethical Aspects of Artificial Intelligence, Robotics and Related Technologies \n",
      "specifically recommends to the  Commission to propose legislative action to harness the \n",
      "opportunities and benefits of AI, but also to ensure protection of ethical principles. The \n",
      "resolution includes a text of the legislative proposal for a regulation on ethical principles for \n",
      "the develo pment, deployment and use of AI, robotics and related technologies. In accordance \n",
      "with the political commitment made by President von der Leyen in her Political Guidelines as \n",
      "regards resolutions adopted by the European Parliament under Article 225 TFEU, th is \n",
      "                                                 \n",
      "3 European Council, Special meeting of the European Council (1 and 2 October 2020) – Conclusions , \n",
      "EUCO 13/20, 2020, p. 6.  \n",
      "4 European Parliament resolution of 20 Oct ober 2020 with recommendations to the Commission on a \n",
      "framework of ethical aspects of artificial intelligence, robotics and related technologies, \n",
      "2020/2012(INL).  \n",
      "5 European Council, European Council meeting (19 October 2017) – Conclusion  EUCO 14/17, 2017, p. \n",
      "8. \n",
      "6 Council of the European Union, Artificial intelligence b) Conclusion s on the coordinated plan on \n",
      "artificial intelligence -Adoption  6177/19, 2019.  \n",
      "7 European Council, Special meeting of the European Council (1and 2 October 2020) – Conclusions  \n",
      "EUCO 13/20, 2020.  \n",
      "8 Council of the European Union, Presidency conclusions - The Charter of Fundamental Rights in the \n",
      "context of Artificial Intelligence and Digital Chan ge, 11481/20, 2020.  \n",
      "9 European Parliament resolution of 20 October 2020 on a framework of ethical aspects of artificial \n",
      "intelligence, robotics and related technologies, 2020/2012(INL) . \n",
      "10 European Parliament resolution of 20 October 2020 on a civil liability regime for artificial intelligence, \n",
      "2020/2014(INL).  \n",
      "11 European Parliament resolution of 20 October 2020 on intellectual property rights for the development \n",
      "of artificial intelligence technologies, 2020/2015(INI).  \n",
      "12 European  Parliament Draft Report, Artificial intelligence in criminal law and its use by the police and \n",
      "judicial authorities in criminal matters,  2020/2016(INI) .  \n",
      "13 European Parliament Draft Report, Artificial intelligence in education, culture and the audiovisual \n",
      "sector, 2020/2017(INI) . In that regard, the Commission has adopted the Digital Education Action Plan \n",
      "2021 -2027: Resetting education and training for the digital age, which foresees the development of \n",
      "ethical guidelines in AI and Data usage in education – Commission Communicatio n COM(2020) 624 \n",
      "final.  EN 3  EN proposal takes into account the aforementioned resolution of the European Parliament in full \n",
      "respect of proportionality, subsidiarity and better law making principles.  \n",
      "Against this political context, the Commission puts forward the proposed regulatory \n",
      "framework on Artificial Intelligence with the following specific objectives : \n",
      " ensure that AI systems placed on the Union market and used are safe and respect \n",
      "existing law on fundamental rights and Union values;  \n",
      " ensure legal certainty to facilitate investmen t and innovation in AI;  \n",
      " enhance governance and effective enforcement of existing law on fundamental \n",
      "rights and safety requirements applicable to AI systems;  \n",
      " facilitate the development of a single market for lawful, safe and trustworthy AI \n",
      "applications and prevent market fragmentation.  \n",
      "To achieve those objectives, this proposal presents a balanced and proportionate horizontal \n",
      "regulatory approach to AI that is limited to the minimum necessary requirements to address \n",
      "the risks and problems linked to AI, withou t unduly constraining or hindering technological \n",
      "development or otherwise disproportionately increasing the cost of placing AI solutions on \n",
      "the market.  The proposal sets a robust and flexible legal framework. On the one hand, it is \n",
      "comprehensive and future -proof in its fundamental regulatory choices, including the \n",
      "principle -based requirements that AI systems should comply with. On the other hand, it puts \n",
      "in place a proportionate regulatory system centred on a well -defined risk -based regulatory \n",
      "approach that  does not create unnecessary restrictions to trade, whereby legal intervention is \n",
      "tailored to those concrete situations where there is a justified cause for concern or where such \n",
      "concern can reasonably be anticipated in the near future. At the same time, t he legal \n",
      "framework includes flexible mechanisms that enable it to be dynamically adapted as the \n",
      "technology evolves and new concerning situations emerge.  \n",
      "The proposal sets harmonised rules for the development, placement on the market and use of \n",
      "AI systems i n the Union following a proportionate risk -based approach. It proposes a single \n",
      "future -proof definition of AI. Certain particularly harmful AI practices are prohibited as \n",
      "contravening Union values, while specific restrictions and safeguards are proposed in  relation \n",
      "to certain uses of remote biometric identification systems for the purpose of law enforcement. \n",
      "The proposal lays down a solid risk methodology to define “high -risk” AI systems that pose \n",
      "significant risks to the health and safety or fundamental ri ghts of persons. Those AI systems \n",
      "will have to comply with a set of horizontal mandatory requirements for trustworthy AI and \n",
      "follow conformity assessment procedures before those systems can be placed on the Union \n",
      "market. Predictable, proportionate and clea r obligations are also placed on providers and users \n",
      "of those systems to ensure safety and respect of existing legislation protecting fundamental \n",
      "rights throughout the whole AI systems’ lifecycle. For some specific AI systems, only \n",
      "minimum transparency obl igations are proposed, in particular when chatbots or ‘deep fakes’ \n",
      "are used.  \n",
      "The proposed rules will be enforced through a governance system at Member States level, \n",
      "building on already existing structures, and a cooperation mechanism at Union level with the \n",
      "establishment of a European Artificial Intelligence Board . Additional measu res are also \n",
      "proposed to support innovation, in particular through AI regulatory sandboxes and other \n",
      "measures to reduce the regulatory burden and to support Small and Medium -Sized Enterprises \n",
      "(‘SMEs’) and start -ups. EN 4  EN 1.2. Consistency with existing policy pr ovisions in the policy area  \n",
      "The horizontal nature of the proposal requires full consistency with existing Union legislation \n",
      "applicable to sectors where high -risk AI systems are already used or likely to be used in the \n",
      "near future.  \n",
      "Consistency is also ensu red with the EU Charter of Fundamental Rights and the existing \n",
      "secondary Union legislation on data protection, consumer protection, non -discrimination and \n",
      "gender equality. The proposal is without prejudice and complements the General Data \n",
      "Protection Regula tion (Regulation (EU) 2016/679) and the Law Enforcement Directive \n",
      "(Directive (EU) 2016/680) with a set of harmonised rules applicable to the design, \n",
      "development and use of certain high -risk AI systems and restrictions on certain uses of remote \n",
      "biometric id entification systems. Furthermore, the proposal complements existing Union law \n",
      "on non -discrimination with specific requirements that aim to minimise the risk of algorithmic \n",
      "discrimination, in particular in relation to the design and the quality of data set s used for the \n",
      "development of AI systems complemented with obligations for testing, risk management, \n",
      "documentation and human oversight throughout the AI systems’ lifecycle. The proposal is \n",
      "without prejudice to the application of Union competition law.  \n",
      "As regards high-risk AI systems which are safety components of products, this proposal will \n",
      "be integrated into the existing sectoral safety legislation to ensure consistency, avoid \n",
      "duplications and minimise additional burdens. In particular, as regards  high-risk AI systems \n",
      "related to products covered by the New Legislative Framework (NLF) legislation (e.g. \n",
      "machinery, medical devices, toys), the requirements for AI systems set out in this proposal \n",
      "will be checked as part of the existing conformity assessment pro cedures under the relevant \n",
      "NLF legislation. With regard to the interplay of requirements, while the safety risks specific \n",
      "to AI systems are meant to be covered by the requirements of this proposal, NLF legislation \n",
      "aims at ensuring the overall safety of the  final product and therefore may contain specific \n",
      "requirements regarding the safe integration of an AI system into the final product. The \n",
      "proposal for a Machinery Regulation, which is adopted on the same day as this proposal fully \n",
      "reflects this approach. As regards high-risk AI systems related to products  covered by relevant \n",
      "Old Approach legislation (e.g. aviation, cars), this proposal would not directly apply. \n",
      "However, the ex -ante essential requirements for high -risk AI systems set out in this proposal \n",
      "will have to be taken into account when adopting relevant implementing or delegated \n",
      "legislation under those acts.  \n",
      "As regards AI systems provided or used by regulated credit institutions , the authorities \n",
      "responsible for the supervision of the Union’s financial  services legislation should be \n",
      "designated as competent authorities for supervising the requirements in this proposal to ensure \n",
      "a coherent enforcement of the obligations under this proposal and the Union’s financial \n",
      "services legislation where AI systems ar e to some extent implicitly regulated in relation to the \n",
      "internal governance system of credit institutions . To further enhance consistency, the \n",
      "conformity assessment procedure and some of the providers’ procedural obligations under this \n",
      "proposal are integr ated into the procedures under Directive 2013/36/EU on access to the \n",
      "activity of credit institutions and the prudential supervision14.  \n",
      "                                                 \n",
      "14 Directive 2013/36/EU of the European Parliament and of the Council of 26 June 2013 on access to the \n",
      "activity of credit institutions and the prudential supervision of credit institutions and investment firms, \n",
      "amending Directive 2002 /87/EC and repealing Directives 2006/48/EC and 2006/49/EC Text with EEA \n",
      "relevance, OJ L 176, 27.6.2013, p. 338 –436. EN 5  EN This proposal is also consistent with the applicable Union legislation on services, including on \n",
      "intermediary services re gulated by the e -Commerce Directive 2000/31/EC15 and the \n",
      "Commission’s recent proposal for the Digital Services Act (DSA)16. \n",
      "In relation to AI systems that are components of large -scale IT systems in the Area of \n",
      "Freedom, Security and Justice managed by the European Union Agency for the Operational \n",
      "Management of Large -Scale IT Systems (eu -LISA), the proposal will not apply to tho se AI \n",
      "systems that have been placed on the market or put into service before one year has elapsed \n",
      "from the date of application of this Regulation,  unless the replacement or amendment of those \n",
      "legal acts leads to a significant change in the design or intend ed purpose of the AI system or \n",
      "AI systems concerned . \n",
      "1.3. Consistency with other Union policies  \n",
      "The proposal is part of a wider comprehensive package of measures that address problems \n",
      "posed by the development and use of AI, as examined in the White Paper o n AI. Consistency \n",
      "and complementarity is therefore ensured with other ongoing or planned initiatives of the \n",
      "Commission that also aim to address those problems, including the revision of sectoral \n",
      "product legislation (e.g. the Machinery Directive, the Genera l Product Safety Directive) and \n",
      "initiatives that address liability issues related to new technologies, including AI systems. \n",
      "Those initiatives will build on and complement this proposal in order to bring legal clarity and \n",
      "foster the development of an ecosy stem of trust in AI in Europe.  \n",
      "The proposal is also coherent with the Commission’s overall digital strategy in its \n",
      "contribution to promoting technology that works for people, one of the three main pillars of \n",
      "the policy orientation and objectives announced  in the Communication ‘Shaping Europe's \n",
      "digital future’17. It lays down a coherent, effective and proportionate framework to ensure AI \n",
      "is developed in ways that respect people’s rights and earn their trust, making Europe fit for the \n",
      "digital age and turning the next ten years into the Digital Decade18. \n",
      "Furthermore, the promotion of AI -driven innovation is closely linked to the Data \n",
      "Governance Act19, the Open Data Directive20 and other initiatives under the EU strategy \n",
      "for data21, which will establish trusted mech anisms and services for the re -use, sharing and \n",
      "pooling of data that are essential for the development of data -driven AI models of high \n",
      "quality.  \n",
      "The proposal also strengthens significantly the Union’s role to help shape global norms and \n",
      "standards and promo te trustworthy AI that is consistent with Union values and interests. It \n",
      "provides the Union with a powerful basis to engage further with its external partners, \n",
      "including third countries, and at international fora on issues relating to AI.  \n",
      "                                                 \n",
      "15 Directive 2000/31/EC of the European Parliament and of the Council of 8 June 2000 on certain legal \n",
      "aspects of information society services , in particular electronic commerce, in the Internal Market \n",
      "('Directive on electronic commerce'), OJ L 178, 17.7.2000, p. 1 –16. \n",
      "16 See Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL \n",
      "on a Single Market For Digital Services (Digital S ervices Act) and amending Directive 2000/31/EC \n",
      "COM/2020/825 final.  \n",
      "17 Communication from the Commission, Shaping Europe's Digital Future, COM/2020/67 final.  \n",
      "18 2030 Digital Compass: the European way for the Digital Decade . \n",
      "19 Proposal for a Regulation on European data governance (Data Governance Act) COM/2020/767 . \n",
      "20 Directive (EU) 2019/1024 of the European Parliament and of the Council of 20 June 2019 on open data \n",
      "and the re -use of public sector information, PE/28/2019/REV/1, OJ L 172, 26.6.2019, p. 56 –83. \n",
      "21 Commission Communication, A  European strategy for data COM/2020/66 final.  \n",
      " EN 6  EN 2. LEGAL  BASIS,  SUBSIDIARITY  AND  PROPORTIONALITY  \n",
      "2.1. Legal  basis  \n",
      "The legal basis for the proposal is in the first place Article 114 of the Treaty on the \n",
      "Functioning of the European Union (TFEU), which provides for the adoption of measures to \n",
      "ensure the establishment and f unctioning of the internal market.  \n",
      "This proposal constitutes a core part of the EU digital single market strategy. The primary \n",
      "objective of this proposal is to ensure the proper functioning of the internal market by setting \n",
      "harmonised rules in particular on the development, placing on the Union market and the use \n",
      "of products and services making use of AI technologies or provided as stand -alone AI \n",
      "systems. Some Member States are already considering national rules to ensure that AI is safe \n",
      "and is developed a nd used in compliance with fundamental rights obligations. This will likely \n",
      "lead to two main problems: i) a fragmentation of the internal market on essential elements \n",
      "regarding in particular the requirements for the AI products and services , their marketin g, \n",
      "their use, the liability  and the supervision  by public authorities , and ii) the substantial \n",
      "diminishment of legal certainty for both providers and users of AI systems on how existing \n",
      "and new rules will apply to those systems in the Union. Given the wide  circulation of products \n",
      "and services across borders, these two problems can be best solved through EU harmonizing \n",
      "legislation.  \n",
      "Indeed, the proposal defines common mandatory requirements applicable to the design and \n",
      "development of certain AI systems befor e they are placed on the market that will be further \n",
      "operationalised through harmonised technical standards. The proposal also addresses the \n",
      "situation after AI systems have been placed on the market by harmonising the way in which \n",
      "ex-post controls are cond ucted.  \n",
      "In addition, considering that this proposal contains certain specific rules  on the protection of \n",
      "individuals with regard to the processing of personal data,  notably restrictions of the use of AI \n",
      "systems for ‘real -time’ remote biometric identification in publicly accessible spaces for the \n",
      "purpose of law enforcement, it is appropriate to base this regulation, in as far as those specific \n",
      "rules are concerned, on Article 16 of the TFEU.  \n",
      "2.2. Subsidiarity  (for non-exclusive  competence)   \n",
      "The nature of AI, which often relies on large and varied datasets and which may be embedded \n",
      "in any product or service circulating freely within the internal market, entails that the \n",
      "objectives of this proposal cannot be effectively achieved by Member States alone. \n",
      "Furthermore, a n emerging patchwork of potentially divergent national rules will hamper the \n",
      "seamless circulation of products and services related to  AI systems across the EU and will be \n",
      "ineffective in ensuring the safety and protection of fundamental rights and Union values \n",
      "across the different Member States. National approaches in addressing the problems will only \n",
      "create additional legal uncertainty  and barriers, and will slo w market uptake of AI.   \n",
      "The objectives of this proposal can be better achieved at Union level to avoid a further \n",
      "fragmentation of the Single Market into potentially contradictory national frameworks \n",
      "preventing the free circulation of goods and services emb edding AI. A solid European \n",
      "regulatory framework for trustworthy AI will also ensure a level playing field and protect all \n",
      "people, while strengthening Europe’s competitiveness and industrial basis in AI. Only \n",
      "common action at Union level can also protect t he Union’s digital sovereignty and leverage \n",
      "its tools and regulatory powers to shape global rules and standards.   EN 7  EN 2.3. Proportionality  \n",
      "The proposal builds on existing legal frameworks and is proportionate and necessary to \n",
      "achieve its objectives, since it f ollows a risk -based approach and imposes regulatory burdens \n",
      "only when an AI system is likely to pose high risks to fundamental rights and safety. For \n",
      "other, non -high-risk AI systems, only very limited transparency obligations are imposed, for \n",
      "example in te rms of the provision of information to flag the use of an AI system when \n",
      "interacting with humans. For high -risk AI systems, the requirements of high quality data, \n",
      "documentation and traceability, transparency, human oversight, accuracy and robustness, are \n",
      "strictly necessary to mitigate the risks to fundamental rights and safety posed by AI and that \n",
      "are not covered by other existing legal frameworks. Harmonised standards and supporting \n",
      "guidance and compliance tools will assist providers and users in complying  with the \n",
      "requirements laid down by the proposal  and minimise their costs. The costs incurred by \n",
      "operators are proportionate to the objectives achieved and the economic and reputational \n",
      "benefits that operators can expect from this proposal.  \n",
      "2.4. Choice  of the instrument  \n",
      "The choice of a regulation as a legal instrument is justified by the need for a uniform \n",
      "application of the new rules, such as definition of AI, the prohibition of certain harmful AI -\n",
      "enabled practices and the classification of certain AI syst ems. The direct applicability of a \n",
      "Regulation, in accordance with Article 288 TFEU, will reduce legal fragmentation and \n",
      "facilitate the development of a single market for lawful, safe and trustworthy AI systems. It \n",
      "will do so, in particular, by introducing a harmonised set of core requirements with regard to \n",
      "AI systems classified as high -risk and obligations for providers and users of those systems, \n",
      "improving the protection of fundamental rights and providing legal certainty for operators and \n",
      "consumers alike . \n",
      "At the same time, the provisions of the regulation are not overly prescriptive and leave room \n",
      "for different levels of Member State action for elements that do not undermine the objectives \n",
      "of the initiative, in particular the internal organisation of the market surveillance system and \n",
      "the uptake of measures to foster innovation.  \n",
      "3. RESULTS  OF EX-POST  EVALUATIONS,  STAKEHOLDER  \n",
      "CONSULTATIONS  AND  IMPACT  ASSESSMENTS  \n",
      "3.1. Stakeholder  consultation  \n",
      "This proposal is the result of extensive consultation with all major stakeholders, in which the \n",
      "general principles and minimum standards for consultation of interested parties by the \n",
      "Commission were applied.  \n",
      "An online public consultation  was launched on 1 9 February 2020 along with the publication \n",
      "of the White Paper on Artificial Intelligence and ran until 14 June 2020. The objective of that \n",
      "consultation was to collect views and opinions on the White Paper. It targeted all interested \n",
      "stakeholders from the p ublic and private sectors, including governments, local authorities, \n",
      "commercial and non -commercial organisations, social partners, experts, academics and \n",
      "citizens. After analysing all the responses received, the Commission published a summary \n",
      "outcome and t he individual responses on its website22. \n",
      "In total, 1215 contributions were received, of which 352 were from companies or business \n",
      "organisations/associations, 406 from individuals (92%individuals from EU ), 152 on behalf of \n",
      "                                                 \n",
      "22 See all consultation results here.  EN 8  EN academic/research institutions, a nd 73 from public authorities. Civil society’s voices were \n",
      "represented by 160 respondents (among which 9 consumers’ organisations, 129 non -\n",
      "governmental organisations and 22 trade unions), 72 respondents contributed as ‘others’. Of \n",
      "the 352 business and indu stry representatives, 222 were companies and business \n",
      "representatives, 41.5% of which were micro, small and medium -sized enterprises. The rest \n",
      "were business associations. Overall, 84% of business and industry replies came from the EU -\n",
      "27. Depending on the q uestion, between 81 and 598 of the respondents used the free text \n",
      "option to insert comments. Over 450 position papers were submitted through the EU Survey \n",
      "website, either in addition to questionnaire answers (over 400) or as stand -alone contributions \n",
      "(over  50). \n",
      "Overall, there is a general agreement amongst stakeholders on a need for action. A large \n",
      "majority of stakeholders agree that legislative gaps exist or that new legislation is needed. \n",
      "However, several stakeholders warn the Commission to avoid duplicat ion, conflicting \n",
      "obligations and overregulation. There were many comments underlining the importance of a \n",
      "technology neutral and proportionate regulatory framework.  \n",
      "Stakeholders mostly requested a narrow, clear and precise definition for AI. Stakeholders a lso \n",
      "highlighted that besides the clarification of the term of AI, it is important to define ‘risk’, \n",
      "‘high -risk’, ‘low -risk’, ‘remote biometric identification’ and ‘harm’.  \n",
      "Most of the respondents are explicitly in favour of the risk -based approach. Using a risk-based \n",
      "framework was considered a better option than blanket regulation of all AI systems. The types \n",
      "of risks and threats should be based on a sector -by-sector and case -by-case approach. Risks \n",
      "also should be calculated taking into account the impact on  rights and safety.  \n",
      "Regulatory sandboxes could be very useful for the promotion of AI  and are welcomed by \n",
      "certain stakeholders, especially the Business Associations.  \n",
      "Among those who formulated their opinion on the enforcement models, more than 50%, \n",
      "especi ally from the business associations, were in favour of a combination of an ex -ante risk \n",
      "self-assessment and an ex -post enforcement for high -risk AI systems.  \n",
      "3.2. Collection  and use of expertise  \n",
      "The proposal builds on two years of analysis and close involve ment of stakeholders, including \n",
      "academics, businesses, social partners, non -governmental organisations, Member States and \n",
      "citizens. The preparatory work started in 2018 with the setting up of a High -Level Expert \n",
      "Group on AI (HLEG) which had an inclusive an d broad composition of 52 well -known \n",
      "experts tasked to advise the Commission on the implementation of the Commission’s Strategy \n",
      "on Artificial Intelligence. In April 2019, the Commission supported23 the key requirements set \n",
      "out in the HLEG ethics guidelines for Trustworthy AI24, which had been revised to take into \n",
      "account more than 500 submissions from stakeholders. The key requirements reflect a \n",
      "widespread and common approach, as evidenced by a plethora of ethical codes and principles \n",
      "developed by many privat e and public organisations in Europe and beyond, that AI \n",
      "development and use should be guided by certain essential value -oriented principles. The \n",
      "Assessment List for Trustworthy Artificial Intelligence (ALTAI)25 made those requirements \n",
      "operational in a pilo ting process with over 350 organisations.  \n",
      "                                                 \n",
      "23 European Commission, Building Trust in Human -Centric Artificial Intelligence , COM(2019) 168.  \n",
      "24 HLEG, Ethics Guidelines for Trustworthy AI , 2019.  \n",
      "25 HLEG, Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self -assessment , 2020.  EN 9  EN In addition, the AI Alliance26 was formed as a platform for approximately 4000 stakeholders \n",
      "to debate the technological and societal implications of AI, culminating in a yearly AI \n",
      "Assembly.  \n",
      "The White Paper  on AI f urther developed this inclusive approach, inciting comments from \n",
      "more than 1250 stakeholders, including over 450 additional position papers. As a result, the \n",
      "Commission published an Inception Impact Assessment, which in turn attracted more than \n",
      "130 comment s27. Additional stakeholder workshops and events were also organised the \n",
      "results of which support the analysis in the impact assessment and the policy choices made in \n",
      "this proposal28. An external study  was also procured to feed into the impact assessment.  \n",
      "3.3. Impact  assessment  \n",
      "In line with its “Better Regulation” policy, the Commission conducted an impact assessment \n",
      "for this proposal examined by the Commission's Regulatory Scrutiny Board. A meeting with \n",
      "the Regulatory Scrutiny Board was held on 16 December 2020, which was follow ed by a \n",
      "negative opinion. After substantial revision of the impact assessment to address the comments \n",
      "and a resubmission of the impact assessment, the Regulatory Scrutiny Board issued a positive \n",
      "opinion on 21 March 2021. The opinions of the Regulatory Scru tiny Board, the \n",
      "recommendations and an explanation of how they have been taken into account are presented \n",
      "in Annex 1 of the impact assessment.  \n",
      "The Commission examined different policy options to achieve the general objective of the \n",
      "proposal, which is to ensure the proper functioning of the single market  by creating the \n",
      "conditions for the development and use of trustworthy AI in the Union.  \n",
      "Four policy options of different degrees of regulatory intervention were assessed:  \n",
      " Option 1 : EU legislative instrument setting up a voluntary labelling scheme;  \n",
      " Option 2 : a sectoral, “ad -hoc” approach;  \n",
      " Option 3 : Horizontal EU legislative instrument following a proportionate risk -\n",
      "based approach;  \n",
      " Option 3+ : Horizontal EU legislative instrument following a proportionate risk -\n",
      "based approach + codes of conduct for non -high-risk AI systems;  \n",
      " Option 4 : Horizontal EU legislative instrument establishing mandatory \n",
      "requirements for all AI systems, irrespective of the risk they pose.  \n",
      "According to the Commission's established methodology, each policy option was evaluated \n",
      "against economic and societal impacts, with a particular focus on impacts on fundamental \n",
      "rights. The preferred option is option 3+, a regulatory framework for high -risk AI systems \n",
      "only, with the possibility for all providers of non -high-risk AI systems to follow a code of \n",
      "conduct. The requirements will concern data, documentation and traceability, provision of \n",
      "information and transparency, human oversight and robustness and accuracy and would be \n",
      "mandatory for high -risk AI systems. Companies that introduced codes of conduct for other AI \n",
      "systems would do so voluntarily.  \n",
      "                                                 \n",
      "26 The AI Alliance is  a multi -stakeholder forum launched in June 2018, AI Alliance \n",
      "https://ec.europ a.eu/digital -single -market/en/european -ai-alliance  \n",
      "27 European Commission, Inception Impact Assessment For  a Proposal for a legal act of the European \n",
      "Parliament and the Council laying down requirements for Artificial Intelligence .  \n",
      "28 For details of all the consultations that have been carried out see Annex 2 of the impact assessment.  EN 10  EN The preferred option was considered suitable to address in the most effective way the \n",
      "objectives of this proposal. By requiring a restricted yet effective set of actions from AI \n",
      "developers and users, the preferred option limits the risks of violation of fundamental rights \n",
      "and safety of people and foster effective supervision and enforcement, by targeting the \n",
      "requirements only to systems where there is a high risk that such violations could occur. As a \n",
      "result, that option keeps compliance costs to a minimum, thus avoiding an unnecessary \n",
      "slowing of uptake due to higher prices and compliance costs. In order to ad dress possible \n",
      "disadvantages for SMEs, this option includes several provisions to support their compliance \n",
      "and reduce their costs, including creation of regulatory sandboxes and obligation to consider \n",
      "SMEs interests when setting fees related to conformity assessment.  \n",
      "The preferred option will increase people’s trust in AI, companies will gain in legal certainty, \n",
      "and Member States will see no reason to take unilateral action that could fragment the single \n",
      "market. As a result of higher demand due to higher t rust, more available offers due to legal \n",
      "certainty, and the absence of obstacles to cross -border movement of AI systems, the single \n",
      "market for AI will likely flourish. The European Union will continue to develop a fast -\n",
      "growing AI ecosystem of innovative se rvices and products embedding AI technology or \n",
      "stand -alone AI systems, resulting in increased digital autonomy.  \n",
      "Businesses or public authorities that develop or use AI applications that constitute a high risk \n",
      "for the safety or fundamental rights of citizen s would have to comply with specific \n",
      "requirements and obligations. Compliance with these requirements would  imply costs \n",
      "amounting to approximately EUR € 6000 to EUR € 7000 for the supply of an average high -\n",
      "risk AI system of around EUR € 170000 by 2025. For  AI users, there would also be the \n",
      "annual cost for the time spent on ensuring human oversight where this is appropriate, \n",
      "depending on the use case. Those have been estimated at approximately EUR € 5000 to EUR \n",
      "€ 8000 per year. Verification costs could amoun t to another EUR € 3000 to EUR € 7500 for \n",
      "suppliers of high -risk AI. Businesses or public authorities that develop or use any AI \n",
      "applications not classified as high risk would only have minimal obligations of information. \n",
      "However, they could choose to join  others and together adopt a code of conduct to follow \n",
      "suitable requirements,  and to ensure that their AI systems are trustworthy. In such a case, \n",
      "costs would be at most as high as for high -risk AI systems, but most probably lower.  \n",
      "The impacts of the poli cy options on different categories of stakeholders (economic operators/ \n",
      "business; conformity assessment bodies, standardisation bodies and other public bodies; \n",
      "individuals/citizens; researchers) are explained in detail in Annex 3 of the Impact assessment \n",
      "supporting this proposal.  \n",
      "3.4. Regulatory  fitness  and simplification  \n",
      "This proposal lays down obligation that will apply to providers and users of high -risk AI \n",
      "systems. For providers who develop and place such systems on the Union market, it will \n",
      "create lega l certainty and ensure that no obstacle to the cross -border provision of AI -related \n",
      "services and products emerge. For companies using AI, it will promote trust among their \n",
      "customers. For national public administrations, it will promote public trust in the use of AI \n",
      "and strengthen enforcement mechanisms (by introducing a European coordination \n",
      "mechanism, providing for appropriate capacities, and facilitating audits of the AI systems \n",
      "with new requirements for documentation, traceability and transparency). More over, the \n",
      "framework will envisage specific measures supporting innovation, including regulatory \n",
      "sandboxes and specific measures supporting small -scale users and providers of high -risk AI \n",
      "systems to comply with the new rules . \n",
      "The proposal also specifically aims at strengthening Europe’s competitiveness and industrial \n",
      "basis in AI. Full consistency is ensured with existing sectoral Union legislation applicable to EN 11  EN AI systems (e.g. on products and services) that will bring further clarity and simplify the \n",
      "enforc ement of the new rules.  \n",
      "3.5. Fundamental  rights  \n",
      "The use of AI with its specific characteristics (e.g. opacity, complexity, dependency on data, \n",
      "autonomous behaviour) can adversely affect a number of fundamental rights enshrined in the \n",
      "EU Charter of Fundamen tal Rights (‘the Charter’) . This proposal seeks to ensure a high level \n",
      "of protection for those fundamental rights and aims to address various sources of risks \n",
      "through a clearly defined risk -based approach. With a set of requirements for trustworthy AI \n",
      "and proportionate obligations on all value chain participants, the proposal will enhance and \n",
      "promote the protection of the rights  protected by the Charter: the right  to human dignity \n",
      "(Article 1), respect for private life and protection of personal data (Articl es 7 and 8), non -\n",
      "discrimination (Article 21) and equality between women and men (Article 23). It aims to \n",
      "prevent a chilling effect on the rights to freedom of expression (Article 11) and freedom of \n",
      "assembly (Article 12), to ensure protection of the right t o an effective remedy and to a fair \n",
      "trial, the rights of defence and the presumption of innocence (Articles 47 and 48), as well as \n",
      "the general principle of good administration. Furthermore, as applicable in certain domains, \n",
      "the proposal will positively aff ect the rights of a number of special groups, such as the \n",
      "workers’ rights to fair and just working conditions (Article 31), a high level of consumer \n",
      "protection (Article 28), the rights of the child (Article 24) and the integration of persons with \n",
      "disabilit ies (Article 26). The right to a high level of environmental protection and the \n",
      "improvement of the quality of the environment (Article 37) is also relevant, including in \n",
      "relation to the health and safety of people. The obligations for ex ante testing, risk  \n",
      "management and human oversight will also facilitate the respect of other fundamental rights \n",
      "by minimising the risk of erroneous or biased AI -assisted decisions in critical areas such as \n",
      "education and training, employment, important services, law enforceme nt and the  judiciary. In \n",
      "case infringements of fundamental rights still happen, effective redress for affected persons \n",
      "will be made possible by ensuring transparency and traceability of the AI systems coupled \n",
      "with strong ex post controls.  \n",
      "This proposal imp oses some restrictions on the freedom to conduct business (Article 16) and \n",
      "the freedom of art and science (Article 13) to ensure compliance with overriding reasons of \n",
      "public interest such as health, safety, consumer protection and the protection of other \n",
      "fundamental rights  (‘responsible innovation’) when high -risk AI technology is developed and \n",
      "used. Those restrictions are proportionate and limited to the minimum necessary to prevent \n",
      "and mitigate serious safety risks and likely infringements of fundamental rights.  \n",
      "The increased transparency obligations will also not disproportionately affect the right to \n",
      "protection of intellectual property (Article 17(2)), since they will be limited only to the \n",
      "minimum necessary information for individuals to exercise their right to an effective remedy \n",
      "and to the necessary transparency towards supervision and enforcement authorities, in line \n",
      "with their mandates. Any disclosure of information will be carried out in compliance with \n",
      "relevant legislation in the field, including Directive 2016/943 on the protection of undisclosed \n",
      "know -how and business information (trade secrets) against their unlawful acquisition, use and \n",
      "disclosure . When public authorities and notified bodies need to be given access to confidential \n",
      "information or source code to examine compliance with substantial obligations, they are \n",
      "placed under binding confidentiality obligations.  \n",
      "4. BUDGETARY  IMPLICATIONS  \n",
      "Member States will have to designate supervisory authorities in charge of implementing the \n",
      "legislative requ irements. Their supervisory function could build on existing arrangements, for EN 12  EN example regarding conformity assessment bodies or market surveillance, but would require \n",
      "sufficient technological expertise and human and financial resources. Depending on the p re-\n",
      "existing structure in each Member State, this could amount to 1 to 25 Full Time Equivalents \n",
      "per Member State.  \n",
      "A detailed overview of the costs involved is provided in the ‘financial statement’ linked to \n",
      "this proposal.  \n",
      "5. OTHER  ELEMENTS  \n",
      "5.1. Implementati on plans  and monitoring,  evaluation  and reporting  arrangements  \n",
      "Providing for a robust monitoring and evaluation mechanism is crucial to ensure that the \n",
      "proposal will be effective in achieving its specific objectives. The Commission will be in \n",
      "charge of mon itoring the effects of the proposal. It will establish a system for registering \n",
      "stand -alone high -risk AI applications in a public EU -wide database. This registration will also \n",
      "enable competent authorities, users and other interested people to verify if the  high-risk AI \n",
      "system complies with the requirements  laid down in the proposal  and to exercise enhanced \n",
      "oversight over those AI systems posing high risks to fundamental rights. To feed this \n",
      "database, AI providers will be obliged to provide meaningful inform ation about their systems \n",
      "and the conformity assessment carried out on those systems .  \n",
      "Moreover, AI providers will be obliged to inform national competent authorities about serious \n",
      "incidents or malfunctioning that constitute a breach of fundamental rights obligations as soon \n",
      "as they become aware of them, as well as any recalls or withdrawals of AI systems from the \n",
      "market. National competent authorities will then investigate the incidents/or malfunctioning, \n",
      "collect all the necessary information and regularly  transmit it to the Commission with \n",
      "adequate metadata. The Commission will complement this information on the incidents by a \n",
      "comprehensive analysis of the overall market for AI.  \n",
      "The Commission will publish a report evaluating and reviewing the proposed AI  framework \n",
      "five years following the date on which it becomes applicable.  \n",
      "5.2. Detailed  explanation  of the specific  provisions  of the proposal  \n",
      "5.2.1.  SCOPE  AND  DEFINITIONS  (TITLE  I) \n",
      "Title I defines the subject matter of the regulation and the scope of application of the new \n",
      "rules that cover the placing on the market, putting into service and use of AI systems . It also \n",
      "sets out the definitions used throughout the instrument. The definition of AI system in the \n",
      "legal framework aims to be as technology neutral and future proof as possible, taking into \n",
      "account the fast technological and market developments related to AI. In order to provide the \n",
      "needed legal certainty, Title I is complemented by Ann ex I, which contains a detailed list of \n",
      "approaches and techniques for the development of AI to be adapted by the Commission in line \n",
      "with new technological developments. Key participants across the AI value chain are also \n",
      "clearly defined such as providers a nd users of AI systems that cover both public and private \n",
      "operators to ensure a level playing field.  \n",
      "5.2.2.  PROHIBITED ARTIFICIAL INTELLIGENCE PRACTICES (TITLE II)  \n",
      "Title II  establishes a list of prohibited AI. The regulation follows a risk -based approach,  \n",
      "differentiating between uses of AI that create (i) an unacceptable risk, (ii) a high risk, and (iii) \n",
      "low or minimal risk. The list of prohibited practices in Title II comprises all those AI systems \n",
      "whose use is considered unacceptable as contravening Unio n values, for instance by violating \n",
      "fundamental rights. The prohibitions covers practices that have a significant potential to \n",
      "manipulate persons  through subliminal techniques beyond their consciousness or exploit EN 13  EN vulnerabilities of specific vulnerable gro ups such as children or persons with disabilities in \n",
      "order to materially distort their behaviour in a manner that is likely to cause them or another \n",
      "person psychological or physical harm. Other manipulative or exploitative practices affecting \n",
      "adults that m ight be facilitated by AI systems could be covered by the existing data \n",
      "protection, consumer protection and digital service legislation that guarantee that natural \n",
      "persons are properly informed and have free choice not to be subject to profiling or other \n",
      "practices that might affect their behaviour. The proposal also prohibits AI -based social \n",
      "scoring for general purposes done by public authorities. Finally, the use of ‘real time’ remote \n",
      "biometric identification systems in publicly accessible spaces for the purpose of law \n",
      "enforcement is also prohibited unless certain limited exceptions apply.  \n",
      "5.2.3.  HIGH -RISK AI SYSTEMS (TITLE III)  \n",
      "Title III contains specific rules for AI systems that create a high risk to the health and safety \n",
      "or fundamental rights of natural persons. In line with a risk -based approach, those high -risk \n",
      "AI systems are permitted on the European market subject to compliance with certain \n",
      "mandatory requirements and an ex -ante conformity assessment. The classification of an AI \n",
      "system as hi gh-risk is based on the intended purpose of the AI system, in line with existing \n",
      "product safety legislation. Therefore, the classification as high -risk does not only depend on \n",
      "the function performed by the AI system, but also on the specific purpose and mo dalities for \n",
      "which that system is used.  \n",
      "Chapter 1 of Title III sets the classification rules and identifies two main categories of high -\n",
      "risk AI systems:  \n",
      " AI systems intended to be used as safety component of products that are subject to \n",
      "third party ex -ante conformity assessment;  \n",
      " other stand -alone AI systems with mainly fundamental rights implications that are \n",
      "explicitly listed in Annex III.  \n",
      "This list of high -risk AI systems in Annex III contains a limited number of AI systems whose \n",
      "risks have already materi alised or are likely to materialise in the near future. To ensure that \n",
      "the regulation can be adjusted to emerging uses and applications of AI, the Commission may \n",
      "expand the list of high -risk AI systems used  within certain pre -defined areas, by applying a \n",
      "set of criteria and risk assessment methodology.  \n",
      "Chapter 2 sets out the legal requirements for high -risk AI systems in relation to data and data \n",
      "governance, documentation and recording keeping, transparency and provision of information \n",
      "to users, human over sight, robustness, accuracy and security. The proposed minimum \n",
      "requirements are already state -of-the-art for many diligent operators and the result of two \n",
      "years of preparatory work, derived from the Ethics Guidelines of the HLEG29, piloted by \n",
      "more than 350 organisations30. They are also largely consistent with other international \n",
      "recommendations and principles, which ensures that the proposed AI framework is \n",
      "compatible with those adopted by the EU’s international trade partners . The precise technical \n",
      "solution s to achieve compliance with those requirements may be provided by standards or by \n",
      "other technical specifications or otherwise be developed in accordance with general \n",
      "engineering or scientific knowledge at the discretion of the provider of the AI system. T his \n",
      "flexibility is particularly important, because it allows providers of AI systems to choose the \n",
      "                                                 \n",
      "29 High -Level Expert Group on Artificial Intelligence, Ethics Guidelines for Trustworthy AI , 2019.  \n",
      "30 They were also endorsed by the Commission in its 2019 Communication on human -centric approach to \n",
      "AI. EN 14  EN way to meet their requirements, taking into account the state -of-the-art and technological and \n",
      "scientific progress in this field.  \n",
      "Chapter 3 places a clear se t of horizontal obligations on providers of high -risk AI systems. \n",
      "Proportionate obligations are also placed on users and other participants across the AI value \n",
      "chain (e.g., importers, distributors, authorized representatives).  \n",
      "Chapter 4 sets the framework for notified bodies to be involved as independent third parties in \n",
      "conformity assessment procedures, while Chapter 5 explains in detail the conformity \n",
      "assessment procedures to be followed for each type of high -risk AI system.  The conformity \n",
      "assessment app roach aims to minimise the burden for economic operators as well as for \n",
      "notified bodies, whose capacity needs to be progressively ramped up over time. AI systems \n",
      "intended to be used as safety components of products that are regulated under the New \n",
      "Legislat ive Framework legislation (e.g. machinery, toys, medical devices, etc.) will be subject \n",
      "to the same ex -ante and ex -post compliance and enforcement mechanisms of the products of \n",
      "which they are a component. The key difference is that the ex -ante and ex -post mechanisms \n",
      "will ensure compliance not only with the requirements established by sectorial legislation, but \n",
      "also with the requirements established by this regulation.  \n",
      "As regards stand -alone high -risk AI systems that are referred to in Annex III, a new \n",
      "comp liance and enforcement system will be established. This follows the model of the New \n",
      "Legislative Framework legislation implemented through internal control checks by the \n",
      "providers with the exception of remote biometric identification systems that would be subject \n",
      "to third party conformity assessment. A comprehensive ex -ante conformity assessment \n",
      "through internal checks, combined with a strong ex -post enforcement, could be an effective \n",
      "and reasonable solution for those systems, given the early phase of the r egulatory intervention \n",
      "and the fact the AI sector is very innovative and expertise for auditing is only now being \n",
      "accumulated. An assessment through internal checks for ‘stand -alone’ high -risk AI systems \n",
      "would require a full, effective and properly documen ted ex ante compliance with all \n",
      "requirements of the regulation and compliance with robust quality and risk management \n",
      "systems and post -market monitoring. After the provider has performed the relevant \n",
      "conformity assessment, it should register those stand -alone high -risk AI systems in an EU \n",
      "database that will be managed by the Commission to increase public transparency and \n",
      "oversight and strengthen ex post supervision by competent authorities. By contrast, for \n",
      "reasons of consistency with the existing product s afety legislation, the conformity assessments \n",
      "of AI systems that are safety components of products will follow a system with third party \n",
      "conformity assessment procedures already established under the relevant sectoral product \n",
      "safety legislation. New ex ant e re-assessments of the conformity will be needed in case of \n",
      "substantial modifications to the AI systems (and notably changes which go beyond what is \n",
      "pre-determined by the provider in its technical documentation and checked at the moment of \n",
      "the ex -ante con formity assessment).  \n",
      "5.2.4.  TRANSPARENCY OBLIGATIONS FOR CERTAIN AI SYSTEMS (TITLE IV)  \n",
      "Title IV  concerns certain AI systems to take account of the specific risks of manipulation they \n",
      "pose. Transparency obligations will apply for systems that (i) interact with humans, (ii) are \n",
      "used to detect emotions or determine association with (social) categories based on biometric \n",
      "data, or (iii) generate or manipulate content (‘deep fakes’). When persons interact with an AI \n",
      "system or their emotions or characteristics ar e recognised through automated means, people \n",
      "must be informed of that circumstance. If an AI system is used to generate or manipulate \n",
      "image, audio or video content that appreciably resembles  authentic content, there should be an \n",
      "obligation to disclose that  the content is generated through automated means, subject to EN 15  EN exceptions for legitimate purposes (law enforcement, freedom of expression). This allows \n",
      "persons to make informed choices or step back from a given situation.  \n",
      "5.2.5.  MEASURES IN SUPPORT OF INNOV ATION (TITLE V)  \n",
      "Title V contributes to the objective to create a legal framework that is innovation -friendly, \n",
      "future -proof and resilient to disruption. To that end, it encourages national competent \n",
      "authorities to set up regulatory sandboxes and sets a bas ic framework in terms of governance, \n",
      "supervision and liability. AI regulatory sandboxes establish a controlled environment to test \n",
      "innovative technologies for a limited time on the basis of a testing plan agreed with the \n",
      "competent authorities. Title V also  contains measures to reduce the regulatory burden on \n",
      "SMEs and start -ups.  \n",
      "5.2.6.  GOVERNANCE AND IMPLEMENTATION (TITLES VI, VII AND VII)  \n",
      "Title VI sets up the governance systems at Union and national level. At Union level, the \n",
      "proposal establishes a European Artificial Intelligence Board (the ‘Board’), composed of \n",
      "representatives from the Member States and the Commission. The Board will facilitate a \n",
      "smooth, effective and harmonised implementation of this regulation by contributing to the \n",
      "effective cooperation of the national supervisory authorities and the Commission and \n",
      "providing advice and expertise to the Commission. It will also collect and share best practices \n",
      "among the Member States.  \n",
      "At national level, Member States will have to designate one or more national competent \n",
      "authorities and, among them, the national supervisory authority, for the purpose of \n",
      "supervising the application and implementati on of the regulation. The European Data \n",
      "Protection Supervisor will act as the competent authority for the supervision of the Union \n",
      "institutions, agencies and bodies when they fall within the scope of this regulation.  \n",
      "Title VII  aims to facilitate the monito ring work of the Commission and national authorities \n",
      "through the establishment of an EU -wide database for stand -alone high -risk AI systems with \n",
      "mainly fundamental rights implications. The database will be operated by the Commission \n",
      "and provided with data b y the providers of the AI systems, who will be required to register \n",
      "their systems before placing them on the market or otherwise putting them into service.  \n",
      "Title VIII sets out the monitoring and reporting obligations for providers of AI systems with \n",
      "regard  to post -market monitoring and reporting and investigating on AI -related incidents and \n",
      "malfunctioning. Market surveillance authorities would also control the market and investigate \n",
      "compliance with the obligations and requirements for all high -risk AI syste ms already placed \n",
      "on the market. Market surveillance authorities would have all powers under Regulation (EU) \n",
      "2019/1020 on market surveillance. Ex -post enforcement should ensure that once the AI \n",
      "system has been put on the market, public authorities have the  powers and resources to \n",
      "intervene in case AI systems generate unexpected risks, which warrant rapid action. They will \n",
      "also monitor compliance of operators with their relevant obligations under the regulation. The \n",
      "proposal does not foresee the automatic cr eation of any additional bodies or authorities at \n",
      "Member State level. Member States may therefore appoint (and draw upon the expertise of) \n",
      "existing sectorial authorities, who would be entrusted also with the powers to monitor and \n",
      "enforce the provisions of the regulation.  \n",
      "All this is without prejudice to the existing system and allocation of powers of ex -post \n",
      "enforcement of obligations regarding fundamental rights in the Member States. When \n",
      "necessary for their mandate, existing supervision and enforcement a uthorities will also have \n",
      "the power to request and access any documentation maintained following this regulation and, \n",
      "where needed, request market surveillance authorities to organise testing of the high -risk AI \n",
      "system through technical means.  EN 16  EN 5.2.7.  CODES  OF CONDUCT (TITLE IX)  \n",
      "Title IX creates a framework for the creation of codes of conduct, which aim to encourage \n",
      "providers of non -high-risk AI systems to apply voluntarily the mandatory requirements for \n",
      "high-risk AI systems (as laid out in Title III). Pro viders of non -high-risk AI systems may \n",
      "create and implement the codes of conduct themselves. Those codes may also include \n",
      "voluntary commitments related, for example, to environmental sustainability, accessibility for \n",
      "persons with disability, stakeholders’ participation in the design and development of AI \n",
      "systems, and diversity of development teams.  \n",
      "5.2.8.  FINAL PROVISIONS (TITLES X, XI AND XII)  \n",
      "Title X  emphasizes the obligation of all parties to respect the confidentiality of information \n",
      "and data  and sets  out rules for the exchange of information obtained during the \n",
      "implementation of the regulation. Title X also includes measures to ensure the effective \n",
      "implementation of the regulation through effective, proportionate, and dissuasive penalties for \n",
      "infringe ments of the provisions . \n",
      "Title XI sets out rules for the exercise of delegation and implementing powers. The proposal \n",
      "empowers the Commission to adopt, where appropriate, implementing acts to ensure uniform \n",
      "application of the regulation or delegated acts t o update or complement the lists in Annexes I \n",
      "to VII.  \n",
      "Title XII  contains an obligation for the Commission to assess regularly the need for an update \n",
      "of Annex III and to prepare regular reports on the evaluation and review of the regulation. It \n",
      "also lays d own final provisions, including a differentiated transitional period for the initial \n",
      "date of the applicability of the regulation to facilitate the smooth implementation for all \n",
      "parties concerned.  EN 17  EN 2021/0106 (COD)  \n",
      "Proposal for a  \n",
      "REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL  \n",
      "LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE \n",
      "(ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION \n",
      "LEGISLATIVE ACTS  \n",
      "THE EUROPEAN PARLIAMENT AND THE COUNCI L OF THE EUROPEAN UNION,  \n",
      "Having regard to the Treaty on the Functioning of the European Union, and in particular \n",
      "Articles 16 and 114 thereof,  \n",
      "Having regard to the proposal from the European Commission,  \n",
      "After transmission of the draft legislative act to the  national parliaments,  \n",
      "Having regard to the opinion of the European Economic and Social Committee31, \n",
      "Having regard to the opinion of the Committee of the Regions32, \n",
      "Acting in accordance with the ordinary legislative procedure,  \n",
      "Whereas:  \n",
      "(1) The purpose of thi s Regulation is to improve the functioning of the internal market by \n",
      "laying down a uniform legal framework in particular for the development, marketing \n",
      "and use of artificial intelligence in conformity with Union values. This Regulation \n",
      "pursues a number of overriding reasons of public interest, such as a high level of \n",
      "protection of health, safety and fundamental rights, and it ensures the free movement \n",
      "of AI -based goods and services cross -border, thus preventing Member States from \n",
      "imposing restrictions on th e development, marketing and use of AI systems, unless \n",
      "explicitly authorised by this Regulation.  \n",
      "(2) Artificial intelligence systems (AI systems) can be easily deployed in multiple sectors \n",
      "of the economy and society, including cross border, and circulate throughout the \n",
      "Union. Certain Member States have already explored the adoption of national rules to \n",
      "ensure that artificial intelligence is safe and is developed and used in compliance with \n",
      "fundamental rights obligations. Differing national rules may lead t o fragmentation of \n",
      "the internal market and decrease legal certainty for operators that develop or use AI \n",
      "systems. A consistent and high level of protection throughout the Union should \n",
      "therefore be ensured, while divergences hampering the free circulation o f AI systems \n",
      "and related products and services within the internal market should be prevented, by \n",
      "laying down uniform obligations for operators and guaranteeing the uniform \n",
      "protection of overriding reasons of public interest and of rights of persons throug hout \n",
      "the internal market based on Article 114 of the Treaty on the Functioning of the \n",
      "European Union (TFEU). To the extent that this Regulation contains specific rules on \n",
      "the protection of individuals with regard to the processing of personal data  concerni ng \n",
      "                                                 \n",
      "31 OJ C […], […], p. […].  \n",
      "32 OJ C […], […], p. […].  EN 18  EN restrictions of the use of AI systems for ‘real -time’ remote biometric identification in \n",
      "publicly accessible spaces for the purpose of law enforcement, it is appropriate to base \n",
      "this Regulation, in as far as those specific rules are concerned, on Articl e 16 of the \n",
      "TFEU. In light of  those specific rules and the recourse to Article 16 TFEU, it is \n",
      "appropriate  to consult the European Data Protection Board.  \n",
      "(3) Artificial intelligence is a fast evolving family of technologies that can contribute to a \n",
      "wide arr ay of economic and societal benefits across the entire spectrum of industries \n",
      "and social activities. By improving prediction, optimising operations and resource \n",
      "allocation, and personalising digital solutions available for individuals and \n",
      "organisations, th e use of artificial intelligence can provide key competitive advantages \n",
      "to companies and support socially and environmentally beneficial outcomes, for \n",
      "example in healthcare, farming, education and training, infrastructure management, \n",
      "energy, transport and logistics, public services, security, justice, resource and energy \n",
      "efficiency, and climate change mitigation and adaptation.  \n",
      "(4) At the same time, depending on the circumstances regarding its specific application \n",
      "and use, artificial intelligence may genera te risks and cause harm to public interests \n",
      "and rights that are protected by Union law. Such harm might be material or \n",
      "immaterial.  \n",
      "(5) A Union legal framework laying down harmonised rules on artificial intelligence is \n",
      "therefore needed to foster the develop ment, use and uptake of artificial intelligence in \n",
      "the internal market that at the same time meets a high level of protection of public \n",
      "interests, such as health and safety and the protection of fundamental rights, as \n",
      "recognised and protected by Union law.  To achieve that objective, rules regulating the \n",
      "placing on the market and putting into service of certain AI systems should be laid \n",
      "down, thus ensuring the smooth functioning of the internal market and allowing those \n",
      "systems to benefit from the principle of free movement of goods and services. By \n",
      "laying down those rules, this Regulation supports the objective of the Union of being a \n",
      "global leader in the development of secure, trustworthy and ethical artificial \n",
      "intelligence, as stated by the European Counci l33, and it ensures the protection of \n",
      "ethical principles, as specifically requested by the European Parliament34. \n",
      "(6) The notion of AI system should be clearly defined to ensure legal certainty, while \n",
      "providing the flexibility to accommodate future technolog ical developments. The \n",
      "definition should be based on the key functional characteristics of the software, in \n",
      "particular the ability, for a given set of human -defined objectives, to generate outputs \n",
      "such as content, predictions, recommendations, or decisions  which influence the \n",
      "environment with which the system interacts, be it in a physical or digital dimension. \n",
      "AI systems can be designed to operate with varying levels of autonomy and be used on \n",
      "a stand -alone basis or as a component of a product, irrespectiv e of whether the system \n",
      "is physically integrated into the product (embedded) or serve the functionality of the \n",
      "product without being integrated therein (non -embedded). The definition of AI system \n",
      "should be complemented by a list of specific techniques and approaches used for its \n",
      "development, which should be kept up -to–date in the light of market and technological \n",
      "                                                 \n",
      "33 European Council, Special meeting of the European Council (1 and 2 October 2020) – Conclusions, \n",
      "EUCO 13/20, 2020, p. 6.  \n",
      "34 European Parliament resolution of 20 October 2020 with recommendations to the Commissio n on a \n",
      "framework of ethical aspects of artificial intelligence, robotics and related technologies, \n",
      "2020/2012(INL).  EN 19  EN developments through the adoption of delegated acts by the Commission to amend that \n",
      "list. \n",
      "(7) The notion of biometric data used in this Regulation  is in line with and should be \n",
      "interpreted consistently with the notion of biometric data as defined in Article 4(14) of \n",
      "Regulation (EU) 2016/679 of the European Parliament and of the Council35, Article \n",
      "3(18) of Regulation (EU) 2018/1725 of the European Par liament and of the Council36 \n",
      "and Article 3(13) of Directive (EU) 2016/680 of the European Parliament and of the \n",
      "Council37.   \n",
      "(8) The notion of remote biometric identification system as used in this Regulation should \n",
      "be defined functionally, as  an AI system  intended for the identification of natural \n",
      "persons at a distance through the comparison of a person’s biometric data with the \n",
      "biometric data contained in a reference database, and without prior knowledge whether \n",
      "the targeted person will be present and can  be identified, irrespectively of the \n",
      "particular technology, processes or types of biometric data used. Considering their \n",
      "different characteristics and manners in which they are used, as well as the different \n",
      "risks involved, a distinction should be made be tween ‘real -time’ and ‘post’ remote \n",
      "biometric identification systems. In the case of ‘real -time’ systems, the capturing of \n",
      "the biometric data, the comparison and the identification occur all instantaneously, \n",
      "near-instantaneously or in any event without a s ignificant delay. In this regard, there \n",
      "should be no scope for circumventing the rules of this Regulation on the ‘real -time’ \n",
      "use of the AI systems in question by providing for minor delays. ‘Real -time’ systems \n",
      "involve the use of ‘live’ or ‘near -‘live’ mate rial, such as video footage, generated by a \n",
      "camera or other device with similar functionality. In the case of ‘post’ systems, in \n",
      "contrast, the biometric data have already been captured and the comparison and \n",
      "identification occur only after a significant de lay. This involves material, such as \n",
      "pictures or video footage generated by  closed circuit television cameras or private \n",
      "devices, which has been generated before the use of the system in respect of the \n",
      "natural persons concerned.  \n",
      "(9) For the purposes of thi s Regulation the notion of publicly accessible space should be \n",
      "understood as referring to any physical place that is accessible to the public, \n",
      "irrespective of whether the place in question is privately or publicly owned. Therefore, \n",
      "the notion does not cove r places that are private in nature and normally not freely \n",
      "accessible for third parties, including law enforcement authorities, unless those parties \n",
      "have been specifically invited or authorised, such as homes, private clubs, offices, \n",
      "warehouses and factor ies.  Online spaces are not covered either, as they are not \n",
      "physical spaces. However, the mere fact that certain conditions for accessing a \n",
      "                                                 \n",
      "35 Regulation  (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the \n",
      "protection of natural persons with regard to  the processing of personal data and on the free movement of \n",
      "such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, \n",
      "p. 1).  \n",
      "36 Regulation (EU) 2018/1725 of the European Parliament and of the Council of 23 Octobe r 2018 on the \n",
      "protection of natural persons with regard to the processing of personal data by the Union institutions, \n",
      "bodies, offices and agencies and on the free movement of such data, and repealing Regulation (EC) No \n",
      "45/2001 and Decision No 1247/2002/EC (OJ L 295, 21.11.2018, p. 39)  \n",
      "37 Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the \n",
      "protection of natural persons with regard to the processing of personal data by competent authorities for \n",
      "the purposes of the prevention, investigation, detection or prosecution of criminal offences or the \n",
      "execution of criminal penalties, and on the free movement of such data, and repealing Council \n",
      "Framework Decision 2008/977/JHA (Law Enforcement Directive) ( OJ L 119, 4.5.2016, p . 89).  EN 20  EN particular space may apply, such as admission tickets or age restrictions, does not \n",
      "mean that the space is not publi cly accessible within the meaning of this Regulation. \n",
      "Consequently, in addition to public spaces such as streets, relevant parts of \n",
      "government buildings and most transport infrastructure, spaces such as cinemas, \n",
      "theatres, shops and shopping centres are nor mally also publicly accessible. Whether a \n",
      "given space is accessible to the public should however be determined on a case -by-\n",
      "case basis, having regard to the specificities of the individual situation at hand.   \n",
      "(10) In order to ensure a level playing field a nd an effective protection of rights and \n",
      "freedoms of individuals across the Union, the rules established by this Regulation \n",
      "should apply to providers of AI systems in a non -discriminatory manner, irrespective \n",
      "of whether they are established within the Unio n or in a third country, and to users of \n",
      "AI systems established within the Union.  \n",
      "(11) In light of their digital nature, certain AI systems should fall within the scope of this \n",
      "Regulation even when they are neither placed on the market, nor put into servic e, nor \n",
      "used in the Union. This is the case for example of an operator established in the Union \n",
      "that contracts certain services to an operator established outside the Union in relation \n",
      "to an activity to be performed by an AI system that would qualify as hig h-risk and \n",
      "whose effects impact natural persons located in the Union. In those circumstances, the \n",
      "AI system used by the operator outside the Union could process data lawfully \n",
      "collected in and transferred from the Union, and provide to the contracting opera tor in \n",
      "the Union the output of that AI system resulting from that processing, without that AI \n",
      "system being placed on the market, put into service or used in the Union. To prevent \n",
      "the circumvention of this Regulation and to ensure an effective protection of  natural \n",
      "persons located in the Union, this Regulation should also apply to providers and users \n",
      "of AI systems that are established in a third country, to the extent the output produced \n",
      "by those systems is used in the Union. Nonetheless, to take into accoun t existing \n",
      "arrangements and special needs for cooperation with foreign partners with whom \n",
      "information and evidence is exchanged, this Regulation should not apply to public \n",
      "authorities of a third country and international organisations when acting in the \n",
      "framework of international agreements concluded at national or European level for law \n",
      "enforcement and judicial cooperation with the Union or with its Member States. Such \n",
      "agreements have been concluded bilaterally between Member States and third \n",
      "countries or between the European Union, Europol and other EU agencies and third \n",
      "countries and international organisations.  \n",
      "(12) This Regulation should also apply to Union institutions, offices, bodies and agencies \n",
      "when acting as a provider or user of an AI system. AI  systems exclusively developed \n",
      "or used for military purposes should be excluded from the scope of this Regulation \n",
      "where that use falls under the exclusive remit of the Common Foreign and Security \n",
      "Policy regulated under Title V of the Treaty on the European  Union (TEU).  This \n",
      "Regulation should be without prejudice to the provisions regarding the liability of \n",
      "intermediary service providers set out in Directive 2000/31/EC of the European \n",
      "Parliament and of the Council [as amended by the Digital Services Act].  \n",
      "(13) In order to ensure a consistent and high level of protection of public interests as \n",
      "regards health, safety and fundamental rights, common normative standards for all \n",
      "high-risk AI systems should be established. Those standards should be consistent with  \n",
      "the Charter of fundamental rights of the European Union (the Charter) and should be \n",
      "non-discriminatory and in line with the Union’s international trade commitments.  EN 21  EN (14) In order  to introduce a proportionate and effective set of binding rules for AI syste ms, \n",
      "a clearly defined risk -based approach should be followed. That approach should tailor \n",
      "the type and content of such rules to the intensity and scope of the risks that AI \n",
      "systems can generate. It is therefore necessary to prohibit certain artificial inte lligence \n",
      "practices, to lay down requirements for high -risk AI systems and obligations for the \n",
      "relevant operators, and to lay down transparency obligations for certain AI systems.  \n",
      "(15) Aside from the many beneficial uses of artificial intelligence, that tec hnology can also \n",
      "be misused and provide novel and powerful tools for manipulative, exploitative and \n",
      "social control practices. Such practices are particularly harmful and should be \n",
      "prohibited because they contradict Union values of respect for human dignity , \n",
      "freedom, equality, democracy and the rule of law and Union fundamental rights, \n",
      "including the right to non -discrimination, data protection and privacy  and the rights of \n",
      "the child . \n",
      "(16) The placing on the market, putting into service or use of certain AI s ystems intended \n",
      "to distort human  behaviour, whereby physical or psychological harms are likely to \n",
      "occur, should be forbidden. Such AI systems deploy subliminal components \n",
      "individuals cannot perceive or exploit vulnerabilities of children and people due to \n",
      "their age, physical or mental incapacities. They do so with the intention to materially \n",
      "distort the behaviour of a person and in a manner that causes or is likely to cause harm \n",
      "to that or another person. The intention may not be presumed if the distortion of \n",
      "human behaviour results from factors external to the AI system which are outside of \n",
      "the control of the provider or the user. Research for legitimate purposes in relation to \n",
      "such AI systems should not be stifled by the prohibition, if such research does not \n",
      "amount to use of the AI system in human -machine relations that exposes natural \n",
      "persons to harm and such research is carried out in accordance with recognised ethical \n",
      "standards for scientific research.  \n",
      "(17) AI systems  providing social scoring of natural  persons for general purpose by public \n",
      "authorities or on their behalf may lead to discriminatory outcomes and the exclusion of \n",
      "certain groups. They may violate the right to dignity and non -discrimination and the \n",
      "values of equality and justice. Such AI syst ems evaluate or classify the trustworthiness \n",
      "of natural persons based on their social behaviour in multiple contexts or known or \n",
      "predicted personal or personality characteristics. The social score obtained from such \n",
      "AI systems may lead to the detrimental o r unfavourable treatment of natural persons or \n",
      "whole groups thereof in social contexts, which are unrelated to the context in which \n",
      "the data was originally generated or collected or to a detrimental treatment that is \n",
      "disproportionate or unjustified to the gravity of their social behaviour.  Such AI \n",
      "systems should be therefore prohibited.  \n",
      "(18) The use of AI systems for ‘real -time’ remote biometric identification of natural \n",
      "persons in publicly accessible spaces for the purpose of law enforcement is considered \n",
      "particularly intrusive in the rights and freedoms of the concerned persons, to the extent \n",
      "that it may affect the private life of a large part of the population, evoke a feeling of \n",
      "constant surveillance and indirectly dissuade the exercise of the freedom of  assembly \n",
      "and other fundamental rights. In addition, the immediacy of the impact and the limited \n",
      "opportunities for further checks or corrections  in relation to the use of such systems \n",
      "operating in ‘real -time’ carry heightened risks for the rights and freed oms of the \n",
      "persons that are concerned by law enforcement activities.  \n",
      "(19) The use of those systems for the purpose of law enforcement should therefore be \n",
      "prohibited, except in three exhaustively listed and narrowly defined situations, where EN 22  EN the use is stri ctly necessary to achieve a substantial public interest, the importance of \n",
      "which outweighs the risks. Those situations involve the search for potential victims of \n",
      "crime, including missing children; certain threats to the life or physical safety of \n",
      "natural persons or of a terrorist attack; and the detection, localisation, identification or \n",
      "prosecution of perpetrators or suspects of the criminal offences referred to in Council \n",
      "Framework Decision 2002/584/JHA38 if those criminal offences are punishable in the \n",
      "Member State concerned by a custodial sentence or a detention order for a maximum \n",
      "period of at least three years and as they are defined in the law of that Member State . \n",
      "Such threshold for the custodial sentence or detention order in accordance with \n",
      "nationa l law contributes to ensure that the offence should be serious enough to \n",
      "potentially justify the use of ‘real -time’ remote biometric identification systems. \n",
      "Moreover, of the 32 criminal offences listed in the Council Framework Decision \n",
      "2002/584/JHA, some a re in practice likely to be more relevant than others, in that the \n",
      "recourse to ‘real-time’ remote biometric identification  will foreseeably be necessary \n",
      "and proportionate to highly varying degrees for the practical pursuit of the detection, \n",
      "localisation, i dentification or prosecution of a perpetrator or suspect of the different \n",
      "criminal offences listed and having regard to the likely differences in  the seriousness, \n",
      "probability and scale of the harm or possible negative consequences.  \n",
      "(20) In order to ensure that those systems are used in a responsible and proportionate \n",
      "manner, it is also important to establish that, in each of those three exhaustively listed \n",
      "and narrowly defined situations, certain elements should be taken into account, in \n",
      "particular as regar ds the nature of the situation giving rise to the request and the \n",
      "consequences of the use for the rights and freedoms of all persons concerned  and the \n",
      "safeguards and conditions provided for with the use. In addition, the use of ‘real -time’ \n",
      "remote biometric identification systems in publicly accessible spaces for the purpose \n",
      "of law enforcement should be subject to appropriate limits in time and space, hav ing \n",
      "regard in particular to the evidence or indications regarding the threats, the victims or \n",
      "perpetrator.  The reference database of persons should be appropriate for each use case \n",
      "in each of the three situations mentioned above.  \n",
      "(21) Each use of a ‘real -time’ remote biometric identification system in publicly accessible \n",
      "spaces for the purpose of law enforcement should be subject to an express and specific \n",
      "authorisation by a judicial authority or by an independent administrative authority of a \n",
      "Member State.  Such authorisation should in principle be obtained prior to the use, \n",
      "except in duly justified situations of urgency, that is, situations where the need to use \n",
      "the systems in question is such as to make it effectively and objectively impossible to \n",
      "obtain a n authorisation before commencing the use. In such situations of urgency, the \n",
      "use should be restricted to the absolute minimum necessary and be subject to \n",
      "appropriate safeguards and conditions, as determined in national law and specified in \n",
      "the context of each individual urgent use case by the law enforcement authority itself. \n",
      "In addition, the law enforcement authority should in such situations seek to obtain an \n",
      "authorisation as soon as possible, whilst providing the reasons for not having been able \n",
      "to requ est it earlier.  \n",
      "(22) Furthermore, it is appropriate to provide, within the exhaustive framework set by this \n",
      "Regulation that such use in the territory of a Member State in accordance with this \n",
      "Regulation should only be possible where and in as far as the M ember State in \n",
      "question has decided to expressly provide for the possibility to authorise such use in its \n",
      "                                                 \n",
      "38 Council Framework Decision 2002/584/JHA of 13 June 2002 on the European arrest warrant and the \n",
      "surrender procedures between Member States ( OJ L 190, 18.7.2002, p. 1).  EN 23  EN detailed rules of national law. Consequently, Member States remain free under this \n",
      "Regulation not to provide for such a possibility at all or to only provide for such a \n",
      "possibility in respect of some of the objectives capable of justifying authorised use \n",
      "identified in this Regulation.  \n",
      "(23) The use of AI systems for ‘real -time’ remote biometric identification of natural \n",
      "persons in publicly accessible spa ces for the purpose of law enforcement necessarily \n",
      "involves the processing of biometric data. The rules of this Regulation that prohibit, \n",
      "subject to certain exceptions, such use, which are based on Article 16 TFEU, should \n",
      "apply as lex specialis  in respect of the rules on the processing of biometric data \n",
      "contained in Article 10 of Directive (EU) 2016/680, thus regulating such use and the \n",
      "processing of biometric data involved in an exhaustive manner. Therefore, such use \n",
      "and processing should only be possible in as far as it is compatible with the framework \n",
      "set by this Regulation, without there being scope, outside that framework, for the \n",
      "competent authorities, where they act for purpose of law enforcement, to use such \n",
      "systems and process such data in connectio n thereto on the grounds listed in Article 10 \n",
      "of Directive (EU) 2016/680. In this context, this Regulation is not intended to provide \n",
      "the legal basis for the processing of personal data under Article 8 of Directive \n",
      "2016/680. However , the use of ‘real -time’  remote biometric identification systems in \n",
      "publicly accessible spaces for purposes other than law enforcement, including by \n",
      "competent authorities, should not be covered by the specific framework regarding such \n",
      "use for the purpose of law enforcement set by  this Regulation. Such use for purposes \n",
      "other than law enforcement should therefore not be subject to the requirement of an \n",
      "authorisation under this Regulation  and the applicable detailed rules of national law \n",
      "that may give effect to it.  \n",
      "(24) Any processin g of biometric data and other personal data involved in the use of AI \n",
      "systems for biometric identification, other than in connection to the use of ‘real -time’ \n",
      "remote biometric identification systems in publicly accessible spaces for the purpose \n",
      "of law enfo rcement as regulated by this Regulation, including where those systems are \n",
      "used by competent authorities in publicly accessible spaces for other purposes than \n",
      "law enforcement, should continue to comply with all requirements resulting from \n",
      "Article 9(1) of R egulation (EU) 2016/679, Article 10(1) of Regulation (EU) \n",
      "2018/1725 and Article 10 of Directive (EU) 2016/680, as applicable.   \n",
      "(25) In accordance with Article 6a of Protocol No 21 on the position of the United \n",
      "Kingdom and Ireland in respect of the area of freedom, security and justice, as \n",
      "annexed to the TEU and to the TFEU, Ireland is not bound by the rules laid down in \n",
      "Article 5(1), point (d), (2) and (3) of this Regulation adopted on the basis of Article 16 \n",
      "of the TFEU which relate to the processing of pe rsonal data by the Member States \n",
      "when carrying out activities falling within the scope of Chapter 4 or Chapter 5 of Title \n",
      "V of Part Three of the TFEU, where Ireland is not bound by the rules governing the \n",
      "forms of judicial cooperation in criminal matters o r police cooperation which require \n",
      "compliance with the provisions laid down on the basis of Article 16 of the TFEU.   \n",
      "(26) In accordance with Articles 2 and 2a of Protocol No 22 on the position of Denmark, \n",
      "annexed to the TEU and TFEU, Denmark is not bound b y rules laid down in Article \n",
      "5(1), point (d), (2) and (3) of this Regulation adopted on the basis of Article 16 of the \n",
      "TFEU, or subject to their application, which relate to the processing of personal data \n",
      "by the Member States when carrying out activities falling within the scope of Chapter \n",
      "4 or Chapter 5 of Title V of Part Three of the TFEU.   EN 24  EN (27) High -risk AI systems should only be placed on the Union market or put into service if \n",
      "they comply with certain mandatory requirements. Those requirements should ensure \n",
      "that high -risk AI systems available in the Union or whose output is otherwise used in \n",
      "the Union do not pose unacceptable risks to important Union public interests  as \n",
      "recognised and protected by Union law . AI systems identified as high -risk should be  \n",
      "limited to those that have a significant harmful impact on the health, safety and \n",
      "fundamental rights of persons in the Union and such limitation minimises any \n",
      "potential restriction to international trade, if any.  \n",
      "(28) AI systems could produce adverse outc omes to health and safety of persons, in \n",
      "particular when such systems operate as components of products. Consistently with \n",
      "the objectives of Union harmonisation legislation to facilitate the free movement of \n",
      "products in the internal market and to ensure th at only safe and otherwise compliant \n",
      "products find their way into the market, it is important that the safety risks that may be \n",
      "generated by a product as a whole due to its digital components, including AI systems, \n",
      "are duly prevented and mitigated. For ins tance, increasingly autonomous robots, \n",
      "whether in the context of manufacturing or personal assistance and care should be able \n",
      "to safely operate and performs their functions in complex environments. Similarly, in \n",
      "the health sector where the stakes for life and health are particularly high, increasingly \n",
      "sophisticated diagnostics systems and systems supporting human decisions should be \n",
      "reliable and accurate. The extent of the adverse impact caused by the AI system on the \n",
      "fundamental rights protected by the Cha rter is of particular relevance when classifying \n",
      "an AI system as high -risk. Those rights include the right to human dignity, respect for \n",
      "private and family life, protection of personal data, freedom of expression and \n",
      "information, freedom of assembly and of  association, and non -discrimination, \n",
      "consumer protection, workers’ rights,  rights of persons with disabilities, right to an \n",
      "effective remedy and to a fair trial, right of defence and the presumption of innocence, \n",
      "right to good administration.  In addition to those rights, it is important to highlight that \n",
      "children have specific rights as enshrined in Article 24 of the EU Charter and in the \n",
      "United Nations Convention on the Rights of the Child (further elaborated in the \n",
      "UNCRC General Comment No. 25 as regards  the digital environment), both of which \n",
      "require consideration of the children’s vulnerabilities and provision of such protection \n",
      "and care as necessary for their well -being.  The fundamental right to a high level of \n",
      "environmental protection enshrined in the  Charter and implemented in Union policies \n",
      "should also be considered when assessing the severity of the harm that an AI system \n",
      "can cause, including in relation to the health and safety of persons.  \n",
      "(29) As regards high -risk AI systems that are safety compon ents of products or systems, or \n",
      "which are themselves products or systems falling within the scope of Regulation (EC) \n",
      "No 300/2008 of the European Parliament and of the Council39, Regulation (EU) No \n",
      "167/2013  of the European Parliament and of the Council40, Reg ulation (EU) No \n",
      "168/2013  of the European Parliament and of the Council41, Directive 2014/90/EU  of \n",
      "                                                 \n",
      "39 Regulation (EC) No 300/2008 of the European Parliament and of the Council of 11 March 2008 on \n",
      "common rules in the field of civil aviation security and repealing Regulation (EC) No 2320/2002 (OJ L \n",
      "97, 9.4.2008, p. 72).  \n",
      "40 Regulation (EU) No 167/2013 of the European Parliament and of the Council of 5 February 2013 on the \n",
      "approval and market surveillance of agricultural and forestry vehicles (OJ L 60, 2.3.2013, p. 1).  \n",
      "41 Regulation (EU) No 168/2013 of the European Parliament and of the Council of 15 January 2013 on the \n",
      "approval and market surveillance of two - or three -wheel vehicles and quadricycles (OJ L 60, 2.3.2013, \n",
      "p. 52).  EN 25  EN the European Parliament and of the Council42, Directive (EU) 2016/797  of the \n",
      "European Parliament and of the Council43, Regulation (EU) 2018/858  of the European \n",
      "Parliament and of the Council44, Regulation (EU) 2018/1139 of the European \n",
      "Parliament and of the Council45, and Regulation (EU) 2019/2144  of the European \n",
      "Parliament and of the Council46, it is appropriate to amend those acts to ensure that the \n",
      "Commission take s into account, on the basis of the technical and regulatory \n",
      "specificities of each sector, and without interfering with existing governance, \n",
      "conformity assessment and enforcement mechanisms and authorities established \n",
      "therein, the mandatory requirements fo r high -risk AI systems laid down in this \n",
      "Regulation when adopting any relevant future delegated or implementing acts on the \n",
      "basis of those acts.  \n",
      "(30) As regards AI systems that are safety components of products, or which are \n",
      "themselves products, falling wi thin the scope of certain Union harmonisation \n",
      "legislation, it is appropriate to classify them as high -risk under this Regulation if the \n",
      "product in question undergoes the conformity assessment procedure with a third -party \n",
      "conformity assessment body pursuant  to that relevant Union harmonisation legislation. \n",
      "In particular, such products are machinery, toys, lifts, equipment and protective \n",
      "systems intended for use in potentially explosive atmospheres, radio equipment, \n",
      "pressure equipment, recreational craft equi pment, cableway installations,  appliances \n",
      "burning gaseous fuels, medical devices, and in vitro diagnostic medical devices.  \n",
      "(31) The classification of an AI system as high -risk pursuant to this Regulation should not \n",
      "necessarily mean that the product whose s afety component is the AI system, or the AI \n",
      "system itself as a product, is considered ‘high -risk’ under the criteria established in the \n",
      "relevant Union harmonisation legislation that applies to the product. This is notably \n",
      "the case for Regulation (EU) 2017/ 745 of the European Parliament and of the \n",
      "                                                 \n",
      "42 Directive 2014/90/EU of the European Parliament and of the Council of 23 July 2014 on marine \n",
      "equipment and repealing Council Directive 96/98/EC (OJ L 257, 28.8.2014, p. 146).  \n",
      "43 Directive (EU) 2016/797 of the Europ ean Parliament and of the Council of 11 May 2016 on the \n",
      "interoperability of the rail system within the European Union (OJ L 138, 26.5.2016, p. 44).  \n",
      "44 Regulation (EU) 2018/858 of the European Parliament and of the Council of 30 May 2018 on the \n",
      "approval and market surveillance of motor vehicles and their trailers, and of systems, components and \n",
      "separate technical units intended for such vehicles, amending Regulations (EC) No 715/2007 and (EC) \n",
      "No 595/2009 and repealing Directive 2007/46/EC (OJ L 151, 14.6.2018 , p. 1).  \n",
      "45 Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common \n",
      "rules in the field of civil aviation and establishing a European Union Aviation Safety Agency, and \n",
      "amending Regulations (EC) No 2111/2005, (EC) No 1 008/2008, (EU) No 996/2010, (EU) No 376/2014 \n",
      "and Directives 2014/30/EU and 2014/53/EU of the European Parliament and of the Council, and \n",
      "repealing Regulations (EC) No 552/2004 and (EC) No 216/2008 of the European Parliament and of the \n",
      "Council and Council R egulation (EEC) No 3922/91 (OJ L 212, 22.8.2018, p. 1).  \n",
      "46 Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November 2019 on \n",
      "type-approval requirements for motor vehicles and their trailers, and systems, components and separate \n",
      "technical units intended for such vehicles, as regards their general safety and the protection of vehicle \n",
      "occupants and vulnerable road users, amending Regulation (EU) 2018/858 of the European Parliament \n",
      "and of the Council and repealing Regulations (EC) No  78/2009, (EC) No 79/2009 and (EC) No \n",
      "661/2009 of the European Parliament and of the Council and Commission Regulations (EC) No \n",
      "631/2009, (EU) No 406/2010, (EU) No 672/2010, (EU) No 1003/2010, (EU) No 1005/2010, (EU) No \n",
      "1008/2010, (EU) No 1009/2010, (EU) N o 19/2011, (EU) No 109/2011, (EU) No 458/2011, (EU) No \n",
      "65/2012, (EU) No 130/2012, (EU) No 347/2012, (EU) No 351/2012, (EU) No 1230/2012 and (EU) \n",
      "2015/166 (OJ L 325, 16.12.2019, p. 1).  EN 26  EN Council47 and Regulation (EU) 2017/746 of the European Parliament and of the \n",
      "Council48, where a third -party conformity assessment is provided for medium -risk and \n",
      "high-risk products.   \n",
      "(32) As regards stand -alone AI sy stems, meaning high -risk AI systems other than those that \n",
      "are safety components of products, or which are themselves products, it is appropriate \n",
      "to classify them as high -risk if, in the light of their intended purpose, they pose a high \n",
      "risk of harm to the health and safety or the fundamental rights of persons, taking into \n",
      "account both the severity of the possible harm and its probability of occurrence and \n",
      "they are used in a number of specifically pre -defined areas specified in the Regulation. \n",
      "The identifica tion of those systems is based on the same methodology and criteria \n",
      "envisaged also for any future amendments of the list of high -risk AI systems.  \n",
      "(33) Technical inaccuracies of AI systems intended for the remote biometric identification \n",
      "of natural persons can lead to biased results and entail discriminatory effects. This is \n",
      "particularly relevant when it comes to age, ethnicity, sex or disabilities. Therefore, \n",
      "‘real-time’ and ‘post’ remote biometric identification systems should be classified as \n",
      "high-risk. In view of the risks that they pose, both types of remote biometric \n",
      "identification systems should be subject to specific requirements on logging \n",
      "capabilities and human oversight.   \n",
      "(34) As regards the management and operation of critical infrastructure, it i s appropriate to \n",
      "classify as high -risk the AI systems intended to be used as safety components in the \n",
      "management and operation of road traffic and the supply of water, gas, heating and \n",
      "electricity, since their failure or malfunctioning may put at risk the life and health of \n",
      "persons at large scale and lead to appreciable disruptions in the ordinary conduct of \n",
      "social and economic activities.  \n",
      "(35) AI systems used in education or vocational training, notably for determining access or \n",
      "assigning persons to educa tional and vocational training institutions or to evaluate \n",
      "persons on tests as part of or as a precondition for their education should be considered \n",
      "high-risk, since they may determine the educational and professional course of a \n",
      "person’s life and therefor e affect their ability to secure their livelihood. When \n",
      "improperly designed and used, such systems may violate the right to education and \n",
      "training as well as the right not to be discriminated against and perpetuate historical \n",
      "patterns of discrimination.  \n",
      "(36) AI systems used in employment, workers management and access to self -employment, \n",
      "notably for the recruitment and selection of persons, for making decisions on \n",
      "promotion and termination and for task allocation, monitoring or evaluation of persons \n",
      "in work -related contractual relationships, should also be classified as high -risk, since \n",
      "those systems may appreciably impact future career prospects and livelihoods of these \n",
      "persons. Relevant work -related contractual relationships should involve employees \n",
      "and pe rsons providing services through platforms as referred to in the Commission \n",
      "Work Programme 2021. Such persons should in principle not be considered users \n",
      "within the meaning of this Regulation. Throughout the recruitment process and in the \n",
      "                                                 \n",
      "47 Regulation (EU) 2017/745 of the European Parliament and of the Council  of 5 April 2017 on medical \n",
      "devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No \n",
      "1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (OJ L 117, 5.5.2017, p. 1).  \n",
      "48 Regulation (EU) 2017/746 of the Europea n Parliament and of the Council of 5 April 2017 on in vitro \n",
      "diagnostic medical devices and repealing Directive 98/79/EC and Commission Decision 2010/227/EU \n",
      "(OJ L 117, 5.5.2017, p. 176).  EN 27  EN evaluation, promo tion, or retention of persons in work -related contractual \n",
      "relationships, such systems may perpetuate historical patterns of discrimination, for \n",
      "example against women, certain age groups, persons with disabilities, or persons of \n",
      "certain racial or ethnic ori gins or sexual orientation. AI systems used to monitor the \n",
      "performance and behaviour of these persons may also impact their rights to data \n",
      "protection and privacy.  \n",
      "(37) Another area in which the use of AI systems deserves special consideration is the \n",
      "acces s to and enjoyment of certain essential private and public services and benefits \n",
      "necessary for people  to fully participate in society or to improve one’s standard of \n",
      "living . In particular, AI systems used to evaluate the credit score or creditworthiness of  \n",
      "natural persons should be classified as high -risk AI systems, since they determine \n",
      "those persons’ access to financial resources or essential services such as housing, \n",
      "electricity, and telecommunication services. AI systems used for this purpose may lead \n",
      "to discrimination of persons or groups and perpetuate historical patterns of \n",
      "discrimination, for example based on racial or ethnic origins, disabilities, age, sexual \n",
      "orientation, or create new forms of discriminatory impacts. Considering the very \n",
      "limited sc ale of the impact and the available alternatives on the market, it is \n",
      "appropriate to exempt AI systems for the purpose of creditworthiness assessment and \n",
      "credit scoring when put into service by small -scale providers for their own use. \n",
      "Natural persons apply ing for or receiving public assistance benefits and services from \n",
      "public authorities are typically dependent on those benefits and services and in a \n",
      "vulnerable position in relation to the responsible authorities. If AI systems are used for \n",
      "determining whet her such benefits and services should be denied, reduced, revoked or \n",
      "reclaimed by authorities, they may have a significant impact on persons’ livelihood \n",
      "and may infringe their fundamental rights, such as the right to social protection, non -\n",
      "discrimination, human dignity or an effective remedy. Those systems should therefore \n",
      "be classified as high -risk. Nonetheless, this Regulation should not hamper the \n",
      "development and use of innovative approaches in the public administration, which \n",
      "would stand to benefit from  a wider use of compliant and safe AI systems, provided \n",
      "that those systems do not entail a high risk to legal and natural persons. Finally, AI \n",
      "systems used to dispatch or establish priority in the dispatching of emergency first \n",
      "response services should als o be classified as high -risk since they make decisions in \n",
      "very critical situations for the life and health of persons and their property.  \n",
      "(38) Actions by law enforcement authorities involving certain uses of AI systems are \n",
      "characterised by a significant de gree of power imbalance and may lead to surveillance, \n",
      "arrest or deprivation of a natural person’s liberty as well as other adverse impacts on \n",
      "fundamental rights guaranteed in the Charter. In particular, if the AI system is not \n",
      "trained with high quality dat a, does not meet adequate requirements in terms of its \n",
      "accuracy or robustness, or is not properly designed and tested before being put on the \n",
      "market or otherwise put into service, it may single out people in a discriminatory or \n",
      "otherwise incorrect or unjus t manner. Furthermore, the exercise of important \n",
      "procedural fundamental rights, such as the right to an effective remedy and to a fair \n",
      "trial as well as the right of defence and the presumption of innocence, could be \n",
      "hampered, in particular, where such AI s ystems are not sufficiently transparent, \n",
      "explainable and documented.  It is therefore appropriate to classify as high -risk a \n",
      "number of AI systems intended to be used in the law enforcement context where \n",
      "accuracy, reliability and transparency is particularly  important to avoid adverse \n",
      "impacts, retain public trust and ensure accountability and effective redress. In view of \n",
      "the nature of the activities in question and the risks relating thereto, those high -risk AI \n",
      "systems should include in particular AI systems  intended to be used by law EN 28  EN enforcement authorities for individual risk assessments, polygraphs and similar tools \n",
      "or to detect the emotional state of natural person, to detect ‘deep fakes’,  for the \n",
      "evaluation of the reliability of evidence in criminal proc eedings, for predicting the \n",
      "occurrence or reoccurrence of an actual or potential criminal offence based on \n",
      "profiling of natural persons, or assessing personality traits and characteristics or past \n",
      "criminal behaviour of natural persons or groups, for profil ing in the course of \n",
      "detection, investigation or prosecution of criminal offences, as well as for  crime \n",
      "analytics regarding natural persons.  AI systems specifically intended to be used for \n",
      "administrative proceedings by tax and customs authorities should no t be considered \n",
      "high-risk AI systems used by law enforcement authorities for the purposes of \n",
      "prevention, detection, investigation and prosecution of criminal offences.   \n",
      "(39) AI systems used in migration, asylum and border control management affect people \n",
      "who are often in particularly vulnerable position and who are dependent on the \n",
      "outcome of the actions of the competent public authorities. The accuracy, non -\n",
      "discriminatory nature and transparency of the AI systems used in those contexts are \n",
      "therefore partic ularly important to guarantee the respect of the fundamental rights of \n",
      "the affected persons, notably their rights to free movement, non -discrimination, \n",
      "protection of private life and personal data, international protection and good \n",
      "administration. It is th erefore appropriate to classify as high -risk AI systems intended \n",
      "to be used by the competent public authorities charged with tasks in the fields of \n",
      "migration, asylum and border control management as polygraphs and similar tools or \n",
      "to detect the emotional s tate of a natural person; for assessing certain risks posed by \n",
      "natural persons entering the territory of a Member State or applying for visa or \n",
      "asylum;  for verifying the authenticity of the relevant documents of natural persons; for \n",
      "assisting competent pub lic authorities for the examination of applications for asylum, \n",
      "visa and residence permits and associated complaints with regard to the objective to \n",
      "establish the eligibility of the natural persons applying for a status.  AI systems in the \n",
      "area of migration , asylum and border control management covered by this Regulation \n",
      "should comply with the relevant procedural requirements set by the Directive \n",
      "2013/32/EU  of the European Parliament and of the Council49, the Regulation (EC) No \n",
      "810/2009 of the European Parliament and of the Council50 and other relevant \n",
      "legislation.  \n",
      "(40) Certain AI systems  intended for the administration of justice and democratic processes  \n",
      "should be classified as high -risk, considering their potentially significant impact on \n",
      "democracy, rul e of law, individual freedoms as well as the right to an effective remedy \n",
      "and to a fair trial. In particular, to address the risks of potential biases, errors and \n",
      "opacity, it is appropriate to qualify as high -risk AI systems intended to assist judicial \n",
      "authorities in researching and interpreting facts and the law and in applying the law to \n",
      "a concrete set of facts. Such qualification should not extend, however, to AI systems \n",
      "intended for purely ancillary administrative activities that do not affect the actua l \n",
      "administration of justice in individual cases, such as anonymisation or \n",
      "pseudonymisation of judicial decisions, documents or data, communication between \n",
      "personnel, administrative tasks or allocation of resources.  \n",
      "                                                 \n",
      "49  Directive 2013/32/EU of the European Parliament and of the Council of 26  June 2013 on common \n",
      "procedures for granting and withdrawing international protection ( OJ L 180, 29.6.2013, p. 60).  \n",
      "50  Regulation (EC) No  810/2009 of the European Parliament and of the Council of 13  July 2009 \n",
      "establishing a Community Code on Visas (Vi sa Code) ( OJ L 243, 15.9.2009, p.  1). \n",
      " EN 29  EN (41) The fact that an AI system is classi fied as high risk under this Regulation should not \n",
      "be interpreted as indicating that the use of the system is necessarily lawful under other \n",
      "acts of Union law or under national law compatible with Union law, such as on the \n",
      "protection of personal data, on t he use of polygraphs and similar tools or other systems \n",
      "to detect the emotional state of natural persons . Any such use should continue to occur \n",
      "solely in accordance with the applicable requirements resulting from the Charter and \n",
      "from the applicable acts of  secondary Union law and national law. This Regulation \n",
      "should not be understood as providing for the legal ground for processing of personal \n",
      "data, including special categories of personal data,  where relevant .  \n",
      "(42) To mitigate the risks from high -risk AI systems placed or otherwise put into service on \n",
      "the Union market for users and affected persons, certain mandatory requirements \n",
      "should apply, taking into account the intended purpose of the use of the system and \n",
      "according to the risk management system to be established by the provider.  \n",
      "(43) Requirements should apply to high -risk AI systems as regards the quality of data sets \n",
      "used, technical documentation and record -keeping, transparency and the provision of \n",
      "informati on to users, human oversight, and robustness, accuracy and cybersecurity. \n",
      "Those requirements are necessary to effectively mitigate the risks for health, safety \n",
      "and fundamental rights, as applicable in the light of the intended purpose of the \n",
      "system, and no  other less trade restrictive measures are reasonably available, thus \n",
      "avoiding unjustified restrictions to trade.  \n",
      "(44) High data quality is essential for the performance of many AI systems, especially \n",
      "when techniques involving the training of models are u sed, with a view to ensure that \n",
      "the high -risk AI system performs as intended and safely and it does not become the \n",
      "source of discrimination prohibited by Union law. High quality training, validation \n",
      "and testing data sets require the implementation of appro priate data governance and \n",
      "management practices. Training, validation and testing data sets should be sufficiently \n",
      "relevant, representative and free of errors and complete in view of the intended \n",
      "purpose of the system. They should also have the appropriate  statistical properties, \n",
      "including as regards the persons or groups of persons on which the high -risk AI \n",
      "system is intended to be used. In particular, training, validation and testing data sets \n",
      "should take into account, to the extent required in the light of their intended purpose, \n",
      "the features, characteristics or elements that are particular to the specific geographical, \n",
      "behavioural or functional setting or context within which the AI system is intended to \n",
      "be used.  In order to protect the right of others f rom the discrimination that might result \n",
      "from the bias in AI systems, the providers shouldbe able to process also special \n",
      "categories of personal data, as a matter of substantial public interest, in order to ensure \n",
      "the bias monitoring, detection and correct ion in relation to high -risk AI systems.  \n",
      "(45) For the development of high -risk AI systems, certain actors, such as providers, \n",
      "notified bodies and other relevant entities, such as digital innovation hubs, testing \n",
      "experimentation facilities and researchers, should be able to access and use high \n",
      "quality datasets within their respective fields of activities which are related to this \n",
      "Regulation. European common data spaces established by the Commission and the \n",
      "facilitation of data sharing between businesses and with government in the public \n",
      "interest will be instrumental to provide trustful, accountable and non -discriminatory \n",
      "access to high quality data for the training, validation and testing of AI systems. For \n",
      "example, in health, the European health data space w ill facilitate non -discriminatory \n",
      "access to health data and the training of artificial intelligence algorithms on those \n",
      "datasets, in a privacy -preserving, secure, timely, transparent and trustworthy manner, \n",
      "and with an appropriate institutional governance.  Relevant competent authorities, EN 30  EN including sectoral ones, providing or supporting the access to data may also support \n",
      "the provision of high -quality data for the training, validation and testing of AI systems.  \n",
      "(46) Having information on how high -risk AI sys tems have been developed and how they \n",
      "perform throughout their lifecycle is essential to verify compliance with the \n",
      "requirements under this Regulation. This requires keeping records and the availability \n",
      "of a technical documentation, containing information which is necessary to assess the \n",
      "compliance of the AI system with the relevant requirements. Such information should \n",
      "include the general characteristics, capabilities and limitations of the system, \n",
      "algorithms, data, training, testing and validation process es used as well as \n",
      "documentation on the relevant risk management system. The technical documentation \n",
      "should be kept up to date.  \n",
      "(47) To address the opacity that may make certain AI systems incomprehensible to or too \n",
      "complex for natural persons, a certain d egree of transparency should be required for \n",
      "high-risk AI systems. Users should be able to interpret the system output and use it \n",
      "appropriately. High -risk AI systems should therefore be accompanied by relevant \n",
      "documentation and instructions of use and incl ude concise and clear information, \n",
      "including in relation to possible risks to fundamental rights and discrimination, where \n",
      "appropriate.  \n",
      "(48) High -risk AI systems should be designed and developed in such a way that natural \n",
      "persons can oversee their function ing. For this purpose, appropriate human oversight \n",
      "measures should be identified by the provider of the system before its placing on the \n",
      "market  or putting into service . In particular, where appropriate, such measures should \n",
      "guarantee that the system is sub ject to in -built operational constraints that cannot be \n",
      "overridden by the system itself and is responsive to the human operator, and that the \n",
      "natural persons to whom human oversight has been assigned have the necessary \n",
      "competence, training and authority to  carry out that role.   \n",
      "(49) High -risk AI systems should perform consistently throughout their lifecycle and meet \n",
      "an appropriate level of accuracy, robustness and cybersecurity in accordance with the \n",
      "generally acknowledged state of the art. The level of acc uracy and accuracy metrics \n",
      "should be communicated to the users.  \n",
      "(50) The technical robustness is a key requirement for high -risk AI systems. They should \n",
      "be resilient against risks connected to the limitations of the system (e.g. errors, faults, \n",
      "inconsiste ncies, unexpected situations) as well as against malicious actions that may \n",
      "compromise the security of the AI system and result in harmful or otherwise \n",
      "undesirable behaviour. Failure to protect against these risks could lead to safety \n",
      "impacts or negatively  affect the fundamental rights, for example due to erroneous \n",
      "decisions or wrong or biased outputs generated by the AI system.  \n",
      "(51) Cybersecurity plays a crucial role in ensuring that AI systems are resilient against \n",
      "attempts to alter their use, behaviour, performance or compromise their security \n",
      "properties by malicious third parties exploiting the system’s vulnerabilities. \n",
      "Cyberattacks against AI systems can leverage AI specific assets, such as training data \n",
      "sets (e.g. data poisoning) or trained models (e.g . adversarial attacks), or exploit \n",
      "vulnerabilities in the AI system’s digital assets or the underlying ICT infrastructure. \n",
      "To ensure a level of cybersecurity appropriate to the risks, suitable measures should \n",
      "therefore be taken by the providers of high -risk AI systems, also taking into account as \n",
      "appropriate the underlying ICT infrastructure.  EN 31  EN (52) As part of Union harmonisation legislation, rules applicable to the placing on the \n",
      "market, putting into service and use of high -risk AI systems should be laid dow n \n",
      "consistently with Regulation (EC) No 765/2008 of the European Parliament and of the \n",
      "Council51 setting out the requirements for accreditation and the market surveillance of \n",
      "products, Decision No 768/2008/EC of the European Parliament and of the Council52 \n",
      "on a common framework for the marketing of products and Regulation (EU) \n",
      "2019/1020 of the European Parliament and of the Council53 on market surveillance \n",
      "and compliance of products  (‘New Legislative Framework for the marketing of \n",
      "products’).  \n",
      "(53) It is appropr iate that a specific natural or legal person, defined as the provider, takes \n",
      "the responsibility for the placing on the market or putting into service of a high -risk AI \n",
      "system, regardless of whether that natural or legal person is the person who designed \n",
      "or developed the system.  \n",
      "(54) The provider should establish a sound quality management system, ensure the \n",
      "accomplishment of the required conformity assessment procedure, draw up the \n",
      "relevant documentation and establish a robust post -market monitoring system. Public \n",
      "authorities which pu t into service high -risk AI systems for their own use may adopt \n",
      "and implement the rules for the quality management system as part of the quality \n",
      "management system adopted at a national or regional level, as appropriate, taking into \n",
      "account the specificitie s of the sector and the competences and organisation of the \n",
      "public authority in question.  \n",
      "(55) Where a high -risk AI system that is a safety component of a product which is covered \n",
      "by a relevant New Legislative Framework sectorial legislation is not placed  on the \n",
      "market or put into service independently from the product, the manufacturer of the \n",
      "final product as defined under the relevant New Legislative Framework legislation \n",
      "should comply with the obligations of the provider established in this Regulation a nd \n",
      "notably ensure that the AI system embedded in the final product complies with the \n",
      "requirements of this Regulation.  \n",
      "(56) To enable enforcement of this Regulation and create a level -playing field for \n",
      "operators, and taking into account the different forms of making available of digital \n",
      "products, it is important to ensure that, under all circumstances, a person established in \n",
      "the Union can provide authorities with all the necessary information on the compliance \n",
      "of an AI system. Therefore, prior to making the ir AI systems available in the Union, \n",
      "where an importer cannot be identified, providers established outside the Union shall, \n",
      "by written mandate, appoint an authorised representative established in the Union.  \n",
      "(57) In line with New Legislative Framework pri nciples, specific obligations for relevant \n",
      "economic operators, such as importers and distributors, should be set to ensure legal \n",
      "certainty and facilitate regulatory compliance by those relevant operators.  \n",
      "                                                 \n",
      "51 Regulation (EC) No 765/2008 of the European Parliament and of the Council of 9 July 2008 setting out \n",
      "the requirements for accreditation and market surveillance relating to the marketing of products and \n",
      "repealing Reg ulation (EEC) No 339/93 (OJ L 218, 13.8.2008, p. 30).  \n",
      "52 Decision No 768/2008/EC of the European Parliament and of the Council of 9 July 2008 on a common \n",
      "framework for the marketing of products, and repealing Council Decision 93/465/EEC (OJ L 218, \n",
      "13.8.2008 , p. 82).  \n",
      "53 Regulation (EU) 2019/1020 of the European Parliament and of the Council of 20 June 2019 on market \n",
      "surveillance and compliance of products and amending Directive 2004/42/EC and Regulations (EC) No \n",
      "765/2008 and (EU) No 305/2011 (Text with EEA rel evance) (OJ L 169, 25.6.2019, p. 1 –44). EN 32  EN (58) Given the nature of AI systems and the risks t o safety and fundamental rights possibly \n",
      "associated with their use, including as regard the need to ensure proper monitoring of \n",
      "the performance of an AI system in a real -life setting, it is appropriate to set specific \n",
      "responsibilities for users. Users shou ld in particular use high -risk AI systems in \n",
      "accordance with the instructions of use and certain other obligations should be \n",
      "provided for with regard to monitoring of the functioning of the AI systems and with \n",
      "regard to record -keeping, as appropriate.   \n",
      "(59) It is appropriate to envisage that the user of the AI system should be the natural or \n",
      "legal person, public authority, agency or other body under whose authority the AI \n",
      "system is operated except where the use is made in the course of a personal non -\n",
      "profes sional activity.  \n",
      "(60) In the light of the complexity of the artificial intelligence value chain, relevant third \n",
      "parties, notably the ones involved in the sale and the supply of software, software tools \n",
      "and components, pre -trained models and data, or provid ers of network services, should \n",
      "cooperate, as appropriate, with providers and users to enable their compliance with the \n",
      "obligations under this Regulation and with competent authorities established under this \n",
      "Regulation.  \n",
      "(61) Standardisation should play a k ey role to provide technical solutions to providers to \n",
      "ensure compliance with this Regulation. Compliance with harmonised standards as \n",
      "defined in Regulation (EU) No 1025/2012 of the European Parliament and of the \n",
      "Council54 should be a means for providers to  demonstrate conformity with the \n",
      "requirements of this Regulation. However, the Commission could adopt common \n",
      "technical specifications in areas where no harmonised standards exist or where they \n",
      "are insufficient.   \n",
      "(62) In order to ensure a high level of trus tworthiness of high -risk AI systems, those \n",
      "systems should be subject to a conformity assessment prior to their placing on the \n",
      "market or putting into service.  \n",
      "(63) It is appropriate that, in order to minimise the burden on operators and avoid any \n",
      "possible d uplication, for high -risk AI systems related to products which are covered by \n",
      "existing Union harmonisation legislation following the New Legislative Framework \n",
      "approach, the compliance of those AI systems with the requirements of this Regulation \n",
      "should be a ssessed as part of the conformity assessment already foreseen under that \n",
      "legislation. The applicability of the requirements of this Regulation should thus not \n",
      "affect the specific logic, methodology or general structure of conformity assessment \n",
      "under the re levant specific New Legislative Framework legislation. This approach is \n",
      "fully reflected in the interplay between this Regulation and the [Machinery \n",
      "Regulation]. While safety risks of AI systems ensuring safety functions in machinery \n",
      "are addressed by the re quirements of this Regulation, certain specific requirements in \n",
      "the [Machinery Regulation] will ensure the safe integration of the AI system into the \n",
      "overall machinery, so as not to compromise the safety of the machinery as a whole. \n",
      "                                                 \n",
      "54 Regulation (EU) No 1025/2012 of the European Parliament and of the Council of 25 October 2012 on \n",
      "European standardisation, amending Council Directives 89/686/EEC and 93/15/EEC and Directives \n",
      "94/9/EC, 94/25/EC, 95/1 6/EC, 97/23/EC, 98/34/EC, 2004/22/EC, 2007/23/EC, 2009/23/EC and \n",
      "2009/105/EC of the European Parliament and of the Council and repealing Council Decision \n",
      "87/95/EEC and Decision No 1673/2006/EC of the European Parliament and of the Council (OJ L 316, \n",
      "14.11. 2012, p. 12).  EN 33  EN The [Machinery Regulati on] applies the same definition of AI system as this \n",
      "Regulation.  \n",
      "(64) Given the more extensive experience of professional pre -market certifiers in the field \n",
      "of product safety and the different nature of risks involved, it is appropriate to limit, at \n",
      "least in an initial phase of application of this Regulation, the scope of application of \n",
      "third -party conformity assessment for high -risk AI systems other than those related to \n",
      "products. Therefore, the conformity assessment of such systems should be carried out \n",
      "as a general rule by the provider under its own responsibility, with the only exception \n",
      "of AI systems intended to be used for the remote biometric identification of persons, \n",
      "for which the involvement of a notified body in the conformity assessment should b e \n",
      "foreseen, to the extent they are not prohibited.  \n",
      "(65) In order to carry out third -party conformity assessment for AI systems intended to be \n",
      "used for the remote biometric identification of persons, notified bodies should be \n",
      "designated under this Regulati on by the national competent authorities, provided they \n",
      "are compliant with a set of requirements, notably on independence, competence and \n",
      "absence of conflicts of interests.  \n",
      "(66) In line with the commonly established notion of substantial modification for products \n",
      "regulated by Union harmonisation legislation, it is appropriate that an AI system \n",
      "undergoes a new conformity assessment whenever a change occurs which may affect \n",
      "the compliance of the system with this Regulation or when the intended purpose of the  \n",
      "system changes. In addition, as regards AI systems which continue to ‘learn’ after \n",
      "being placed on the market or put into service (i.e. they automatically adapt how \n",
      "functions are carried out), it is necessary to provide rules establishing that changes to \n",
      "the algorithm and its performance that have been pre -determined by the provider and \n",
      "assessed at the moment of the conformity assessment should not constitute a \n",
      "substantial modification.   \n",
      "(67) High -risk AI systems should bear the CE marking to indicate thei r conformity with \n",
      "this Regulation so that they can move freely within the internal market. Member States \n",
      "should not create unjustified obstacles to the placing on the market or putting into \n",
      "service of high -risk AI systems that comply with the requirements laid down in this \n",
      "Regulation and bear the CE marking.  \n",
      "(68) Under certain conditions, rapid availability of innovative technologies may be crucial \n",
      "for health and safety of persons and for society as a whole. It is thus appropriate that \n",
      "under exceptional rea sons of public security or protection of life and health of natural \n",
      "persons and the protection of industrial and commercial property, Member States \n",
      "could authorise the placing on the market or putting into service of AI systems which \n",
      "have not undergone a c onformity assessment.  \n",
      "(69) In order to facilitate the work of the Commission and the Member States in the \n",
      "artificial intelligence field as well as to increase the transparency towards the public, \n",
      "providers of high -risk AI systems other than those related t o products falling within \n",
      "the scope of relevant existing Union harmonisation legislation, should be required to \n",
      "register their high -risk AI system in a EU database , to be established and managed by \n",
      "the Commission. The Commission should be the controller of  that database, in \n",
      "accordance with Regulation (EU) 2018/1725 of the European Parliament and of the EN 34  EN Council55. In order to ensure the full functionality of the database, when deployed, the \n",
      "procedure for setting the database should include the elaboration of functional \n",
      "specifications by the Commission and an independent audit report.   \n",
      "(70) Certain AI systems intended to interact with natural persons or to generate content \n",
      "may pose specific risks of impersonation or deception irrespective of whether they \n",
      "qualif y as high -risk or not. In certain circumstances, the use of these systems should \n",
      "therefore be subject to specific transparency obligations without prejudice to the \n",
      "requirements and obligations for high -risk AI systems. In particular, natural persons \n",
      "should  be notified that they are interacting with an AI system, unless this is obvious \n",
      "from the circumstances and the context of use. Moreover, natural persons should be \n",
      "notified when they are exposed to an emotion recognition system or a biometric \n",
      "categorisatio n system. Such information and notifications should be provided in \n",
      "accessible formats for persons with disabilities. Further, users, who use an AI system \n",
      "to generate or manipulate image, audio or video content that appreciably resembles \n",
      "existing persons, p laces or events and would falsely appear to a person to be authentic, \n",
      "should disclose that the content has been artificially created or manipulated by \n",
      "labelling the artificial intelligence output accordingly and disclosing its artificial \n",
      "origin.  \n",
      "(71) Artificial intelligence is a rapidly developing family of technologies that requires \n",
      "novel forms of regulatory oversight and a safe space for experimentation, while \n",
      "ensuring responsible innovation and integration of appropriate safeguards and risk \n",
      "mitigati on measures. To ensure a legal framework that is innovation -friendly, future -\n",
      "proof and resilient to disruption, national competent authorities from one or more \n",
      "Member States should be encouraged to establish artificial intelligence regulatory \n",
      "sandboxes to facilitate the development and testing of innovative AI systems under \n",
      "strict regulatory oversight before these systems are placed on the market or otherwise \n",
      "put into service.  \n",
      "(72) The objectives of the regulatory sandboxes should be to foster AI innovatio n by \n",
      "establishing a controlled experimentation and testing environment in the development \n",
      "and pre -marketing phase with a view to ensuring compliance of the innovative AI \n",
      "systems  with this Regulation and other relevant Union and Member States legislation; \n",
      "to enhance legal certainty for innovators and the competent authorities’ oversight and \n",
      "understanding of the opportunities, emerging risks and the impacts of AI use, and to \n",
      "accelerate access to markets, including by removing barriers for small and medium \n",
      "enterprises (SMEs) and start -ups. To ensure uniform implementation across the Union \n",
      "and economies of scale, it is appropriate to establish common rules for the regulatory \n",
      "sandboxes’ implementation and a framework for cooperation between the relevant \n",
      "authorit ies involved in the supervision of the sandboxes. This Regulation should \n",
      "provide the legal basis for the use of personal data collected for other purposes for \n",
      "developing certain AI systems in the public interest within the AI regulatory sandbox, \n",
      "in line wi th Article 6(4) of Regulation (EU) 2016/679, and Article 6 of Regulation \n",
      "(EU) 2018/1725, and without prejudice to Article 4(2) of Directive (EU) 2016/680. \n",
      "Participants in the sandbox should ensure appropriate safeguards and cooperate with \n",
      "the competent aut horities, including by following their guidance and acting \n",
      "                                                 \n",
      "55 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the \n",
      "protection of natural persons with regard to the processing of personal data and on the free movement of \n",
      "such data, and repealing Directive 95/46 /EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, \n",
      "p. 1).  EN 35  EN expeditiously and in good faith to mitigate any high -risks to safety and fundamental \n",
      "rights that may arise during the development and experimentation in the sandbox. The \n",
      "conduct of the participants  in the sandbox should be taken into account when \n",
      "competent authorities decide whether to impose an administrative fine under Article \n",
      "83(2) of Regulation 2016/679 and Article 57 of Directive 2016/680.  \n",
      "(73) In order to promote and protect innovation, it is  important that the interests of small -\n",
      "scale providers and users of AI systems are taken into particular account. To this \n",
      "objective, Member States should develop initiatives, which are targeted at those \n",
      "operators, including on awareness raising and informa tion communication. Moreover, \n",
      "the specific interests and needs of small -scale providers shall be taken into account \n",
      "when Notified Bodies set conformity assessment fees.  Translation costs related to \n",
      "mandatory documentation and communication with authorities  may constitute a \n",
      "significant cost for providers and other operators, notably those of a smaller scale. \n",
      "Member States should possibly ensure that one of the languages determined and \n",
      "accepted by them for relevant providers’ documentation and for communicati on with \n",
      "operators is one which is broadly understood by the largest possible number of cross -\n",
      "border users.  \n",
      "(74) In order to minimise the risks to implementation resulting from lack of knowledge and \n",
      "expertise in the market as well as to facilitate complian ce of providers and notified \n",
      "bodies with their obligations under this Regulation, the AI -on demand platform, the \n",
      "European Digital Innovation Hubs and the Testing and Experimentation Facilities \n",
      "established by the Commission and the Member States at national  or EU level should \n",
      "possibly contribute to the implementation of this Regulation. Within their respective \n",
      "mission and fields of competence, they may provide in particular technical and \n",
      "scientific support to providers and notified bodies.  \n",
      "(75) It is approp riate that the Commission facilitates, to the extent possible, access to \n",
      "Testing and Experimentation Facilities to bodies, groups or laboratories established or \n",
      "accredited pursuant to any relevant Union harmonisation legislation and which fulfil \n",
      "tasks in t he context of conformity assessment of products or devices covered by that \n",
      "Union harmonisation legislation. This is notably the case for expert panels, expert \n",
      "laboratories and reference laboratories in the field of medical devices pursuant to \n",
      "Regulation (E U) 2017/745 and Regulation (EU) 2017/746.  \n",
      "(76) In order to facilitate a smooth, effective and harmonised implementation of this \n",
      "Regulation a European Artificial Intelligence Board should be established. The Board \n",
      "should be responsible for a number of advi sory tasks, including issuing opinions, \n",
      "recommendations, advice or guidance on matters related to the implementation of this \n",
      "Regulation, including on technical specifications or existing standards regarding the \n",
      "requirements established in this Regulation a nd providing advice to and assisting the \n",
      "Commission on specific questions related to artificial intelligence.  \n",
      "(77) Member States hold a key role in the application and enforcement of this Regulation. \n",
      "In this respect, each Member State should designate one  or more national competent \n",
      "authorities for the purpose of supervising the application and implementation of this \n",
      "Regulation. In order to increase organisation efficiency on the side of Member States \n",
      "and to set an official point of contact vis -à-vis the pu blic and other counterparts at \n",
      "Member State and Union levels, in each Member State one national authority should \n",
      "be designated as national supervisory authority.  \n",
      "(78) In order to ensure that providers of high -risk AI systems can take into account the \n",
      "exper ience on the use of high -risk AI systems for improving their systems and the EN 36  EN design and development process or can take any possible corrective action in a timely \n",
      "manner, all providers should have a post -market monitoring system in place. This \n",
      "system is al so key to ensure that the possible risks emerging from AI systems which \n",
      "continue to ‘learn’ after being placed on the market or put into service can be more \n",
      "efficiently and timely addressed. In this context, providers should also be required to \n",
      "have a syst em in place to report to the relevant authorities any serious incidents or any \n",
      "breaches to national and Union law protecting fundamental rights resulting from the \n",
      "use of their AI systems.   \n",
      "(79) In order to ensure an appropriate and effective enforcement of  the requirements and \n",
      "obligations set out by this Regulation, which is Union harmonisation legislation, the \n",
      "system of market surveillance and compliance of products established by Regulation \n",
      "(EU) 2019/1020 should apply in its entirety. Where necessary for their mandate, \n",
      "national public authorities or bodies, which supervise the application of Union law \n",
      "protecting fundamental rights, including equality bodies, should also have access to \n",
      "any documentation created under this Regulation.  \n",
      "(80) Union legislation on financial services includes internal governance and risk \n",
      "management rules and requirements which are applicable to regulated financial \n",
      "institutions in the course of provision of those services, including when they make use \n",
      "of AI systems. In order to ens ure coherent application and enforcement of the \n",
      "obligations under this Regulation and relevant rules and requirements of the Union \n",
      "financial services legislation, the authorities responsible for the supervision and \n",
      "enforcement of the financial services leg islation,  including where applicable the \n",
      "European Central Bank , should be designated as competent authorities for the purpose \n",
      "of supervising the implementation of this Regulation, including for market \n",
      "surveillance activities, as regards AI systems provided  or used by regulated and \n",
      "supervised financial institutions. To further enhance the consistency between this \n",
      "Regulation and the rules applicable to credit institutions regulated under Directive \n",
      "2013/36/EU of the European Parliament and of the Council56, it is also appropriate to \n",
      "integrate the conformity assessment procedure and some of the providers’ procedural \n",
      "obligations in relation to risk management, post marketing monitoring and \n",
      "documentation into the existing obligations and procedures under Directive \n",
      "2013/36/EU. In order to avoid overlaps, limited derogations should also be envisaged \n",
      "in relation to the quality management system of providers and the monitoring \n",
      "obligation placed on users of high -risk AI systems to the extent that these apply to \n",
      "credit in stitutions regulated by Directive 2013/36/EU.  \n",
      "(81) The development of AI systems other than high -risk AI systems in accordance with \n",
      "the requirements of this Regulation may lead to a larger uptake of trustworthy artificial \n",
      "intelligence in the Union. Provide rs of non -high-risk AI systems should be encouraged \n",
      "to create codes of conduct intended to foster the voluntary application of the \n",
      "mandatory requirements applicable to high -risk AI systems. Providers should also be \n",
      "encouraged to apply on a voluntary basis additional requirements related, for example, \n",
      "to environmental sustainability, accessibility to persons with disability, stakeholders’ \n",
      "participation in the design and development of AI systems, and diversity of the \n",
      "development teams. The Commission may dev elop initiatives, including of a sectorial \n",
      "                                                 \n",
      "56 Directive 2013/36/EU of the European Parliament and of the Council of 26 June 2013 on access to the \n",
      "activity of credit institutions and the prudential supervision of credit institutions  and investment firms, \n",
      "amending Directive 2002/87/EC and repealing Directives 2006/48/EC and 2006/49/EC (OJ L 176, \n",
      "27.6.2013, p. 338).  EN 37  EN nature, to facilitate the lowering of technical barriers hindering cross -border exchange \n",
      "of data for AI development, including on data access infrastructure, semantic and \n",
      "technical interoperability of different ty pes of data.  \n",
      "(82) It is important that AI systems related to products that are not high -risk in accordance \n",
      "with this Regulation and thus are not required to comply with the requirements set out \n",
      "herein are nevertheless safe when placed on the market or put into service. To \n",
      "contribute to this objective,  the Directive 2001/95/EC of the European Parliament and \n",
      "of the Council57  would apply as a safety net.  \n",
      "(83) In order to ensure trustful and constructive cooperation of competent authorities on \n",
      "Union and nationa l level, all parties involved in the application of this Regulation \n",
      "should respect the confidentiality of information and data obtained in carrying out \n",
      "their tasks.  \n",
      "(84) Member States should take all necessary measures to ensure that the provisions of thi s \n",
      "Regulation are implemented, including by laying down effective, proportionate and \n",
      "dissuasive penalties for their infringement. For certain specific infringements, Member \n",
      "States should take into account the margins and criteria set out in this Regulation.  The \n",
      "European Data Protection Supervisor should have the power to impose fines on Union \n",
      "institutions, agencies and bodies falling within the scope of this Regulation.  \n",
      "(85) In order to ensure that the regulatory framework can be adapted where necessary, the  \n",
      "power to adopt acts in accordance with Article 290 TFEU should be delegated to the \n",
      "Commission to amend the techniques and approaches referred to in Annex I to define \n",
      "AI systems, the Union harmonisation legislation listed in Annex II, the high -risk AI \n",
      "systems listed in Annex III, the provisions regarding technical documentation listed in \n",
      "Annex IV, the content of the EU declaration of conformity in Annex V, the provisions \n",
      "regarding the conformity assessment procedures in Annex VI and VII and the \n",
      "provisions e stablishing the high -risk AI systems to which the conformity assessment \n",
      "procedure based on assessment of the quality management system and assessment of \n",
      "the technical documentation should apply. It is of particular importance that the \n",
      "Commission carry out appropriate consultations during its preparatory work, including \n",
      "at expert level, and that those consultations be conducted in accordance with the \n",
      "principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better \n",
      "Law-Making58. In particula r, to ensure equal participation in the preparation of \n",
      "delegated acts, the European Parliament and the Council receive all documents at the \n",
      "same time as Member States’ experts, and their experts systematically have access to \n",
      "meetings of Commission expert g roups dealing with the preparation of delegated acts.  \n",
      "(86) In order to ensure uniform conditions for the implementation of this Regulation, \n",
      "implementing powers should be conferred on the Commission. Those powers should \n",
      "be exercised in accordance with Regul ation (EU) No 182/2011 of the European \n",
      "Parliament and of the Council59. \n",
      "(87) Since the objective of this Regulation cannot be sufficiently achieved by the Member \n",
      "States and can rather, by reason of the scale or effects of the action, be better achieved \n",
      "                                                 \n",
      "57  Directive 2001/95/EC of the European Parliament and of the Council of 3 December 2001 on general \n",
      "product safety (OJ L 11, 15.1.2002, p. 4).  \n",
      "58 OJ L 123, 12.5.2016, p. 1.  \n",
      "59 Regulation (EU) No 182/2011 of the European Parliament and of the Council of 16 February 2011 \n",
      "laying down the rules and general principles concerning mechanisms for control by the Member States \n",
      "of the Commission's exercise of implementing powers (OJ L 55, 28.2.2011, p.13).  EN 38  EN at Union level, the Union may adopt measures in accordance with the principle of \n",
      "subsidiarity as set out in Article 5 TEU. In accordance with the principle of \n",
      "proportionality as set out in that Article, this Regulation does not go beyond what is \n",
      "necessary in o rder to achieve that objective.  \n",
      "(88) This Regulation should apply from … [ OP – please insert the date established in Art. \n",
      "85]. However, the infrastructure related to the governance and the conformity \n",
      "assessment system should be operational before that date , therefore the provisions on \n",
      "notified bodies and governance structure should apply from … [OP – please insert the \n",
      "date – three months following the entry into force of this Regulation ]. In addition, \n",
      "Member States should lay down and notify to the Commissi on the rules on penalties, \n",
      "including administrative fines, and ensure that they are properly and effectively \n",
      "implemented by the date of application of this Regulation. Therefore the provisions on \n",
      "penalties should apply from [ OP – please insert the date – twelve months following the \n",
      "entry into force of this Regulation ]. \n",
      "(89) The European Data Protection Supervisor and the European Data Protection Board \n",
      "were consulted in accordance with Article 42(2) of Regulation (EU) 2018/1725 and \n",
      "delivered an opinion on [ …]”.  \n",
      "HAVE ADOPTED THIS REGULATION:  \n",
      "TITLE  I \n",
      "GENERAL  PROVISIONS  \n",
      "Article 1  \n",
      "Subject matter  \n",
      "This Regulation lays down:  \n",
      "(a) harmonised rules for the placing on the market, the putting into service and the \n",
      "use of artificial intelligence systems (‘AI systems’) in the Union;  \n",
      "(a) prohibitions of certain artificial intelligence practices;  \n",
      "(b) specific requirements for high -risk AI systems and obligations for operators of \n",
      "such systems;  \n",
      "(c) harmonised transparency rules for AI systems intended to interact with natural \n",
      "persons, emotion recognition systems and biometric categorisation systems, \n",
      "and AI systems used to generate or manipulate image, audio or video content;  \n",
      "(d) rules on market monitoring and surveillance.  \n",
      "Article 2  \n",
      "Scope  \n",
      "1. This Regulation applies to:  \n",
      "(a) providers placing on the market or putting into service AI systems in the \n",
      "Union, irrespective of whether those providers are established within the Union \n",
      "or in a third country;  \n",
      "(b) users of AI systems located within the Union;  EN 39  EN (c) providers and users of AI  systems that are located in a third country, where the \n",
      "output produced by the system is used in the Union;  \n",
      "2. For high -risk AI systems that are safety components of products or systems, or which \n",
      "are themselves products or systems, falling within the scope  of the following acts, \n",
      "only Article 84 of this Regulation shall apply:  \n",
      "(a) Regulation (EC) 300/2008;  \n",
      "(b) Regulation  (EU) No 167/2013;  \n",
      "(c) Regulation (EU) No 168/2013;  \n",
      "(d) Directive 2014/90/EU;  \n",
      "(e) Directive (EU) 2016/797;  \n",
      "(f) Regulation (EU) 2018/858;  \n",
      "(g) Regulation (EU) 201 8/1139;  \n",
      "(h) Regulation (EU) 2019/2144.  \n",
      "3. This Regulation shall not apply to AI systems developed or used exclusively for \n",
      "military purposes.  \n",
      "4. This Regulation shall not apply to public authorities in a third country nor to \n",
      "internati onal organisations falling within the scope of this Regulation pursuant to \n",
      "paragraph 1, where those authorities or organisations use AI systems in the \n",
      "framework of international agreements for law enforcement and judicial cooperation \n",
      "with the Union or with  one or more Member States.  \n",
      "5. This Regulation shall not affect the application of the provisions on the liability of \n",
      "intermediary service providers set out in Chapter II, Section IV of Directive \n",
      "2000/31/EC of the European Parliament and of the Council60 [as to be replaced by  \n",
      "the corresponding provisions of the Digital Services Act ]. \n",
      "Article 3  \n",
      "Definitions  \n",
      "For the purpose of this Regulation, the following definitions apply:  \n",
      "(1) ‘artificial intelligence system’ (AI system) means software that is developed with  one \n",
      "or more of the techniques and approaches listed in Annex I and can, for a given set of \n",
      "human -defined objectives, generate outputs such as content, predictions, \n",
      "recommendations, or decisions influencing the environments they interact with;  \n",
      "(1) ‘provider’ m eans a natural or legal person, public authority, agency or other body \n",
      "that develops an AI system or that has an AI system developed with a view to \n",
      "placing it on the market or putting it into service under its own name or trademark, \n",
      "whether for payment or free of charge;  \n",
      "                                                 \n",
      "60 Directive 2000/31/EC of the European Parliament and of the Council of 8 June 2000 on certain legal \n",
      "aspects of information society services, in particular electronic commerce, in the  Internal Market \n",
      "('Directive on electronic commerce') (OJ L 178, 17.7.2000, p. 1).  EN 40  EN (3) ‘small -scale provider’ means a provider that is a micro or small enterprise within the \n",
      "meaning of Commission Recommendation 2003/361/EC61; \n",
      "(4) ‘user’ means any natural or legal person, public authority, agency or other body \n",
      "using an AI s ystem under its authority, except where the AI system is used in the \n",
      "course of a personal non -professional activity;  \n",
      "(5) ‘authorised representative’ means any natural or legal person established in the \n",
      "Union who has received a written mandate from a provid er of an AI system to, \n",
      "respectively, perform and carry out on its behalf the obligations and procedures \n",
      "established by this Regulation;  \n",
      "(6) ‘importer’ means any natural or legal person established in the Union that places on \n",
      "the market or puts into service  an AI system that bears the name or trademark of a \n",
      "natural or legal person established outside the Union;  \n",
      "(7) ‘distributor’ means any natural or legal person in the supply chain, other than the \n",
      "provider or the importer, that makes an AI system available o n the Union market \n",
      "without affecting its properties;  \n",
      "(8) ‘operator’ means the provider, the user, the authorised representative, the importer \n",
      "and the distributor;  \n",
      "(9) ‘placing on the market’ means the first making available of an AI system on the \n",
      "Union market;  \n",
      "(10) ‘making available on the market’ means any supply of an AI system for distribution \n",
      "or use on the Union market in the course of a commercial activity, whether in return \n",
      "for payment or free of charge;  \n",
      "(11) ‘putting into service’ means the supply  of an AI system for first use directly to the \n",
      "user or for own use on the Union market for its intended purpose;  \n",
      "(12) ‘intended purpose’ means the use for which an AI system is intended by the provider, \n",
      "including the specific context and conditions of use,  as specified in the information \n",
      "supplied by the provider in the instructions for use, promotional or sales materials \n",
      "and statements, as well as in the technical documentation;  \n",
      "(13) ‘reasonably foreseeable misuse’ means the use of an AI system in a way tha t is not in \n",
      "accordance with its intended purpose, but which may result from reasonably \n",
      "foreseeable human behaviour or interaction with other systems;  \n",
      "(14) ‘safety component of a product or system’ means a component of a product or of a \n",
      "system which fulfils  a safety function for that product or system or the failure or \n",
      "malfunctioning of which endangers the health and safety of persons or property;  \n",
      "(15) ‘instructions for use’ means the information provided by the provider to inform the \n",
      "user of in particular a n AI system’s intended purpose and proper use, inclusive of the \n",
      "specific geographical, behavioural or functional setting within which the high -risk AI \n",
      "system is intended to be used;  \n",
      "(16) ‘recall of an AI system’ means any measure aimed at achieving the ret urn to the \n",
      "provider of an AI system made available to users;  \n",
      "                                                 \n",
      "61 Commission Recommendation of 6 May 2003 concerning the definition of micro, small and medium -\n",
      "sized enterprises (OJ L 124, 20.5.2003, p. 36).  EN 41  EN (17) ‘withdrawal of an AI system’ means any measure aimed at preventing the \n",
      "distribution, display and offer of an AI system;  \n",
      "(18) ‘performance of an AI system’ means the ability of an AI system t o achieve its \n",
      "intended purpose;  \n",
      "(19) ‘notifying authority’ means the national authority responsible for setting up and \n",
      "carrying out the necessary procedures for the assessment, designation and \n",
      "notification of conformity assessment bodies and for their monitoring;  \n",
      "(20) ‘conformity assessment’ means the process of verifying whether the requirements set \n",
      "out in Title III, Chapter 2 of this Regulation relating to an AI system have been \n",
      "fulfilled;  \n",
      "(21) ‘conformity assessment body’ means a body that performs t hird-party conformity \n",
      "assessment activities, including testing, certification and inspection;  \n",
      "(22) ‘notified body’ means a conformity assessment body designated in accordance with \n",
      "this Regulation and other relevant Union harmonisation legislation;  \n",
      "(23) ‘substantial modification’ means a change to the AI system following its placing on \n",
      "the market or putting into service which affects the compliance of the AI system with \n",
      "the requirements set out in Title III, Chapter 2 of this Regulation or results in a \n",
      "modif ication to the intended purpose for which the AI system has been assessed;  \n",
      "(24) ‘CE marking of conformity’ (CE marking) means a marking by which a provider \n",
      "indicates that an AI system is in conformity with the requirements set out in Title III, \n",
      "Chapter 2 o f this Regulation and other applicable Union legislation harmonising the \n",
      "conditions for the marketing of products (‘Union harmonisation legislation’) \n",
      "providing for its affixing;  \n",
      "(25) ‘post -market monitoring’ means all activities carried out by providers of  AI systems \n",
      "to proactively collect and review experience gained from the use of AI systems they \n",
      "place on the market or put into service for the purpose of identifying any need to \n",
      "immediately apply any necessary corrective or preventive actions;  \n",
      "(26) ‘marke t surveillance authority’ means the national authority carrying out the \n",
      "activities and taking the measures pursuant to Regulation (EU) 2019/1020;  \n",
      "(27) ‘harmonised standard’ means a European standard as defined in Article 2(1)(c) of \n",
      "Regulation (EU) No 1025/ 2012;  \n",
      "(28) ‘common specifications’ means a document, other than a standard, containing \n",
      "technical solutions providing a means to, comply with certain requirements and \n",
      "obligations established under this Regulation;  \n",
      "(29) ‘training data’ means data used for tr aining an AI system through fitting its learnable \n",
      "parameters, including the weights of a neural network;  \n",
      "(30) ‘validation data’ means data used for providing an evaluation of the trained AI \n",
      "system and for tuning its non -learnable parameters and its learnin g process, among \n",
      "other things, in order to prevent overfitting; whereas the validation dataset can be a \n",
      "separate dataset or part of the training dataset, either as a fixed or variable split;  \n",
      "(31) ‘testing data’ means data used for providing an independent evaluation of the trained \n",
      "and validated AI system in order to confirm the expected performance of that system \n",
      "before its placing on the market or putting into service;  EN 42  EN (32) ‘input data’ means data provided to or directly acquired by an AI system on the ba sis \n",
      "of which the system produces an output;  \n",
      "(33) ‘biometric data’ means personal data resulting from specific technical processing \n",
      "relating to the physical, physiological or behavioural characteristics of a natural \n",
      "person, which allow or confirm the unique  identification of that natural person, such \n",
      "as facial images or dactyloscopic data;  \n",
      "(34) ‘emotion recognition system’ means an AI system for the purpose of identifying or \n",
      "inferring emotions or intentions of natural persons on the basis of their biometric \n",
      "data;  \n",
      "(35) ‘biometric categorisation system’ means an AI system for the purpose of assigning \n",
      "natural persons to specific categories, such as sex, age, hair colour, eye colour, \n",
      "tattoos, ethnic origin or sexual or political orientation, on the basis of their  biometric \n",
      "data;  \n",
      "(36) ‘remote biometric identification system’ means  an AI system  for the purpose of \n",
      "identifying natural persons at a distance through the comparison of a person’s \n",
      "biometric data with the biometric data contained in a reference database, an d without \n",
      "prior knowledge of the user of the AI system whether the person will be present and \n",
      "can be identified ;  \n",
      "(37) ‘‘real -time’ remote biometric identification system’ means a remote biometric \n",
      "identification system whereby the capturing of biometric da ta, the comparison and \n",
      "the identification all occur without a significant delay. This comprises not only \n",
      "instant identification, but also limited short delays in order to avoid circumvention.  \n",
      "(38) ‘‘post’ remote biometric identification system’ means a remote biometric \n",
      "identification system other than a ‘real -time’ remote biometric identification system;  \n",
      "(39) ‘publicly accessible space’ means any physical place accessible to the public, \n",
      "regardless of whether certain conditions for access may apply;  \n",
      "(40) ‘law enforcement authority’ means:   \n",
      "(a) any public authority competent for the prevention, investigation, detection or \n",
      "prosecution of criminal offences or the execution of criminal penalties, \n",
      "including the safeguarding against and the prevention of threat s to public \n",
      "security; or  \n",
      "(b) any other body or entity entrusted by Member State law to exercise public \n",
      "authority and public powers for the purposes of the prevention, investigation, \n",
      "detection or prosecution of criminal offences or the execution of criminal  \n",
      "penalties, including the safeguarding against and the prevention of threats to \n",
      "public security;  \n",
      "(41) ‘law enforcement’ means activities carried out by law enforcement authorities for the \n",
      "prevention, investigation, detection or prosecution of criminal offences or the \n",
      "execution of criminal penalties, including the safeguarding against and the \n",
      "prevention of threats to public security;  \n",
      "(42) ‘national supervisory authority’ means the authority to which a Member State assigns \n",
      "the responsibility for the imple mentation and application of this Regulation, for \n",
      "coordinating the activities entrusted to that Member State, for acting as the single \n",
      "contact point for the Commission, and for representing the Member State at the \n",
      "European Artificial Intelligence Board;  EN 43  EN (43) ‘national competent authority’ means the national supervisory authority, the \n",
      "notifying authority and the market surveillance authority;  \n",
      "(44) ‘serious incident’ means any incident that directly or indirectly leads, might have led \n",
      "or might lead to any of  the following:  \n",
      "(a) the death of a person or serious damage to a person’s health,  to property or the \n",
      "environment,  \n",
      "(b) a serious and irreversible disruption of the management and operation of \n",
      "critical infrastructure.  \n",
      "Article 4  \n",
      "Amendments to Annex I  \n",
      "The Com mission is empowered to adopt delegated acts in accordance with Article 73 to \n",
      "amend the list of techniques and approaches listed in Annex I, in order to update that list to \n",
      "market and technological developments on the basis of characteristics that are simi lar to the \n",
      "techniques and approaches listed therein.  \n",
      "TITLE  II \n",
      "PROHIBITED  ARTIFICIAL  INTELLIGENCE  PRACTICES  \n",
      "Article 5  \n",
      "1. The following artificial intelligence practices shall be prohibited:  \n",
      "(a) the placing on the market, putting into service or use of an A I system that \n",
      "deploys subliminal techniques beyond a person’s consciousness in order to \n",
      "materially distort a person’s behaviour in a manner that causes or is likely to \n",
      "cause that person or another person physical or psychological harm;  \n",
      "(b) the placing on t he market, putting into service or use of an AI system that \n",
      "exploits any of the vulnerabilities of a specific group of persons due to their \n",
      "age, physical or mental disability, in order to materially distort the behaviour of \n",
      "a person pertaining to that grou p in a manner that causes or is likely to cause \n",
      "that person or another person physical or psychological harm;  \n",
      "(c) the placing on the market, putting into service or use of AI systems by public \n",
      "authorities or on their behalf for the evaluation or classifica tion of the \n",
      "trustworthiness of natural persons over a certain period of time based on their \n",
      "social behaviour or known or predicted personal or personality characteristics, \n",
      "with the social score leading to either or both of the following:  \n",
      "(i) detrimental or  unfavourable treatment of certain natural persons or whole \n",
      "groups thereof in social contexts which are unrelated to the contexts in \n",
      "which the data was originally generated or collected;  \n",
      "(ii) detrimental or unfavourable treatment of certain natural person s or whole \n",
      "groups thereof that is unjustified or disproportionate to their social \n",
      "behaviour or its gravity;  \n",
      "(d) the use of  ‘real-time’ remote biometric identification systems in publicly \n",
      "accessible spaces for the purpose of law enforcement, unless and in a s far as \n",
      "such use is strictly necessary for one of the following objectives : EN 44  EN (i) the targeted search for specific potential victims of crime, including \n",
      "missing children;  \n",
      "(ii) the prevention of a specific, substantial and imminent  threat to the life or \n",
      "physical safety of natural persons or of a terrorist attack;  \n",
      "(iii) the detection, localisation, identification or prosecution of a perpetrator \n",
      "or suspect of a criminal offence referred to  in Article 2(2) of Council \n",
      "Framework Decision 2002/584/JHA62 and punish able in the Member \n",
      "State concerned by a custodial sentence or a detention order for a \n",
      "maximum period of at least three years, as determined by the law of that \n",
      "Member State.  \n",
      "2. The use of ‘real -time’ remote biometric identification systems in publicly acces sible \n",
      "spaces for the purpose of law enforcement for any of the objectives referred to in \n",
      "paragraph 1 point d) shall take into account the following elements:  \n",
      "(a) the nature of the situation giving rise to the possible use, in particular the \n",
      "seriousness, pr obability and scale of the harm caused in the absence of the use \n",
      "of the system;  \n",
      "(b) the consequences of the use of the system for the rights and freedoms of all \n",
      "persons concerned, in particular the seriousness, probability and scale of those \n",
      "consequences.  \n",
      "In addition, the use of ‘real -time’ remote biometric identification systems in publicly \n",
      "accessible spaces for the purpose of law enforcement for any of the objectives \n",
      "referred to in paragraph 1 point d) shall comply with necessary and proportionate \n",
      "safegu ards and conditions in relation to the use, in particular as regards the temporal, \n",
      "geographic and personal limitations.  \n",
      "3. As regards paragraphs 1, point (d) and 2, each individual use for the purpose of law \n",
      "enforcement of a ‘real-time’ remote biometric identification system in publicly \n",
      "accessible spaces shall be subject to a prior authorisation granted by a judicial \n",
      "authority or by an independent administrative authority of the Member State in \n",
      "which the use is to take place,  issued upon a reasoned request and in accordance with \n",
      "the detailed rules of national law referred to in paragraph 4. However, in a duly \n",
      "justified situation of urgency, the use of the system may be commenced without an \n",
      "authorisation and the authorisation m ay be requested only during or after the use.   \n",
      "The competent judicial or administrative authority shall only grant the authorisation \n",
      "where it is satisfied, based on objective evidence or clear indications presented to it, \n",
      "that the use of the ‘real -time’ r emote biometric identification system at issue is \n",
      "necessary for and proportionate to achieving one of the objectives specified in \n",
      "paragraph 1, point (d), as identified in the request. In deciding on the request, the \n",
      "competent judicial or administrative aut hority shall  take into account the elements \n",
      "referred to in paragraph 2.  \n",
      "4. A Member State may decide to provide for the possibility to fully or partially \n",
      "authorise the use of ‘real -time’ remote biometric identification systems in publicly \n",
      "accessible space s for the purpose of law enforcement within the limits and under the \n",
      "                                                 \n",
      "62 Council Framework Decision 2002/584/JHA  of 13 June 2002 on the European arrest warrant and the \n",
      "surrender procedures between Member States ( OJ L 190, 18.7.2002, p. 1) . \n",
      " EN 45  EN conditions listed in paragraphs 1, point (d), 2 and 3. That Member State shall lay \n",
      "down in its national law the necessary detailed rules for the request, issuance and \n",
      "exercise of, as well  as supervision relating to, the authorisations referred to in \n",
      "paragraph 3. Those rules shall also specify in respect of which of the objectives listed \n",
      "in paragraph 1, point (d), including which of the criminal offences referred to in \n",
      "point (iii) thereof, the competent authorities may be authorised to use those systems \n",
      "for the purpose of law enforcement.  \n",
      "TITLE  III \n",
      "HIGH -RISK  AI SYSTEMS  \n",
      "CHAPTER 1 \n",
      "CLASSIFICATION  OF AI SYSTEMS  AS HIGH -RISK  \n",
      "Article 6  \n",
      "Classification rules for high -risk AI systems  \n",
      "1. Irrespective of whether an AI system is placed on the market or put into service \n",
      "independently from the products referred to in points (a) and (b), that AI system shall \n",
      "be considered high -risk where both of the following conditions are fulfilled:  \n",
      "(a) the AI system is i ntended to be used as a safety component of a product, or is \n",
      "itself a product, covered by the Union harmonisation legislation listed in Annex \n",
      "II;  \n",
      "(b) the product  whose safety component is the AI system, or the AI system itself as \n",
      "a product, is required to  undergo a third -party conformity assessment with a \n",
      "view to the placing on the market or putting into service of that product \n",
      "pursuant to the Union harmonisation legislation listed in Annex II.  \n",
      "2. In addition to the high -risk AI systems referred to in para graph 1, AI systems \n",
      "referred to in Annex III shall also be considered high -risk. \n",
      "Article 7  \n",
      "Amendments to Annex III  \n",
      "1. The Commission is empowered to adopt delegated acts in accordance with Article 73 \n",
      "to update the list in Annex III by adding high -risk AI s ystems where both of the \n",
      "following conditions are fulfilled:  \n",
      "(a) the AI systems are intended to be used in any of the areas listed in points 1 to 8 \n",
      "of Annex III;  \n",
      "(b) the AI systems pose a risk of harm to the health and safety, or a risk of adverse \n",
      "impact o n fundamental rights, that is, in respect of its severity and probability \n",
      "of occurrence, equivalent to or greater than the risk of harm or of adverse \n",
      "impact posed by the high -risk AI systems already referred to in Annex III.  \n",
      "2. When assessing for the purpo ses of paragraph 1 whether an AI system poses a risk of \n",
      "harm to the health and safety or a risk of adverse impact on fundamental rights that is \n",
      "equivalent to or greater than the risk of harm posed by the high -risk AI systems EN 46  EN already referred to in Annex II I, the Commission shall take into account the \n",
      "following criteria:  \n",
      "(a) the intended purpose of the AI system;  \n",
      "(b) the extent to which an AI system has been used or is likely to be used;  \n",
      "(c) the extent to which the use of an AI system has already caused harm  to the \n",
      "health and safety or adverse impact on the fundamental rights or has given rise \n",
      "to significant concerns in relation to the materialisation of such harm or \n",
      "adverse impact, as demonstrated by reports or documented allegations \n",
      "submitted to national co mpetent authorities;  \n",
      "(d) the potential extent of such harm or such adverse impact, in particular in terms \n",
      "of its intensity and its ability to affect a plurality of persons;  \n",
      "(e) the extent to which potentially harmed or adversely impacted persons are \n",
      "depend ent on the outcome produced with an AI system, in particular because \n",
      "for practical or legal reasons it is not reasonably possible to opt -out from that \n",
      "outcome;  \n",
      "(f) the extent to which potentially harmed or adversely impacted persons are in a \n",
      "vulnerable pos ition in relation to the user of an AI system, in particular due to \n",
      "an imbalance of power, knowledge, economic or social circumstances, or age;  \n",
      "(g) the extent to which the outcome produced with an AI system is easily \n",
      "reversible, whereby outcomes having an impact on the health or safety of \n",
      "persons shall not be considered as easily reversible;  \n",
      "(h) the extent to which existing Union legislation provides for:  \n",
      "(i) effective measures of redress in relation to the risks posed by an AI \n",
      "system, with the exclusion of  claims for damages;  \n",
      "(ii) effective measures to prevent or substantially minimise those risks.  \n",
      "CHAPTER 2 \n",
      "REQUIREMENTS FOR HIG H-RISK AI SYSTEMS  \n",
      "Article 8  \n",
      "Compliance with the requirements  \n",
      "1. High -risk AI systems shall comply with the requirements established  in this Chapter.  \n",
      "2. The intended purpose of the high -risk AI system and the risk management system \n",
      "referred to in Article 9 shall be taken into account when ensuring compliance with \n",
      "those requirements.  \n",
      "Article 9  \n",
      "Risk management system  \n",
      "1. A risk management  system shall be established, implemented, documented and \n",
      "maintained in relation to high -risk AI systems.  \n",
      "2. The risk management system shall consist of a continuous iterative process run \n",
      "throughout the entire lifecycle of a high -risk AI system, requiring regular systematic \n",
      "updating. It shall comprise the following steps:  EN 47  EN (a) identification and analysis of the known and foreseeable risks associated with \n",
      "each high -risk AI system;  \n",
      "(b) estimation and evaluation of the risks that may emerge when the high -risk A I \n",
      "system is used in accordance with its intended purpose and under conditions of \n",
      "reasonably foreseeable misuse;  \n",
      "(c) evaluation of other possibly arising risks based on the analysis of data gathered \n",
      "from the post -market monitoring system referred to in Art icle 61;  \n",
      "(d) adoption of suitable risk management measures in accordance with the \n",
      "provisions of the following paragraphs.  \n",
      "3. The risk management measures referred to in paragraph 2, point (d) shall give due \n",
      "consideration to the effects and possible interac tions resulting from the combined \n",
      "application of the requirements set out in this Chapter 2. They shall take into account \n",
      "the generally acknowledged state of the art, including as reflected in relevant \n",
      "harmonised standards or common specifications.  \n",
      "4. The risk management measures referred to in paragraph 2, point (d) shall be such that \n",
      "any residual risk associated with each hazard as well as the overall residual risk of \n",
      "the high -risk AI systems is judged acceptable, provided that the high -risk AI system \n",
      "is used in accordance with its intended purpose or under conditions of reasonably \n",
      "foreseeable misuse. Those residual risks shall be communicated to the user.  \n",
      "In identifying the most appropriate risk management measures, the following shall be \n",
      "ensured:  \n",
      "(a) elimination or reduction of risks as far as possible through adequate design and \n",
      "development;  \n",
      "(b) where appropriate, implementation of adequate mitigation and control \n",
      "measures in relation to risks that cannot be eliminated;  \n",
      "(c) provision of adequate informati on pursuant to Article 13, in particular as \n",
      "regards the risks referred to in paragraph 2, point (b) of this Article, and, where \n",
      "appropriate, training to users.  \n",
      "In eliminating or reducing risks related to the use of the high -risk AI system, due \n",
      "consideratio n shall be given to the technical knowledge, experience, education, \n",
      "training to be expected by the user and the environment in which the system is \n",
      "intended to be used.  \n",
      "5. High -risk AI systems shall be tested for the purposes of identifying the most \n",
      "appropr iate risk management measures. Testing shall ensure that high -risk AI \n",
      "systems perform consistently for their intended purpose and they are in compliance \n",
      "with the requirements set out in this Chapter.  \n",
      "6. Testing procedures shall be suitable to achieve the i ntended purpose of the AI system \n",
      "and do not need to go beyond what is necessary to achieve that purpose.  \n",
      "7. The testing of the high -risk AI systems shall be performed, as appropriate, at any \n",
      "point in time throughout the development process, and, in any eve nt, prior to the \n",
      "placing on the market or the putting into service. Testing shall be made against \n",
      "preliminarily defined metrics and probabilistic thresholds that are appropriate to the \n",
      "intended purpose of the high -risk AI system.  EN 48  EN 8. When implementing the r isk management system described in paragraphs 1 to 7, \n",
      "specific consideration shall be given to whether the high -risk AI system is likely to \n",
      "be accessed by or have an impact on children.  \n",
      "9. For credit institutions regulated by Directive 2013/36/EU, the aspe cts described in \n",
      "paragraphs 1 to 8 shall be part of the risk management procedures established by \n",
      "those institutions pursuant to Article 74 of that Directive.  \n",
      "Article 10  \n",
      "Data and data governance  \n",
      "1. High -risk AI systems which make use of techniques involvin g the training of models \n",
      "with data shall be developed on the basis of training, validation and testing data sets \n",
      "that meet the quality criteria referred to in paragraphs 2 to 5.  \n",
      "2. Training, validation and testing data sets shall be subject to appropriate data \n",
      "governance and management practices. Those practices shall concern in particular,  \n",
      "(a) the relevant design choices;  \n",
      "(b) data collection;  \n",
      "(c) relevant data preparation processing operations, such as annotation, labelling, \n",
      "cleaning, enrichment and aggregation ; \n",
      "(d) the formulation of relevant assumptions, notably with respect to the \n",
      "information that the data are supposed to measure and represent;  \n",
      "(e) a prior assessment of  the availability, quantity and suitability of the data sets \n",
      "that are needed;  \n",
      "(f) examination in view of possible biases;  \n",
      "(g) the identification of any possible data gaps or shortcomings, and how those \n",
      "gaps and shortcomings can be addressed.  \n",
      "3. Training, validation and testing data sets shall be relevant, representative, free of \n",
      "errors and complete. They shall have the appropriate statistical properties, including, \n",
      "where applicable, as regards the persons or groups of persons on which the high -risk \n",
      "AI syst em is intended to be used. These characteristics of the data sets may be met at \n",
      "the level of individual data sets or a combination thereof.  \n",
      "4. Training, validation and testing data sets shall take into account, to the extent \n",
      "required by the intended purpos e, the characteristics or elements that are particular to \n",
      "the specific geographical, behavioural or functional setting within which the high -\n",
      "risk AI system is intended to be used.  \n",
      "5. To the extent that it is strictly necessary for the purposes of ensuring  bias monitoring, \n",
      "detection and correction in relation to the high -risk AI systems, the providers of such \n",
      "systems may process special categories of personal data  referred to in Article 9(1) of \n",
      "Regulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680  and Article 10(1) \n",
      "of Regulation (EU) 2018/1725, subject to appropriate safeguards for the fundamental \n",
      "rights and freedoms of natural persons, including technical limitations on the re -use \n",
      "and use of state -of-the-art security and privacy -preserving measure s, such as \n",
      "pseudonymisation, or encryption where anonymisation may significantly affect the \n",
      "purpose pursued.  EN 49  EN 6. Appropriate data governance and management practices shall apply for the \n",
      "development of high -risk AI systems other than those which make use of techniques \n",
      "involving the training of models in order to ensure that those high -risk AI systems \n",
      "comply with paragraph 2.  \n",
      "Article 11  \n",
      "Technical documentation  \n",
      "1. The technical documentation of a high -risk AI system shall be drawn up before that \n",
      "system is pla ced on the market or put into service and shall be kept up -to date.  \n",
      "The technical documentation shall be drawn up in such a way to demonstrate that the \n",
      "high-risk AI system complies with the requirements set out in this Chapter and \n",
      "provide national competen t authorities and notified bodies with all the necessary \n",
      "information to assess the compliance of the AI system with those requirements. It \n",
      "shall contain, at a minimum, the elements set out in Annex IV.  \n",
      "2. Where a high -risk AI system related to a product, t o which the legal acts listed in \n",
      "Annex II, section A apply, is placed on the market or put into service one single \n",
      "technical documentation shall be drawn up containing all the information set out in \n",
      "Annex IV as well as the information required under those legal acts.  \n",
      "3. The Commission is empowered to adopt delegated acts in accordance with Article 73 \n",
      "to amend Annex IV where necessary to ensure that, in the light of technical progress, \n",
      "the technical documentation provides all the necessary information to ass ess the \n",
      "compliance of the system with the requirements set out in this Chapter.  \n",
      "Article 12  \n",
      "Record -keeping  \n",
      "1. High -risk AI systems shall be designed and developed with capabilities enabling the \n",
      "automatic recording of events (‘logs’) while the high -risk AI s ystems is operating. \n",
      "Those logging capabilities shall conform to recognised standards or common \n",
      "specifications.  \n",
      "2. The logging capabilities shall ensure a level of traceability of the AI system’s \n",
      "functioning throughout its lifecycle that is appropriate to the intended purpose of the \n",
      "system.  \n",
      "3. In particular, logging capabilities shall enable the monitoring of the operation of the \n",
      "high-risk AI system with respect to the occurrence of situations that may result in the \n",
      "AI system presenting a risk within the m eaning of Article 65(1) or lead to a \n",
      "substantial modification, and facilitate the post -market monitoring referred to in \n",
      "Article 61.  \n",
      "4. For high -risk AI systems referred to in paragraph 1, point (a) of Annex III, the \n",
      "logging capabilities shall provide, at a minimum:  \n",
      "(a) recording of the period of each use of the system (start date and time and end \n",
      "date and time of each use);  \n",
      "(b) the reference database against which input data has been checked by the \n",
      "system;  \n",
      "(c) the input data for which the search has led to a match;  EN 50  EN (d) the identification of the natural persons involved in the verification of the \n",
      "results, as referred to in Article 14 (5).  \n",
      "Article 13  \n",
      "Transparency and provision of information to users  \n",
      "1. High -risk AI systems shall be designed and developed in such a way to ensure that \n",
      "their operation is sufficiently transparent to enable users to interpret the system’s \n",
      "output and use it appropriately. An appropriate type and degree of transparency shall \n",
      "be ensured, with a view to achieving compliance with th e relevant obligations of the \n",
      "user and of the provider set out  in Chapter 3 of this Title . \n",
      "2. High -risk AI systems shall be accompanied by instructions for use in an appropriate \n",
      "digital format or otherwise that include concise, complete, correct and clear \n",
      "information that is relevant, accessible and comprehensible to users.  \n",
      "3. The information referred to in paragraph 2 shall specify:  \n",
      "(a) the identity and the contact details of the provider and, where applicable, of its \n",
      "authorised representative;  \n",
      "(b) the cha racteristics, capabilities and limitations of performance of the high -risk \n",
      "AI system, including:  \n",
      "(i) its intended purpose;  \n",
      "(ii) the level of accuracy, robustness and cybersecurity referred to in Article \n",
      "15 against which the high -risk AI system has been tested and validated \n",
      "and which can be expected, and any known and foreseeable \n",
      "circumstances that may have an impact on that expected level of \n",
      "accuracy, robustness and cybersecurity;  \n",
      "(iii) any known or foreseeable circumstance, related to the use of the hig h-\n",
      "risk AI system in accordance with its intended purpose or under \n",
      "conditions of reasonably foreseeable misuse, which may lead to risks to \n",
      "the health and safety or fundamental rights;  \n",
      "(iv) its performance as regards the persons or groups of persons on which  the \n",
      "system is intended to be used;  \n",
      "(v) when appropriate, specifications for the input data,  or any other relevant \n",
      "information in terms of the training, validation and testing data sets used, \n",
      "taking into account the intended purpose of the AI system . \n",
      "(c) the changes to the high -risk AI system and its performance which have been \n",
      "pre-determined by the provider at the moment of the initial conformity \n",
      "assessment, if any;   \n",
      "(d) the human oversight measures referred to in Article 14, including the technical \n",
      "measur es put in place to facilitate the interpretation of the outputs of AI \n",
      "systems by the users;  \n",
      "(e) the expected lifetime of the high -risk AI system and any necessary \n",
      "maintenance and care measures to ensure the proper functioning of that AI \n",
      "system, including a s regards software updates.  EN 51  EN Article 14  \n",
      "Human oversight  \n",
      "1. High -risk AI systems shall be designed and developed in such a way, including with \n",
      "appropriate human -machine interface tools, that they can be effectively overseen by \n",
      "natural persons during the peri od in which the AI system is in use.  \n",
      "2. Human oversight shall aim at preventing or minimising the risks to health, safety or \n",
      "fundamental rights that may emerge when a high -risk AI system is used in \n",
      "accordance with its intended purpose or under conditions of reasonably foreseeable \n",
      "misuse, in particular when such risks persist notwithstanding the application of other \n",
      "requirements set out in this Chapter . \n",
      "3. Human oversight shall be ensured through either one or all of the following \n",
      "measures:  \n",
      "(a) identified a nd built, when technically feasible, into the high -risk AI system by \n",
      "the provider before it is placed on the market or put into service;  \n",
      "(b) identified by the provider  before placing the high -risk AI system on the market \n",
      "or putting it into service and tha t are appropriate to be implemented by the \n",
      "user.  \n",
      "4. The measures referred to in paragraph 3 shall enable the individuals  to whom human \n",
      "oversight is assigned to do the following, as appropriate to the circumstances:  \n",
      "(a) fully understand the capacities and l imitations of the high -risk AI system and \n",
      "be able to duly monitor its operation, so that signs of anomalies, dysfunctions \n",
      "and unexpected performance can be detected and addressed as soon as \n",
      "possible;  \n",
      "(b) remain aware of the possible tendency of automatical ly relying or over -relying \n",
      "on the output produced by a high -risk AI system (‘automation bias’), in \n",
      "particular for high -risk AI systems used to provide information or \n",
      "recommendations for decisions to be taken by natural persons;  \n",
      "(c) be able to correctly int erpret the high -risk AI system’s output, taking into \n",
      "account in particular the characteristics of the system and the interpretation \n",
      "tools and methods available;  \n",
      "(d) be able to decide, in any particular situation, not to use the high -risk AI system \n",
      "or other wise disregard, override or reverse the output of the high -risk AI \n",
      "system;  \n",
      "(e) be able to intervene on the operation of the high -risk AI system or interrupt the \n",
      "system through a “stop” button or a similar procedure.  \n",
      "5. For high -risk AI systems referred to in point 1(a) of Annex III, the measures referred \n",
      "to in paragraph 3 shall be such as to ensure that, in addition, no action or decision is \n",
      "taken by the user on the basis of the identification resulting from the system unless \n",
      "this has been verified and conf irmed by at least two natural persons.  \n",
      "Article 15  \n",
      "Accuracy, robustness and cybersecurity  \n",
      "1. High -risk AI systems shall be designed and developed in such a way that they \n",
      "achieve, in the light of their intended purpose, an appropriate level of accuracy, EN 52  EN robustness and cybersecurity, and perform consistently in those respects throughout \n",
      "their lifecycle.  \n",
      "2. The levels of accuracy and the relevant accuracy metrics of high -risk AI systems \n",
      "shall be declared in the accompanying instructions of use.  \n",
      "3. High -risk AI systems shall be resilient as regards errors, faults or inconsistencies that \n",
      "may occur within the system or the environment in which the system operates, in \n",
      "particular due to their interaction with natural persons or other systems.  \n",
      "The robustness of high -risk AI systems may be achieved through technical \n",
      "redundancy solutions, which may include backup or fail -safe plans.  \n",
      "High -risk AI systems that continue to learn after being placed on the market or put \n",
      "into service shall be developed in such a way to ensure that possibly biased outputs \n",
      "due to outputs used as an input for future operations (‘feedback loops’) are duly \n",
      "addressed with appropriate mitigation measures.  \n",
      "4. High -risk AI systems shall be resilient as regards attempts by unauthorised third \n",
      "parties to a lter their use or performance by exploiting the system vulnerabilities.  \n",
      "The technical solutions aimed at ensuring the cybersecurity of high -risk AI systems \n",
      "shall be appropriate to the relevant circumstances and the risks.  \n",
      "The technical solutions to address AI specific vulnerabilities shall include, where \n",
      "appropriate, measures to prevent and control for attacks trying to manipulate the \n",
      "training dataset  (‘data poisoning’), inputs designed to cause the model to make a \n",
      "mistake  (‘adversarial examples’), or model flaws.  \n",
      "CHAPTER 3 \n",
      "OBLIGATIONS OF PROVI DERS AND USERS OF HI GH-RISK AI SYSTEMS AND \n",
      "OTHER PARTIES  \n",
      "Article 16  \n",
      "Obligations of providers of high -risk AI systems  \n",
      "Providers of high -risk AI systems shall:  \n",
      "(a) ensure that their hig h-risk AI systems are compliant with the requirements set out in \n",
      "Chapter 2 of this Title;  \n",
      "(b) have a quality management system in place which complies with Article 17;  \n",
      "(c) draw -up the technical documentation of the high -risk AI system;  \n",
      "(d) when under their  control, keep the logs automatically generated by their high -risk AI \n",
      "systems;  \n",
      "(e) ensure that the high -risk AI system undergoes the relevant conformity assessment \n",
      "procedure, prior to its placing on the market or putting into service;  \n",
      "(f) comply with the r egistration obligations referred to in Article 51;  \n",
      "(g) take the necessary corrective actions , if the high -risk AI system is not in conformity \n",
      "with the requirements set out in Chapter 2 of this Title ; EN 53  EN (h) inform the national competent authorities of the Mem ber States in which they made \n",
      "the AI system available or put it into service and, where applicable, the notified body \n",
      "of the non -compliance and of any corrective actions taken;  \n",
      "(i) to affix the CE marking to their high -risk AI systems to indicate the confo rmity with \n",
      "this Regulation in accordance with Article 49;  \n",
      "(j) upon request of a national competent authority, demonstrate the conformity of the \n",
      "high-risk AI system with the requirements set out in Chapter 2 of this Title . \n",
      "Article 17  \n",
      "Quality management syst em  \n",
      "1. Providers of high -risk AI systems shall put a quality management system in place \n",
      "that ensures compliance with this Regulation. That system shall be documented in a \n",
      "systematic and orderly manner in the form of written policies, procedures and \n",
      "instruc tions, and shall include at least the following aspects:  \n",
      "(a) a strategy for regulatory compliance, including compliance with conformity \n",
      "assessment procedures and procedures for the management of modifications to \n",
      "the high -risk AI system;  \n",
      "(b) techniques, pro cedures and systematic actions to be used for the design, design \n",
      "control and design verification of the high -risk AI system;  \n",
      "(c) techniques, procedures and systematic actions to be used for the development, \n",
      "quality control and quality assurance of the high -risk AI system;  \n",
      "(d) examination, test and validation procedures to be carried out before, during and \n",
      "after the development of the high -risk AI system, and the frequency with which \n",
      "they have to be carried out;  \n",
      "(e) technical specifications, including standa rds, to be applied and, where the \n",
      "relevant harmonised standards are not applied in full, the means to be used to \n",
      "ensure that the high -risk AI system complies with the requirements set out in \n",
      "Chapter 2 of this Title;  \n",
      "(f) systems and procedures for data management, including data collection, data \n",
      "analysis, data labelling, data storage, data filtration, data mining, data \n",
      "aggregation, data retention and any other operation regarding the data that is \n",
      "performed before and for the purposes of the placing on th e market or putting \n",
      "into service of high -risk AI systems;  \n",
      "(g) the risk management system referred to in Article 9;  \n",
      "(h) the setting -up, implementation and maintenance of a post -market monitoring \n",
      "system, in accordance with Article 61;  \n",
      "(i) procedures related to the reporting of serious incidents and of malfunctioning \n",
      "in accordance with Article 62;  \n",
      "(j) the handling of communication with national competent authorities, competent \n",
      "authorities, including sectoral ones, providing or supporting the access to data, \n",
      "notified bodies, other operators, customers or other interested parties;  \n",
      "(k) systems and procedures for record keeping of all relevant documentation and \n",
      "information;  \n",
      "(l) resource management, including security of supply related measures;  EN 54  EN (m) an accountabil ity framework setting out the responsibilities of the management \n",
      "and other staff with regard to all aspects listed in this paragraph.  \n",
      "2. The implementation of aspects referred to in paragraph 1 shall be proportionate to the \n",
      "size of the provider’s organisat ion.  \n",
      "3. For providers that are credit institutions regulated by Directive 2013/36/ EU, the \n",
      "obligation to put a quality management system in place shall be deemed to be \n",
      "fulfilled by complying with the rules on internal governance arrangements, processes \n",
      "and mechanisms pursuant to Article 74 of that Directive. In that context, any \n",
      "harmonised standards referred to in Article 40 of this Regulation shall be taken into \n",
      "account.  \n",
      "Article 18  \n",
      "Obligation to draw up technical documentation  \n",
      "1. Providers of high -risk AI  systems shall draw up the technical documen tation referred \n",
      "to in Article 11 in accordance with Annex IV.  \n",
      "2. Providers that are credit institutions regulated by Directive 2013/36/EU shall \n",
      "maintain the technical documentation as part of the documentation c oncerning \n",
      "internal governance, arrangements, processes and mechanisms pursuant to Article 74 \n",
      "of that Directive.  \n",
      "Article 19  \n",
      "Conformity assessment  \n",
      "1. Providers of high -risk AI systems shall ensure that their systems undergo the relevant \n",
      "conformity assessmen t procedure in accordance with Article 43, prior to their placing \n",
      "on the market or putting into service. Where the compliance of the AI systems with \n",
      "the requirements set out in Chapter 2 of this Title has been demonstrated following \n",
      "that conformity assessm ent, the providers shall draw up an EU declaration of \n",
      "conformity in accordance with Article 48 and affix the CE marking of conformity in \n",
      "accordance with Article 49.  \n",
      "2. For high -risk AI systems referred to in point 5(b) of Annex III that are placed on the \n",
      "market or put into service by providers that are credit institutions regulated by \n",
      "Directive 2013/36/EU, the conformity assessment  shall be carried out as part of the \n",
      "procedure referred to in Articles 97 to101 of that Directive.  \n",
      "Article 20  \n",
      "Automatically gen erated logs  \n",
      "1. Providers of high -risk AI systems shall keep the logs automatically generated by \n",
      "their high -risk AI systems, to the extent such logs are under their control by virtue of \n",
      "a contractual arrangement with the user or otherwise by law. The logs s hall be kept \n",
      "for a period that is appropriate in the light of the intended purpose of high -risk AI \n",
      "system and applicable legal obligations under Union or national law.  \n",
      "2. Providers that are credit institutions regulated by Directive 2013/36/EU shall \n",
      "mainta in the logs automatically generated by their high -risk AI systems as part of the \n",
      "documentation under Articles 74 of that Directive.  EN 55  EN Article 21  \n",
      "Corrective actions  \n",
      "Providers of high -risk AI systems which consider o r have reason to consider that a high -risk \n",
      "AI system which they have placed on the market or put into service is not in conformity with \n",
      "this Regulation shall immediately take the necessary corrective actions to bring that system \n",
      "into conformity, to withdraw it or to recall it, as appropriate. They s hall inform the \n",
      "distributors of the high -risk AI system in question and, where applicable, the authorised \n",
      "representative and importers accordingly.  \n",
      "Article 22  \n",
      "Duty of information  \n",
      "Where the high-risk AI system presents a risk within the meaning of Article 6 5(1) and that \n",
      "risk is known to the provider of the system, that provider shall immediately inform the \n",
      "national competent authorities of the Member States in which it made the system available \n",
      "and, where applicable, the notified body that issued a certifica te for the  high-risk AI system, \n",
      "in particular of the non -compliance and of any corrective actions taken.   \n",
      "Article 23  \n",
      "Cooperation with competent authorities  \n",
      "Providers of high -risk AI systems shall, upon request by a national competent authority, \n",
      "provide tha t authority with all the information and documentation necessary to demonstrate \n",
      "the conformity of the high -risk AI system with the requirements set out in Chapter 2 of this \n",
      "Title , in an official Union language determined by the Member State concerned. Upon  a \n",
      "reasoned request from a national competent authority, providers shall also give that authority \n",
      "access to the logs automatically generated by the high -risk AI system, to the extent such logs \n",
      "are under their control by virtue of a contractual arrangement with the user or otherwise by \n",
      "law. \n",
      "Article 24  \n",
      "Obligations of product manufacturers  \n",
      "Where a high -risk AI system related to products to which the legal acts listed in Annex II, \n",
      "section A, apply, is placed on the market or put into service together with the product \n",
      "manufactured in accordance with those legal acts and under the name of the product \n",
      "manufacturer, the manufacturer of the product shall take the responsibility of the compliance \n",
      "of the AI system with this Regulation and, as far as the AI system is c oncerned, have the same \n",
      "obligations imposed by the present Regulation on the provider.  \n",
      "Article 25  \n",
      "Authorised representatives  \n",
      "1. Prior to making their systems available on the Union market, where an importer \n",
      "cannot be identified, providers established outside the Union shall, by written \n",
      "mandate, appoint an authorised representative which is established in the Union.  \n",
      "2. The authorised representative shall perform the tasks specified in the mandate \n",
      "received from the provider. The mandate shall empower the  authorised representative \n",
      "to carry out the following tasks:  EN 56  EN (a) keep a copy of the EU declaration of conformity and the technical \n",
      "documentation at the disposal of the national competent authorities and \n",
      "national authorities referred to in Article 63(7);  \n",
      "(b) provide a national competent authority, upon a reasoned request, with all the \n",
      "information and documentation necessary to demonstrate the conformity of a \n",
      "high-risk AI system with the requirements set out in Chapter 2 of this Title, \n",
      "including access to the  logs automatically generated by the high -risk AI system  \n",
      "to the extent such logs are under the control of the provider by virtue of a \n",
      "contractual arrangement with the user or otherwise by law ; \n",
      "(c) cooperate with competent national authorities, upon a reaso ned request, on any \n",
      "action the latter takes in relation to the high -risk AI system.  \n",
      "Article 26  \n",
      "Obligations of importers  \n",
      "1. Before placing a high -risk AI system on the market, importers of such system shall \n",
      "ensure that:  \n",
      "(a) the appropriate conformity assess ment procedure has been carried out by the \n",
      "provider of that AI system  \n",
      "(b) the provider has drawn up the technical documentation in accordance with \n",
      "Annex IV;  \n",
      "(c) the system bears the required conformity marking and is accompanied by the \n",
      "required documenta tion and instructions of use.  \n",
      "2. Where an importer considers or has reason to consider that a high -risk AI system is \n",
      "not in conformity with this Regulation, it shall not place that system on the market \n",
      "until that AI system has been brought into conformity . Where the high -risk AI \n",
      "system presents a risk within the meaning of Article 65(1), the importer shall inform \n",
      "the provider of the AI system and the market surveillance authorities to that effect.  \n",
      "3. Importers shall indicate their name, registered trade na me or registered trade mark, \n",
      "and the address at which they can be contacted on the high -risk AI system or, where \n",
      "that is not possible, on its packaging or its accompanying documentation, as \n",
      "applicable.  \n",
      "4. Importers shall ensure that, while a high -risk AI s ystem is under their responsibility, \n",
      "where applicable, storage or transport conditions do not jeopardise its compliance \n",
      "with the requirements set out in Chapter 2 of this Title . \n",
      "5. Importers shall provide national competent authorities, upon a reasoned req uest, with \n",
      "all necessary information and documentation to demonstrate the conformity of a \n",
      "high-risk AI system with the requirements set out in Chapter 2 of this Title in a \n",
      "language which can be easily understood by that national competent authority, \n",
      "includ ing access to the logs automatically generated by the high -risk AI system to the \n",
      "extent such logs are under the control of the provider by virtue of a contractual \n",
      "arrangement with the user or otherwise by law. They shall also cooperate with those \n",
      "authoriti es on any action national competent authority takes in relation to that \n",
      "system.  EN 57  EN Article 27  \n",
      "Obligations of distributors  \n",
      "1. Before making a high -risk AI system available on the market, distributors shall \n",
      "verify that the high -risk AI system bears the required  CE conformity marking, that it \n",
      "is accompanied by the required documentation and instruction of use, and that the \n",
      "provider and the importer of the system, as applicable, have complied with the \n",
      "obligations set out in this Regulation.  \n",
      "2. Where a distributor considers or has reason to consider that a high -risk AI system is \n",
      "not in conformity with the requirements set out in Chapter 2 of this Title, it shall not \n",
      "make the high -risk AI system available on the market until that system has been \n",
      "brought into conformi ty with those requirements. Furthermore, where the system \n",
      "presents a risk within the meaning of Article 65(1), the distributor shall inform the \n",
      "provider or the importer of the system, as applicable, to that effect.  \n",
      "3. Distributors shall ensure that, while a high -risk AI system is under their \n",
      "responsibility, where applicable, storage or transport conditions do not jeopardise the \n",
      "compliance of the system with the requirements set out in Chapter 2 of this Title . \n",
      "4. A distributor that considers or has reason to  consider that a high -risk AI system \n",
      "which it has made available on the market is not in conformity with the requirements \n",
      "set out in Chapter 2 of this Title shall take the corrective actions necessary to bring \n",
      "that system into conformity with those require ments, to withdraw it or recall it or \n",
      "shall ensure that the provider, the importer or any relevant operator, as appropriate, \n",
      "takes those corrective actions. Where the high -risk AI system presents a risk within \n",
      "the meaning of Article 65(1), the distributor shall immediately inform the national \n",
      "competent authorities of the Member States in which it has made the product \n",
      "available to that effect, giving details, in particular, of the non -compliance and of any \n",
      "corrective actions taken.  \n",
      "5. Upon a reasoned request  from a national competent authority, distributors of high -\n",
      "risk AI systems shall provide that authority with all the information and \n",
      "documentation necessary to demonstrate the conformity of a high -risk system with \n",
      "the requirements set out in Chapter 2 of t his Title . Distributors shall also cooperate \n",
      "with that national competent authority on any action taken by that authority.  \n",
      "Article 28  \n",
      "Obligations of distributors, importers, users or any other third -party  \n",
      "1. Any distributor, importer, user or other third -party shall be considered a provider for \n",
      "the purposes of this Regulation and shall be subject to the obligations of the provider \n",
      "under Article 16, in any of the following circumstances:  \n",
      "(a) they place on the market or put into service a high -risk AI syste m under their \n",
      "name or trademark;  \n",
      "(b) they modify the intended purpose of a high -risk AI system already placed on \n",
      "the market or put into service;  \n",
      "(c) they make a substantial modification to the high -risk AI system.  \n",
      "2. Where the circumstances referred to in paragraph 1, point (b) or (c), occur, the \n",
      "provider that initially placed the high -risk AI system on the market or put it into \n",
      "service shall no longer be considered a provider for the purposes of this Regulation.  EN 58  EN Article 29  \n",
      "Obligations of users of high -risk AI systems  \n",
      "1. Users of high -risk AI systems shall use such systems in accordance with the \n",
      "instructions of use accompanying the systems, pursuant to paragraphs 2 and 5.  \n",
      "2. The obligations in paragraph 1 are without prejudice to other user obligations under \n",
      "Union or national law and to the user’s discretion  in organising its own resources and \n",
      "activities for the purpose of implementing the human oversight measures indicated \n",
      "by the provider.  \n",
      "3. Without prejudice to paragraph 1, to the extent the user exer cises control over the \n",
      "input data, that user shall ensure that input data is relevant in view of the intended \n",
      "purpose of the high -risk AI system.  \n",
      "4. Users shall monitor the operation of the high -risk AI system on the basis of the \n",
      "instructions of use. When  they have reasons to consider that the use in accordance \n",
      "with the instructions of use may result in the AI system presenting a risk within the \n",
      "meaning of Article 65(1) they shall inform the provider or distributor and suspend \n",
      "the use of the system. They s hall also inform the provider or distributor when they \n",
      "have identified any serious incident or any malfunctioning within the meaning of \n",
      "Article 62 and interrupt the use of the AI system. In case the user is not able to reach \n",
      "the provider, Article 62 shall apply mutatis mutandis.  \n",
      "For users that are credit institutions regulated by Directive 2013/36/EU, the \n",
      "monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled \n",
      "by complying with the rules on internal governance arrangements, processes and \n",
      "mechanisms pursuant to Article 74 of that Directive.  \n",
      "5. Users of high -risk AI systems shall keep the logs automatically generated by that \n",
      "high-risk AI system, to the extent such logs are under their control. The logs shall be \n",
      "kept for a perio d that is appropriate in the light of the intended purpose of the high -\n",
      "risk AI system and applicable legal obligations under Union or national law.  \n",
      "Users that are credit institutions regulated by Directive 2013/36/EU shall maintain \n",
      "the logs as part of the documentation concerning internal governance arrangements, \n",
      "processes and mechanisms  pursuant to Article 74 of that Directive.  \n",
      "6. Users of high -risk AI systems shall use the information provided under Article 13 to \n",
      "comply with their obligation to carry out a data protection impact assessment under \n",
      "Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, \n",
      "where applicable.  \n",
      "CHAPTER 4 \n",
      "NOTIFIYING AUTHORITI ES AND NOTIFIED BODI ES \n",
      "Article 30  \n",
      "Notifying authorities  \n",
      "1. Each Member State shall de signate or establish a notifying authority responsible for \n",
      "setting up and carrying out the necessary procedures for the assessment, designation \n",
      "and notification of conformity assessment bodies and for their monitoring .  \n",
      "2. Member States may designate a nat ional accreditation body referred to in Regulation \n",
      "(EC) No 765/2008 as a notifying authority.  EN 59  EN 3. Notifying authorities shall be established, organised and operated in such a way that \n",
      "no conflict of interest arises with conformity assessment bodies and the objectivity \n",
      "and impartiality of their activities are safeguarded.  \n",
      "4. Notifying authorities shall be organised in such a way that decisions relating to the \n",
      "notification of conformity assessment bodies are taken by competent persons \n",
      "different from those who carried out the assessment of those bodies.  \n",
      "5. Notifying authorities shall not offer or provide any activities that conformity \n",
      "assessment bodies perform or any consultancy services on a commercial or \n",
      "competitive basis.  \n",
      "6. Notifying authorities shall safegu ard the confidentiality of the information they \n",
      "obtain.  \n",
      "7. Notifying authorities shall have a sufficient number of competent personnel at their \n",
      "disposal for the proper performance of their tasks.  \n",
      "8. Notifying authorities shall make sure that conformity ass essments are carried out in a \n",
      "proportionate manner, avoiding unnecessary burdens for providers and that notified \n",
      "bodies perform their activities taking due account of the size of an undertaking, the \n",
      "sector in which it operates, its structure and the degree  of complexity of the AI \n",
      "system in question.  \n",
      "Article 31  \n",
      "Application of a conformity assessment body for notification  \n",
      "1. Conformity assessment bodies shall submit an application for notification to the \n",
      "notifying authority of the Member State in which they are established.  \n",
      "2. The application for notification shall be accompanied by a description of the \n",
      "conformity assessment activities, the conformity assessment module or modules and \n",
      "the artificial intelligence technologies for which the conformity assessment  body \n",
      "claims to be competent, as well as by an accreditation certificate, where one exists, \n",
      "issued by a national accreditation body attesting that the conformity assessment body \n",
      "fulfils the requirements laid down in Article 33. Any valid document related t o \n",
      "existing designations of the applicant notified body under any other Union \n",
      "harmonisation legislation shall be added.  \n",
      "3. Where the conformity assessment body concerned cannot provide an accreditation \n",
      "certificate, it shall provide the notifying authority with the documentary evidence \n",
      "necessary for the verification, recognition and regular monitoring of its compliance \n",
      "with the requirements laid down in Article 33. For notified bodies which are \n",
      "designated under any other Union harmonisation legislation, all documents and \n",
      "certificates linked to those designations may be used to support their designation \n",
      "procedure under this Regulation, as appropriate.  \n",
      "Article 32  \n",
      "Notification procedure  \n",
      "1. Notifying authorities may notify only conformity assessment bodies which have \n",
      "satisfied the requirements laid down in Article 33.  \n",
      "2. Notifying authorities shall notify the Commission and the other Member States using \n",
      "the electronic notification tool devel oped and managed by the Commission.  EN 60  EN 3. The notification shall include full details of the conformity assessment activities, the \n",
      "conformity assessment module or modules and the artificial intelligence technologies \n",
      "concerned.  \n",
      "4. The conformity assessment b ody concerned may perform the activities of a notified \n",
      "body only where no objections are raised by the Commission or the other Member \n",
      "States within one month of a notification.  \n",
      "5. Notifying authorities shall notify the Commission and the other Member States of \n",
      "any subsequent relevant changes to the notification.  \n",
      "Article 33  \n",
      "Notified bodies  \n",
      "1. Notified bodies shall verify the conformity of high -risk AI system in accordance with \n",
      "the conformity assessment procedures referred to in Article 43.  \n",
      "2. Notified bodies shall satisfy the organisational, quality management, resources and \n",
      "process requirements that are necessary to fulfil their tasks.  \n",
      "3. The organisational structure, allocation of responsibilities, reporting lines and \n",
      "operation of notified bodies shal l be such as to ensure that there is confidence in the \n",
      "performance by and in the results of the conformity assessment activities that the \n",
      "notified bodies conduct.  \n",
      "4. Notified bodies shall be independent of the provider of a high-risk AI system in \n",
      "relation to which it performs conformity assessment activities. Notified bodies shall \n",
      "also be independent of any other operator having an economic interest in the high -\n",
      "risk AI system that is assessed, as well as of any competitors of the provider.  \n",
      "5. Notified bodie s shall be organised and operated so as to safeguard the independence, \n",
      "objectivity and impartiality of their activities. Notified bodies shall document and \n",
      "implement a structure and procedures to safeguard impartiality and to promote and \n",
      "apply the principl es of impartiality throughout their organisation, personnel and \n",
      "assessment activities.  \n",
      "6. Notified bodies shall have documented procedures in place ensuring that their \n",
      "personnel, committees, subsidiaries, subcontractors and any associated body or \n",
      "personnel  of external bodies respect the confidentiality of the information which \n",
      "comes into their possession during the performance of conformity assessment \n",
      "activities, except when disclosure is required by law. The staff of notified bodies \n",
      "shall be bound to obser ve professional secrecy with regard to all information \n",
      "obtained in carrying out their tasks under this Regulation, except in relation to the \n",
      "notifying authorities of the Member State in which their activities are carried out.  \n",
      "7. Notified bodies shall have  procedures for the performance of activities which take \n",
      "due account of the size of an undertaking, the sector in which it operates, its \n",
      "structure, the degree of complexity of the AI system in question.  \n",
      "8. Notified bodies shall take out appropriate liabili ty insurance for their conformity \n",
      "assessment activities, unless liability is assumed by the Member State concerned in \n",
      "accordance with national law or that Member State is directly responsible for the \n",
      "conformity assessment.  \n",
      "9. Notified bodies shall be capab le of carrying out all the tasks falling to them under \n",
      "this Regulation with the highest degree of professional integrity and the requisite EN 61  EN competence in the specific field, whether those tasks are carried out by notified \n",
      "bodies themselves or on their behal f and under their responsibility.  \n",
      "10. Notified bodies shall have sufficient internal competences to be able to effectively \n",
      "evaluate the tasks conducted by external parties on their behalf. To that end, at all \n",
      "times and for each conformity assessment proced ure and each type of high-risk AI \n",
      "system in relation to which they have been designated, the notified body shall have \n",
      "permanent availability of sufficient administrative, technical and scientific personnel \n",
      "who possess experience and knowledge relating to t he relevant artificial intelligence \n",
      "technologies, data and data computing and to the requirements set out in Chapter 2 of \n",
      "this Title . \n",
      "11. Notified bodies shall participate in coordination activities as referred to in Article 38. \n",
      "They shall also take part d irectly or be represented in European standardisation \n",
      "organisations, or ensure that they are aware and up to date in respect of relevant \n",
      "standards.  \n",
      "12. Notified bodies shall make available and submit upon request all relevant \n",
      "documentation, including the p roviders’ documentation, to the notifying authority \n",
      "referred to in Article 30 to allow it to conduct its assessment, designation, \n",
      "notification, monitoring and surveillance activities and to facilitate the assessment \n",
      "outlined in this Chapter.  \n",
      "Article 34  \n",
      "Subsidiaries of and subcontracting by notified bodies  \n",
      "1. Where a notified body subcontracts specific tasks connected with the conformity \n",
      "assessment or has recourse to a subsidiary, it shall ensure that the subcontractor or \n",
      "the subsidiary meets the requirement s laid down in Article 33 and shall inform the \n",
      "notifying authority accordingly.  \n",
      "2. Notified bodies shall take full responsibility for the tasks performed by \n",
      "subcontractors or subsidiaries wherever these are established.  \n",
      "3. Activities may be subcontracted or carried out by a subsidiary only with the \n",
      "agreement of the provider.  \n",
      "4. Notified bodies shall keep at the disposal of the notifying authority the relevant \n",
      "documents concerning the assessment of the qualifications of the subcontractor or the \n",
      "subsidiary a nd the work carried out by them under this Regulation.  \n",
      "Article 35  \n",
      "Identification numbers and lists of notified bodies designated under this Regulation  \n",
      "1. The Commission shall assign an identification number to notified bodies. It shall \n",
      "assign a single numb er, even where a body is notified under several Union acts.  \n",
      "2. The Commission shall make publicly available the list of the bodies notified under \n",
      "this Regulation, including the identification numbers that have been assigned to them \n",
      "and the activities for w hich they have been notified. The Commission shall ensure \n",
      "that the list is kept up to date.  EN 62  EN Article 36  \n",
      "Changes to notifications  \n",
      "1. Where a notifying authority has suspicions or has been informed that a notified body \n",
      "no longer meets the requirements laid do wn in Article 33, or that it is failing to fulfil \n",
      "its obligations, that authority shall without delay investigate the matter with the \n",
      "utmost diligence. In that context, it shall inform the notified body concerned about \n",
      "the objections raised and give it the  possibility to make its views known. If the \n",
      "notifying authority comes to the conclusion that the notified body investigation no \n",
      "longer meets the requirements laid down in Article 33 or that it is failing to fulfil its \n",
      "obligations, it shall restrict, suspe nd or withdraw the notification as appropriate, \n",
      "depending on the seriousness of the failure. It shall also immediately inform the \n",
      "Commission and the other Member States accordingly.  \n",
      "2. In the event of restriction, suspension or withdrawal of notification, or where the \n",
      "notified body has ceased its activity, the notifying authority shall take appropriate \n",
      "steps to ensure that the files of that notified body are either taken over by another \n",
      "notified body or kept available for the responsible notifying authoriti es at their \n",
      "request.  \n",
      "Article 37  \n",
      "Challenge to the competence of notified bodies  \n",
      "1. The Commission shall, where necessary, investigate all cases where there are reasons \n",
      "to doubt whether a notified body complies with the requirements  laid down in Article \n",
      "33. \n",
      "2. The Notifying authority shall provide the Commission, on request, with all relevant \n",
      "information relating to the notification of the notified body concerned.  \n",
      "3. The Commission shall ensure that all confidential information obtained in the course \n",
      "of its i nvestigations pursuant to this Article is treated confidentially.  \n",
      "4. Where the Commission ascertains that a notified body does not meet or no longer \n",
      "meets the requirements  laid down in Article 33, it shall adopt a reasoned decision \n",
      "requesting the notifying  Member State to take the necessary corrective measures, \n",
      "including withdrawal of notification if necessary. That implementing act shall be \n",
      "adopted in accordance with the examination procedure referred to in Article 74(2).  \n",
      "Article 38  \n",
      "Coordination of notifie d bodies  \n",
      "1. The Commission shall ensure that, with regard to the areas covered by this \n",
      "Regulation, appropriate coordination and cooperation between notified bodies active \n",
      "in the conformity assessment procedures of AI systems pursuant to this Regulation \n",
      "are put in place and properly operated in the form of a sectoral group of notified \n",
      "bodies . \n",
      "2. Member States shall ensure that the bodies notified by them participate in the work \n",
      "of that group, directly or by means of designated representatives.  EN 63  EN Article 39  \n",
      "Conformity assessment bodies of third countries  \n",
      "Conformity assessment bodies established under the law of a third country with which the \n",
      "Union has concluded an agreement may be authorised to carry out the activities of notified \n",
      "Bodies under this Regulation.  \n",
      "CHAPTER 5 \n",
      "STANDARDS,  CONFORMITY  ASSESSMENT,  CERTIFICATES,  REGISTRATION  \n",
      "Article 40  \n",
      "Harmonised standards  \n",
      "High -risk AI systems which are in conformity with harmonised standards or parts thereof the \n",
      "references of which have been published in the Official Journa l of the European Union shall \n",
      "be presumed to be in conformity with the requirements set out in Chapter 2 of this Title , to the \n",
      "extent those standards cover those requirements.  \n",
      "Article 41  \n",
      "Common specifications  \n",
      "1. Where harmonised standards referred to in Ar ticle 40 do not exist or where the \n",
      "Commission considers that the relevant harmonised standards are insufficient or that \n",
      "there is a need to address specific safety or fundamental right concerns, the \n",
      "Commission may, by means of implementing acts, adopt commo n specifications in \n",
      "respect of the requirements set out in Chapter 2 of this Title . Those implementing \n",
      "acts shall be adopted in accordance with the examination procedure referred to in \n",
      "Article 74(2).  \n",
      "2. The Commission, when preparing the common specificati ons referred to in \n",
      "paragraph 1, shall gather the views of relevant bodies or expert groups established \n",
      "under relevant sectorial Union law.  \n",
      "3. High -risk AI systems which are in conformity with the common specifications \n",
      "referred to in paragraph 1 shall be pr esumed to be in conformity with the \n",
      "requirements set out in Chapter 2 of this Title,  to the extent those common \n",
      "specifications cover those requirements.  \n",
      "4. Where providers do not comply with the common specifications referred to in \n",
      "paragraph 1, they shall duly justify that they have adopted technical solutions that are \n",
      "at least equivalent thereto.  \n",
      "Article 42  \n",
      "Presumption of conformity with certain requirements  \n",
      "1. Taking into account their intended purpose, high -risk AI systems that have been \n",
      "trained and test ed on data concerning the specific geographical, behavioural and \n",
      "functional setting within which they are intended to be used shall be presumed to be \n",
      "in compliance with the requirement set out in Article 10(4).  EN 64  EN 2. High -risk AI systems that have been certi fied or for which a statement of conformity \n",
      "has been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 \n",
      "of the European Parliament and of the Council63 and the references of which have \n",
      "been published in the Official Journal of the Euro pean Union shall be presumed to be \n",
      "in compliance with the cybersecurity requirements set out in Article 15 of this \n",
      "Regulation in so far as the cybersecurity certificate or statement of conformity or \n",
      "parts thereof cover those requirements.  \n",
      "Article 43  \n",
      "Confor mity assessment  \n",
      "1. For high -risk AI systems listed in point 1 of Annex III, where, in demonstrating the \n",
      "compliance of a high -risk AI system with the requirements set out in Chapter 2 of \n",
      "this Title, the provider has applied harmonised standards referred to in Article 40, or, \n",
      "where applicable, common specifications referred to in Article 41, the provider shall \n",
      "follow one of the following procedures:  \n",
      "(a) the conformity assessment procedure based on internal control referred to in \n",
      "Annex VI;  \n",
      "(b) the conformity assessment procedure based on assessment of the quality \n",
      "management system and assessment of the technical documentation, with the \n",
      "involvement of a notified body, referred to in Annex VII.  \n",
      "Where, in demonstrating the compliance of a high -risk AI system with  the \n",
      "requirements set out in Chapter 2 of this Title , the provider has not applied or has \n",
      "applied only in part harmonised standards referred to in Article 40, or where such \n",
      "harmonised standards do not exist and common specifications referred to in Article \n",
      "41 are not available, the provider shall follow the conformity assessment procedure \n",
      "set out in Annex VII.  \n",
      "For the purpose of the conformity assessment procedure referred to in Annex VII, the \n",
      "provider may choose any of the notified bodies. However, when the  system is \n",
      "intended to be put into service by law enforcement, immigration or asylum \n",
      "authorities as well as EU institutions, bodies or agencies, the market surveillance \n",
      "authority referred to in Article 63(5) or (6), as applicable, shall act as a notified b ody. \n",
      "2. For high -risk AI systems referred to in points 2 to 8 of Annex III, providers shall \n",
      "follow the conformity assessment procedure based on internal control as referred to \n",
      "in Annex VI, which does not provide for the involvement of a notified body.  For \n",
      "high-risk AI systems referred to in point 5(b) of Annex III, placed on the market or \n",
      "put into service by credit institutions regulated by Directive 2013/36/EU, the \n",
      "conformity assessment  shall be carried out as part of the procedure referred to in \n",
      "Articles 97 to101 of that Directive.  \n",
      "3. For high -risk AI systems, to which legal acts listed in Annex II, section A, apply, the \n",
      "provider shall follow the relevant conformity assessment as required under those \n",
      "legal acts. The requirements set out in Chapter 2 of thi s Title  shall apply to those \n",
      "                                                 \n",
      "63 Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 April 2019 on ENISA \n",
      "(the European Uni on Agency for Cybersecurity) and on information and communications technology \n",
      "cybersecurity certification and repealing Regulation (EU) No 526/2013 (Cybersecurity Act) (OJ L 151, \n",
      "7.6.2019, p. 1).  EN 65  EN high-risk AI systems and shall be part of that assessment. Points 4.3., 4.4., 4.5. and \n",
      "the fifth paragraph of point 4.6 of Annex VII shall also apply.  \n",
      "For the purpose of that assessment, notified bodies which have been notifie d under \n",
      "those legal acts shall be entitled to control the conformity of the high -risk AI systems \n",
      "with the requirements set out in Chapter 2 of this Title , provided that the compliance \n",
      "of those notified bodies with requirements laid down in Article 33(4), ( 9) and (10) \n",
      "has been assessed in the context of the notification procedure under those legal acts.  \n",
      "Where the legal acts listed in Annex II, section A, enable the manufacturer of the \n",
      "product to opt out from a third -party conformity assessment, provided that  that \n",
      "manufacturer has applied all harmonised standards covering all the relevant \n",
      "requirements, that manufacturer  may make use of that option only if he has also \n",
      "applied harmonised standards or, where applicable, common specifications referred \n",
      "to in Articl e 41, covering the requirements set out in Chapter 2 of this Title.  \n",
      "4. High -risk AI systems shall undergo a new conformity assessment procedure \n",
      "whenever they are substantially modified, regardless of whether the modified system \n",
      "is intended to be further distributed or continues to be used by the current user.  \n",
      "For high -risk AI systems that continue to learn after being placed on the market or \n",
      "put into service, changes to the high -risk AI system and its performance that have \n",
      "been pre -determined by the provi der at the moment of the initial conformity \n",
      "assessment and are part of the information contained in the technical documentation \n",
      "referred to in point 2(f) of Annex IV, shall not constitute a substantial modification.  \n",
      "5. The Commission is empowered to adopt delegated acts in accordance with Article 73 \n",
      "for the purpose of updating Annexes VI and Annex VII in order to introduce \n",
      "elements of the conformity assessment procedures that become necessary in light of \n",
      "technical progress.  \n",
      "6. The Commission is empowered to  adopt delegated acts to amend paragraphs 1 and 2 \n",
      "in order to subject high -risk AI systems referred to in points 2 to 8 of Annex III to \n",
      "the conformity assessment procedure referred to in Annex VII or parts thereof. The \n",
      "Commission shall adopt such delegated  acts taking into account the effectiveness of \n",
      "the conformity assessment procedure based on internal control referred to in Annex \n",
      "VI in preventing or minimizing the risks to health and safety and protection of \n",
      "fundamental rights posed by such systems as we ll as the availability of adequate \n",
      "capacities and resources among notified bodies.  \n",
      "Article 44  \n",
      "Certificates  \n",
      "1. Certificates issued by notified bodies in accordance with Annex VII shall be drawn -\n",
      "up in an official Union language determined by the Member State  in which the \n",
      "notified body is established or in an official Union language otherwise acceptable to \n",
      "the notified body.  \n",
      "2. Certificates shall be valid for the period they indicate, which shall not exceed five \n",
      "years. On application by the provider, the vali dity of a certificate may be extended \n",
      "for further periods, each not exceeding five years, based on a re -assessment in \n",
      "accordance with the applicable conformity assessment procedures.  \n",
      "3. Where a notified body finds that an AI system no longer meets the requirements set \n",
      "out in Chapter 2 of this Title , it shall, taking account of the principle of EN 66  EN proportionality, suspend or withdraw the certificate issued or impose any restrictions \n",
      "on it, unless compliance with those requirements is ensured by appropriate cor rective \n",
      "action taken by the provider of the system within an appropriate deadline set by the \n",
      "notified body. The notified body shall give reasons for its decision.  \n",
      "Article 45  \n",
      "Appeal against decisions of notified bodies  \n",
      "Member States shall ensure that an app eal procedure against decisions of the notified bodies \n",
      "is available to parties having a legitimate interest in that decision . \n",
      "Article 46  \n",
      "Information obligations of notified bodies  \n",
      "1. Notified bodies shall inform the notifying authority of the following:  \n",
      "(a) any Union technical documentation assessment certificates, any supplements to \n",
      "those certificates, quality management system approvals issued in accordance \n",
      "with the requirements of Annex VII;  \n",
      "(b) any refusal, restriction, suspension or withdrawal of a Un ion technical \n",
      "documentation assessment certificate or a quality management system approval \n",
      "issued in accordance with the requirements of Annex VII;  \n",
      "(c) any circumstances affecting the scope of or conditions for notification;  \n",
      "(d) any request for informatio n which they have received from market surveillance \n",
      "authorities regarding conformity assessment activities;  \n",
      "(e) on request, conformity assessment activities performed within the scope of \n",
      "their notification and any other activity performed, including cross -border \n",
      "activities and subcontracting.  \n",
      "2. Each notified body shall inform the other notified bodies of:  \n",
      "(a) quality management system approvals which it has refused, suspended or \n",
      "withdrawn, and, upon request, of quality system approvals which it has issued;  \n",
      "(b) EU technical documentation assessment certificates or any supplements thereto \n",
      "which it has refused, withdrawn, suspended or otherwise restricted, and, upon \n",
      "request, of the certificates and/or supplements thereto which it has issued.  \n",
      "3. Each notified body shall provide the other notified bodies carrying out similar \n",
      "conformity assessment activities covering the same artificial intelligence \n",
      "technologies with relevant information on issues relating to negative and, on request, \n",
      "positive confo rmity assessment results.  \n",
      "Article 47  \n",
      "Derogation from conformity assessment procedure  \n",
      "1. By way of derogation from Article 43, any market surveillance authority may \n",
      "authorise the placing on the market or putting into service of specific high -risk AI \n",
      "systems  within the territory of the Member State concerned, for exceptional reasons \n",
      "of public security or the protection of life and health of persons, environmental \n",
      "protection and the protection of key industrial and infrastructural assets. That \n",
      "authorisation sh all be for a limited period of time, while the necessary conformity EN 67  EN assessment procedures are being carried out, and shall terminate once those \n",
      "procedures have been completed.  The completion of those procedures shall be \n",
      "undertaken without undue delay.  \n",
      "2. The authorisation referred to in paragraph 1 shall be issued only if the market \n",
      "surveillance authority concludes that the high -risk AI system complies with the \n",
      "requirements of Chapter 2 of this Title. The market surveillance authority shall \n",
      "inform the Commi ssion and the other Member States of any authorisation issued \n",
      "pursuant to paragraph 1.  \n",
      "3. Where, within 15 calendar days of receipt of the information referred to in paragraph \n",
      "2, no objection has been raised by either a Member State or the Commission in \n",
      "respect of an authorisation issued by a market surveillance authority of a Member \n",
      "State in accordance with paragraph 1, that authorisation shall be deemed justified.  \n",
      "4. Where, within 15 calendar days of receipt of the notification referred to in paragraph \n",
      "2, objections are raised by a Member State against an authorisation issued by a \n",
      "market surveillance authority of another Member State, or where the Commission \n",
      "considers the authorisation to be contrary to Union law or the conclusion of the \n",
      "Member States rega rding the compliance of the system as referred to in paragraph 2 \n",
      "to be unfounded, the Commission shall without delay enter into consultation with the \n",
      "relevant Member State; the operator(s) concerned shall be consulted and have the \n",
      "possibility to present th eir views. In view thereof, the Commission shall decide \n",
      "whether the authorisation is justified or not. The Commission shall address its \n",
      "decision to the Member State concerned and the relevant operator or operators.  \n",
      "5. If the authorisation is considered unj ustified, this shall be withdrawn by the market \n",
      "surveillance authority of the Member State concerned.  \n",
      "6. By way of derogation from paragraphs 1 to 5, for high -risk AI systems intended to be \n",
      "used as safety components of devices, or which are themselves devi ces, covered by \n",
      "Regulation (EU) 2017/745 and Regulation (EU) 2017/746, Article 59 of Regulation \n",
      "(EU) 2017/745 and Article 54 of Regulation (EU) 2017/746 shall apply also with \n",
      "regard to the derogation from the conformity assessment of the compliance with th e \n",
      "requirements set out in Chapter 2 of this Title.  \n",
      "Article 48  \n",
      "EU declaration of conformity  \n",
      "1. The provider shall draw up a written EU declaration of conformity for each AI \n",
      "system and keep it at the disposal of the national competent authorities for 10 years \n",
      "after the AI system has been placed on the market or put into service. The EU \n",
      "declaration of co nformity shall identify the AI system for which it has been drawn \n",
      "up. A copy of the EU declaration of conformity shall be given to the relevant \n",
      "national competent authorities upon request.  \n",
      "2. The EU declaration of conformity shall state that the high -risk AI system in question \n",
      "meets the requirements set out in Chapter 2 of this Title . The EU declaration of \n",
      "conformity shall contain the information set out in Annex V and shall be translated \n",
      "into an official Union language or languages required by the Member S tate(s) in \n",
      "which the high -risk AI system is made available.  \n",
      "3. Where high -risk AI systems are subject to other Union harmonisation legislation \n",
      "which also requires an EU declaration of conformity, a single EU declaration of \n",
      "conformity shall be drawn up in respect of all Union legislations applicable to the EN 68  EN high-risk AI system. The declaration shall contain all the information required for \n",
      "identification of the Union harmonisation legislation to which the declaration relates.  \n",
      "4. By drawing up the EU declara tion of conformity, the provider shall assume \n",
      "responsibility for compliance with the requirements set out in Chapter 2 of this Title . \n",
      "The provider shall keep the EU declaration of conformity up -to-date as appropriate.  \n",
      "5. The Commission shall be empowered t o adopt delegated acts in accordance with \n",
      "Article 73 for the purpose of updating the content of the EU declaration of \n",
      "conformity set out in Annex V in order to introduce elements that become necessary \n",
      "in light of technical progress.  \n",
      "Article 49  \n",
      "CE marking o f conformity  \n",
      "1. The CE marking shall be affixed visibly, legibly and indelibly for high -risk AI \n",
      "systems. Where that is not possible or not warranted on account of the nature of the \n",
      "high-risk AI system, it shall be affixed to the packaging or to the accompa nying \n",
      "documentation, as appropriate.  \n",
      "2. The CE marking referred to in paragraph 1 of this Article shall be subject to the \n",
      "general principles set out in Article 30 of Regulation (EC) No 765/2008.  \n",
      "3. Where applicable, the CE marking shall be followed by the  identification number of \n",
      "the notified body responsible for the conformity assessment procedures set out in \n",
      "Article 43. The identification number shall also be indicated in any promotional \n",
      "material which mentions that the high -risk AI system fulfils the re quirements for CE \n",
      "marking.  \n",
      "Article 50  \n",
      "Document retention  \n",
      "The provider shall, for a period ending 10 years after the AI system has been placed on the \n",
      "market or put into service, keep at the disposal of the national competent authorities:  \n",
      "(a) the technical documentation referred to in Article 11;  \n",
      "(b) the documentation concerning the quality management system referred to Article 17;  \n",
      "(c) the documentation concerning the changes approved by notified bodies where \n",
      "applicable;  \n",
      "(d) the decisions and other documen ts issued by the notified bodies where applicable;  \n",
      "(e) the EU declaration of conformity referred to in Article 48.  \n",
      "Article 51  \n",
      "Registration  \n",
      "Before placing on the market or putting into service a high -risk AI system referred to in \n",
      "Article 6(2), the provider  or, where applicable, the authorised representative shall register that \n",
      "system in the EU database referred to in Article 60.  EN 69  EN TITLE  IV \n",
      "TRANSPARENCY  OBLIGATIONS  FOR  CERTAIN  AI SYSTEMS  \n",
      "Article 52  \n",
      "Transparency obligations for certain AI systems  \n",
      "1. Providers shall ensure that AI systems intended to interact with natural persons are \n",
      "designed and developed in such a way that natural persons are informed that they are \n",
      "interacting with an AI system, unless this is obvious from the circumstances and the \n",
      "context of use. This obligation shall not apply to AI systems authorised by law to \n",
      "detect, prevent, investigate and prosecute criminal offences, unless those systems are \n",
      "available for the public to report a criminal offence.  \n",
      "2. Users of an emotion recognition system or a biometric categorisation system shall \n",
      "inform of the operation of the system the natural persons exposed thereto. This \n",
      "obligation shall not apply to AI systems used for biometric categorisation, which are \n",
      "permitted by law to detect, prevent and investi gate criminal offences.  \n",
      "3. Users of an AI system that generates or manipulates image, audio or video content \n",
      "that appreciably resembles  existing persons, objects, places or other entities or events  \n",
      "and would falsely appear to a person to be authentic or tr uthful (‘deep fake’), shall \n",
      "disclose  that the content has been artificially generated or manipulated.  \n",
      "However, the first subparagraph shall not apply where the use is authorised by law to \n",
      "detect, prevent, investigate and prosecute criminal offences  or it is necessary for the \n",
      "exercise of the right to freedom of expression and the right to freedom of the arts and \n",
      "sciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to \n",
      "appropriate safeguards for the rights and freedoms of third part ies. \n",
      "4. Paragraphs 1, 2 and 3 shall not affect the requirements and obligations set out in Title \n",
      "III of this Regulation.  \n",
      "TITLE V  \n",
      "MEASURES  IN SUPPORT  OF INNOVATION  \n",
      "Article 53  \n",
      "AI regulatory sandboxes  \n",
      "1. AI regulatory sandboxes established by one or more Member States competent \n",
      "authorities or the European Data Protection Supervisor shall provide a controlled \n",
      "environment that facilitates the development, testing and validation of innovative AI \n",
      "systems for a  limited time before their placement on the market or putting into \n",
      "service  pursuant to a specific plan. This shall take place under the direct supervision \n",
      "and guidance by the competent authorities  with a view to ensuring compliance with \n",
      "the requirements of  this Regulation and, where relevant, other Union and Member \n",
      "States legislation supervised within the sandbox.  \n",
      "2. Member States shall ensure that to the extent the innovative AI systems involve the \n",
      "processing of personal data or otherwise fall under the s upervisory remit of other \n",
      "national authorities or competent authorities providing or supporting access to data, EN 70  EN the national data protection authorities and those other national authorities are \n",
      "associated to the operation of the AI regulatory sandbox.  \n",
      "3. The AI regulatory sandboxes shall not affect the supervisory and corrective powers \n",
      "of the competent authorities. Any significant risks to health and safety and \n",
      "fundamental rights identified during the development and testing of such systems \n",
      "shall result in immediate mitigation and, failing that, in the suspension of the \n",
      "development and testing process until such mitigation takes place.  \n",
      "4. Participants in the AI regulatory sandbox shall remain liable under applicable Union \n",
      "and Member States liability legislat ion for any harm inflicted on third parties as a \n",
      "result from the experimentation taking place in the sandbox.  \n",
      "5. Member States’ competent authorities that have established AI regulatory sandboxes \n",
      "shall coordinate their activities and cooperate within the f ramework of the European \n",
      "Artificial Intelligence Board. They shall submit annual reports to the Board and the \n",
      "Commission on the results from the implementation of those scheme, including good \n",
      "practices, lessons learnt and recommendations on their setup and , where relevant, on \n",
      "the application of this Regulation and other Union legislation supervised within the \n",
      "sandbox.  \n",
      "6. The modalities and the conditions of the operation of the AI regulatory sandboxes, \n",
      "including the eligibility criteria and the procedure f or the application, selection, \n",
      "participation and exiting from the sandbox, and the rights and obligations of the \n",
      "participants shall be set out in implementing acts. Those implementing acts shall be \n",
      "adopted in accordance with the examination procedure refer red to in Article 74(2).  \n",
      "Article 54  \n",
      "Further processing of personal data for developing certain AI systems in the public interest in \n",
      "the AI regulatory sandbox  \n",
      "1. In the AI regulatory sandbox personal data lawfully collected for other purposes shall \n",
      "be proce ssed for the purposes of developing and testing certain innovative AI \n",
      "systems in the sandbox under the following conditions:  \n",
      "(a) the innovative AI systems shall be developed for safeguarding substantial \n",
      "public interest in one or more of the following areas : \n",
      "(i) the prevention, investigation, detection or prosecution of criminal \n",
      "offences or the execution of criminal penalties, including the \n",
      "safeguarding against and the prevention of threats to public security, \n",
      "under the control and responsibility of the comp etent authorities. The \n",
      "processing shall be based on Member State or Union law;  \n",
      "(ii) public safety and public health, including disease prevention, control and \n",
      "treatment;  \n",
      "(iii) a high level of protection and improvement of the quality of the \n",
      "environment;  \n",
      "(b) the data processed are necessary for complying with one or more of the \n",
      "requirements referred to in Title III, Chapter 2 where those requirements \n",
      "cannot be effectively fulfilled by processing anonymised, synthetic or other \n",
      "non-personal data;  EN 71  EN (c) there a re effective monitoring mechanisms to identify if any high risks to the \n",
      "fundamental rights of the data subjects may arise during the sandbox \n",
      "experimentation as well as response mechanism to promptly mitigate those \n",
      "risks and, where necessary, stop the proce ssing;  \n",
      "(d) any personal data to be processed in the context of the sandbox are in a \n",
      "functionally separate, isolated and protected data processing environment \n",
      "under the control of the participants and only authorised persons have access to \n",
      "that data;  \n",
      "(e) any personal data processed are not be transmitted, transferred or otherwise \n",
      "accessed by other parties;  \n",
      "(f) any processing of personal data in the context of the sandbox do not lead to \n",
      "measures or decisions affecting the data subjects;  \n",
      "(g) any personal da ta processed in the context of the sandbox are deleted once the \n",
      "participation in the sandbox has terminated or the personal data has reached the \n",
      "end of its retention period;  \n",
      "(h) the logs of the processing of personal data in the context of the sandbox are  \n",
      "kept for the duration of the participation in the sandbox and 1 year after its \n",
      "termination, solely for the purpose of and only as long as necessary for \n",
      "fulfilling accountability and documentation obligations under this Article or \n",
      "other application Union o r Member States legislation;  \n",
      "(i) complete and detailed description of the process and rationale behind the \n",
      "training, testing and validation of the AI system is kept together with the \n",
      "testing results as part of the technical documentation in Annex IV;  \n",
      "(j) a short summary of the AI project developed in the sandbox, its objectives and \n",
      "expected results published on the website of the competent authorities.  \n",
      "2. Paragraph 1 is without prejudice to Union or Member States legislation excluding \n",
      "processing for other p urposes than those explicitly mentioned in that legislation.  \n",
      "Article 55  \n",
      "Measures for small -scale providers and users  \n",
      "1. Member States shall undertake the following actions:  \n",
      "(a) provide small -scale providers and start -ups with priority access to the AI \n",
      "regulatory sandboxes to the extent that they fulfil the eligibility conditions;  \n",
      "(b) organise specific awareness raising activities about the application of this \n",
      "Regulation tailored to the needs of the small -scale providers and users ; \n",
      "(c) where appropriate, establish a dedicated channel for communication with \n",
      "small -scale providers and user and other innovators to provide guidance and \n",
      "respond to queries about the implementation of this Regulation.  \n",
      "2. The specific interests and needs of the s mall-scale providers shall be taken into \n",
      "account when setting the fees for conformity assessment under Article 43, reducing \n",
      "those fees proportionately to their size and market size.  EN 72  EN TITLE  VI \n",
      "GOVERNANCE  \n",
      "CHAPTER 1 \n",
      "EUROPEAN ARTIFICIAL INTELLIGENCE BOARD  \n",
      "Artic le 56  \n",
      "Establishment of the European Artificial Intelligence Board  \n",
      "1. A ‘European Artificial Intelligence Board’ (the ‘Board’) is established.  \n",
      "2. The Board shall provide advice and assistance to the Commission in order to:  \n",
      "(a) contribute to the effective co operation of the national supervisory authorities \n",
      "and the Commission with regard to matters covered by this Regulation;  \n",
      "(b) coordinate and contribute to guidance and analysis by the Commission and the \n",
      "national supervisory authorities and other competent au thorities on emerging \n",
      "issues across the internal market with regard to matters covered by this \n",
      "Regulation;  \n",
      "(c) assist the national supervisory authorities and the Commission in ensuring the \n",
      "consistent application of this Regulation.  \n",
      "Article 57  \n",
      "Structure of the Board  \n",
      "1. The Board shall be composed of the national supervisory authorities, who shall be \n",
      "represented by the head or equivalent high -level official of that authority, and the \n",
      "European Data Protection Supervisor. Other national authoritie s may be invited to \n",
      "the meetings, where the issues discussed are of relevance for them.  \n",
      "2. The Board shall adopt its rules of procedure by a simple majority of its members, \n",
      "following the consent of the Commission. The rules of procedure shall also contain \n",
      "the operational aspects related to the execution of the Board’s tasks as listed in \n",
      "Article 58. The Board may establish sub -groups as appropriate  for the purpose of \n",
      "examining specific questions.  \n",
      "3. The Board shall be chaired by the Commission. The Commissio n shall convene the \n",
      "meetings and prepare the agenda in accordance with the tasks of the Board pursuant \n",
      "to this Regulation and with its rules of procedure. The Commission shall provide \n",
      "administrative and analytical support for the activities of the Board pu rsuant to this \n",
      "Regulation.  \n",
      "4. The Board may invite external experts and observers to attend its meetings and may \n",
      "hold exchanges with interested third parties to inform its activities to an appropriate \n",
      "extent. To that end the Commission may facilitate excha nges between the Board and \n",
      "other Union bodies, offices, agencies and advisory groups.  EN 73  EN Article 58  \n",
      "Tasks of the Board  \n",
      "When providing advice and assistance to the Commission in the context of Article 56(2), the \n",
      "Board shall  in particular:  \n",
      "(a) collect and share  expertise and best practices among Member States;  \n",
      "(b) contribute to uniform administrative practices in the Member States, including for \n",
      "the functioning of regulatory sandboxes referred to in Article 53;  \n",
      "(c) issue opinions, recommendations or written cont ributions on matters related to the \n",
      "implementation of this Regulation, in particular  \n",
      "(i) on technical specifications or existing standards regarding the requirements set \n",
      "out in Title III, Chapter 2 ,  \n",
      "(ii) on the use of harmonised standards or common specif ications referred to in \n",
      "Articles 40 and 41,  \n",
      "(iii) on the preparation of guidance documents, including the guidelines concerning \n",
      "the setting of administrative fines referred to in Article 71.  \n",
      "CHAPTER  2 \n",
      "NATIONAL COMPETENT A UTHORITIES  \n",
      "Article 59  \n",
      "Designation o f national competent authorities  \n",
      "1. National competent authorities shall be established or designated by each Member \n",
      "State for the purpose of ensuring the application and implementation of this \n",
      "Regulation. National competent authorities shall be organised  so as to safeguard the \n",
      "objectivity and impartiality of their activities and tasks.  \n",
      "2. Each Member State shall designate a national supervisory authority among the \n",
      "national competent authorities.  The national supervisory authority shall act as \n",
      "notifying au thority and market surveillance authority unless a Member State has \n",
      "organisational and administrative reasons to designate more than one authority.  \n",
      "3. Member States shall inform the Commission of their designation or designations and, \n",
      "where applicable, the  reasons for designating more than one authority.  \n",
      "4. Member States shall ensure that national competent authorities are provided with \n",
      "adequate financial and human resources to fulfil their tasks under this Regulation. In \n",
      "particular, national competent aut horities shall have a sufficient number of personnel \n",
      "permanently available whose competences and expertise shall include an in -depth \n",
      "understanding of artificial intelligence technologies, data and data computing, \n",
      "fundamental rights, health and safety risks  and knowledge of existing standards and \n",
      "legal requirements.  \n",
      "5. Member States shall report to the Commission on an annual basis on the status of the \n",
      "financial and human resources of the national competent authorities with an \n",
      "assessment of their adequacy. The Commission shall transmit that information to the \n",
      "Board for discussion and possible recommendations.  \n",
      "6. The Commission shall facilitate the exchange of experience between national \n",
      "competent authorities.  EN 74  EN 7. National competent authorities may provide gu idance and advice on the \n",
      "implementation of this Regulation, including to small -scale providers. Whenever \n",
      "national competent authorities intend to provide guidance and advice with regard to \n",
      "an AI system in areas covered by other Union legislation, the compe tent national \n",
      "authorities under that Union legislation shall be consulted, as appropriate. Member \n",
      "States may also establish one central contact point for communication with operators.  \n",
      "8. When Union institutions, agencies and bodies fall within the scope of  this \n",
      "Regulation, the European Data Protection Supervisor shall act as the competent \n",
      "authority for their supervision.  \n",
      "TITLE VII  \n",
      "EU DATABASE  FOR  STAND -ALONE  HIGH -RISK  AI SYSTEMS  \n",
      "Article 60  \n",
      "EU database for stand -alone high -risk AI systems  \n",
      "1. The Commission s hall, in collaboration with the Member States, set up and maintain \n",
      "a EU database containing information referred to in paragraph 2 concerning high -risk \n",
      "AI systems referred to in Article 6(2) which are registered in accordance with Article \n",
      "51. \n",
      "2. The data listed in Annex VIII shall be entered into the EU database by the providers.  \n",
      "The Commission shall provide them with technical and administrative support.  \n",
      "3. Information contained in the EU database shall be accessible to the public.  \n",
      "4. The EU database sha ll contain personal data only insofar as necessary for collecting \n",
      "and processing information in accordance with this Regulation. That information \n",
      "shall include the names and contact details of natural persons who are responsible for \n",
      "registering the system and have the legal authority to represent the provider.  \n",
      "5. The Commission shall be the controller of the EU database. It shall also ensure to \n",
      "providers adequate technical and administrative support.  \n",
      "TITLE  VIII \n",
      "POST -MARKET  MONITORING,  INFORMATION  SHARING,  MARKET  \n",
      "SURVEILLANCE  \n",
      "CHAPTER 1 \n",
      "POST-MARKET MONITORING  \n",
      "Article 61  \n",
      "Post-market monitoring by providers and post -market monitoring plan for high -risk AI \n",
      "systems  \n",
      "1. Providers shall establish and document a post -market monitoring system in a manner \n",
      "that is propor tionate to the nature of the artificial intelligence technologies and the \n",
      "risks of the high -risk AI system.  EN 75  EN 2. The post -market monitoring system shall actively and systematically collect, \n",
      "document and analyse relevant data provided by users or collected th rough other \n",
      "sources on the performance of high -risk AI systems throughout their lifetime, and \n",
      "allow the provider to evaluate the continuous compliance of AI systems with the \n",
      "requirements set out in Title III, Chapter 2 . \n",
      "3. The post -market monitoring system  shall be based on a post -market monitoring plan. \n",
      "The post -market monitoring plan shall be part of the technical documentation \n",
      "referred to in Annex IV. The Commission shall adopt an implementing act laying \n",
      "down detailed provisions establishing a template f or the post -market monitoring plan \n",
      "and the list of elements to be included in the plan.  \n",
      "4. For high -risk AI systems covered by the legal acts referred to in Annex II, where a \n",
      "post-market monitoring system and plan is already established under that legislat ion, \n",
      "the elements described in paragraphs 1, 2 and 3 shall be integrated into that system \n",
      "and plan as appropriate.  \n",
      "The first subparagraph shall also apply to high -risk AI systems referred to in point \n",
      "5(b) of Annex III placed on the market or put into servi ce by credit institutions \n",
      "regulated by Directive 2013/36/EU.  \n",
      "CHAPTER 2 \n",
      "SHARING OF INFORMATIO N ON INCIDENTS AND M ALFUNCTIONING  \n",
      "Article 62  \n",
      "Reporting of serious incidents and of malfunctioning  \n",
      "1. Providers of high -risk AI systems placed on the Union market shall report any \n",
      "serious incident or any malfunctioning of those systems which constitutes a breach of \n",
      "obligations under Union law intended to protect fundamental rights to the market \n",
      "surveillanc e authorities of the Member States where that incident or breach occurred.  \n",
      "Such notification shall be made immediately after the provider has established a \n",
      "causal link between the AI system and the incident or malfunctioning or the \n",
      "reasonable likelihood o f such a link, and, in any event, not later than 15 days after the \n",
      "providers  becomes aware of the serious incident or of the malfunctioning.  \n",
      "2. Upon receiving a notification related to a breach of obligations under Union law \n",
      "intended to protect fundamental  rights, the market surveillance authority shall inform \n",
      "the national public authorities or bodies referred to in Article 64(3). The Commission \n",
      "shall develop dedicated guidance to facilitate compliance with the obligations set out \n",
      "in paragraph 1. That guida nce shall be issued 12 months after the entry into force of \n",
      "this Regulation, at the latest.  \n",
      "3. For high -risk AI systems referred to in point 5(b) of Annex III which are placed on \n",
      "the market or put into service by providers that are credit institutions regu lated by \n",
      "Directive 2013/36/EU and for high -risk AI systems which are safety components of \n",
      "devices, or are themselves devices, covered by Regulation (EU) 2017/745 and \n",
      "Regulation (EU) 2017/746, the notification of serious incidents or malfunctioning \n",
      "shall be  limited to those that that constitute a breach of obligations under Union law \n",
      "intended to protect fundamental rights.  EN 76  EN CHAPTER 3 \n",
      "ENFORCEMENT  \n",
      "Article 63  \n",
      "Market surveillance and control of AI systems in the Union market  \n",
      "1. Regulation (EU) 2019/1020 shall ap ply to AI systems covered by this Regulation. \n",
      "However, for the purpose of the effective enforcement of this Regulation:  \n",
      "(a) any reference to an economic operator under Regulation (EU) 2019/1020 shall \n",
      "be understood as including all operators identified in T itle III, Chapter 3 of this \n",
      "Regulation;  \n",
      "(b) any reference to a product under Regulation (EU) 2019/1020 shall be \n",
      "understood as including all AI systems falling within the scope of this \n",
      "Regulation.  \n",
      "2. The national supervisory authority shall report to the Commission on a regular basis \n",
      "the outcomes of relevant market surveillance activities. The national supervisory \n",
      "authority shall report, without delay, to the Commission and relevant national \n",
      "competition authorities any information identified in the course of market \n",
      "surveillance activities that may be of potential interest for the application of Union \n",
      "law on competition rules.  \n",
      "3. For high -risk AI systems, related to products to which legal acts listed in Annex II, \n",
      "section A apply, the market surveillance aut hority for the purposes of this Regulation \n",
      "shall be the authority responsible for market surveillance activities designated under \n",
      "those legal acts.  \n",
      "4. For AI systems placed on the market, put into service or used by financial institutions \n",
      "regulated by Unio n legislation on financial services, the market surveillance \n",
      "authority for the purposes of this Regulation shall be the relevant authority \n",
      "responsible for the financial supervision of those institutions under that legislation.  \n",
      "5. For AI systems listed in p oint 1(a) in so far as the systems are used for law \n",
      "enforcement purposes, points 6 and 7  of Annex III, Member States shall designate as \n",
      "market surveillance authorities for the purposes of this Regulation either the \n",
      "competent data protection supervisory aut horities under Directive (EU) 2016/680, or \n",
      "Regulation 2016/679 or the national competent authorities supervising the activities \n",
      "of the law enforcement, immigration or asylum authorities putting into service or \n",
      "using those systems.  \n",
      "6. Where Union institutio ns, agencies and bodies fall within the scope of this \n",
      "Regulation, the European Data Protection Supervisor shall act as their market \n",
      "surveillance authority.  \n",
      "7. Member States shall facilitate the coordination between market surveillance \n",
      "authorities designate d under this Regulation and other relevant national authorities or \n",
      "bodies which supervise the application of Union harmonisation legislation listed in \n",
      "Annex II  or other Union legislation that might be relevant for the high -risk AI \n",
      "systems referred to in An nex III . EN 77  EN Article 64  \n",
      "Access to data and documentation  \n",
      "1. Access to data and documentation i n the context of their activities, t he market \n",
      "surveillance authorities shall be granted full access to the training, validation and \n",
      "testing datasets used by the provi der, including through application programming \n",
      "interfaces (‘API’) or other appropriate technical means and tools enabling remote \n",
      "access.  \n",
      "2. Where necessary to assess the conformity of the high -risk AI system with the \n",
      "requirements set out in Title III, Chap ter 2 and upon a reasoned request, the market \n",
      "surveillance authorities shall be granted access to the source code of the AI system . \n",
      "3. National public authorities or bodies which supervise or enforce the respect of \n",
      "obligations under Union law protecting fu ndamental rights in relation to the use of \n",
      "high-risk AI systems referred to in Annex III shall have the power to request and \n",
      "access any documentation created or maintained under this Regulation when access \n",
      "to that documentation is necessary for the fulfilm ent of the competences under their \n",
      "mandate within the limits of their jurisdiction. The relevant public authority or body \n",
      "shall inform the market surveillance authority of the Member State concerned of any \n",
      "such request.  \n",
      "4. By 3 months after the entering in to force of this Regulation, each Member State shall \n",
      "identify the public authorities or bodies referred to in paragraph 3 and make a list \n",
      "publicly available on the website of the national supervisory authority. Member \n",
      "States shall notify the list to the Co mmission and all other Member States and keep \n",
      "the list up to date.  \n",
      "5. Where the documentation referred to in paragraph 3 is insufficient to ascertain \n",
      "whether a breach of obligations under Union law intended to protect fundamental \n",
      "rights has occurred, the public authority or body referred to paragraph 3 may make a \n",
      "reasoned request to the market surveillance authority to organise testing of the high -\n",
      "risk AI system through technical means. The market surveillance authority shall \n",
      "organise the testing with the close involvement of the requesting public authority or \n",
      "body within reasonable time following the request.  \n",
      "6. Any information and documentation obtained by the national public authorities or \n",
      "bodies referred to in paragraph 3 pursuant to the provisions of this Article shall be \n",
      "treated in compliance with the confidentiality obligations set out in Article 70.  \n",
      "Article 65  \n",
      "Procedure for dealing with AI systems presenting a risk at national level  \n",
      "1. AI systems presenting a risk shall be understood as a product pr esenting a risk \n",
      "defined in Article 3, point 19 of Regulation (EU) 2019/1020 insofar as risks to the \n",
      "health or safety or to the protection of fundamental rights of persons are concerned.  \n",
      "2. Where the market surveillance authority of a Member State has suffi cient reasons to \n",
      "consider that an AI system presents a risk as referred to in paragraph 1, they shall \n",
      "carry out an evaluation of the AI system concerned in respect of its compliance with \n",
      "all the requirements and obligations laid down in this Regulation. Wh en risks to the \n",
      "protection of fundamental rights are present, the market surveillance authority shall \n",
      "also inform the relevant national public authorities or bodies referred to in Article \n",
      "64(3).  The relevant operators shall cooperate as necessary with the market EN 78  EN surveillance authorities and the other national public authorities or bodies referred to \n",
      "in Article 64(3).  \n",
      "Where, in the course of that evaluation, the market surveillance authority finds that \n",
      "the AI system does not comply with the requirements and obligations laid down in \n",
      "this Regulation, it shall without delay require the relevant operator to take all \n",
      "appropriate corrective actions to bring the AI system into compliance, to withdraw \n",
      "the AI system from the market, or to recall it within a reasonable  period, \n",
      "commensurate with the nature of the risk, as it may prescribe.  \n",
      "The market surveillance authority shall inform the relevant notified body \n",
      "accordingly. Article 18 of Regulation (EU) 2019/1020 shall apply to the measures \n",
      "referred to in the second subparagraph.  \n",
      "3. Where the market surveillance authority considers that non -compliance is not \n",
      "restricted to its national territory, it shall inform the Commission and the other \n",
      "Member States of the results of the evaluation and of the actions which it has \n",
      "required the operator to take.  \n",
      "4. The operator shall ensure that all appropriate corrective action is taken in respect of \n",
      "all the AI systems concerned that it has made available on the market throughout the \n",
      "Union.  \n",
      "5. Where the operator of an AI system does  not take adequate corrective action within \n",
      "the period referred to in paragraph 2, the market surveillance authority shall take all \n",
      "appropriate provisional measures to prohibit or restrict the AI system's being made \n",
      "available on its national market, to wit hdraw the product from that market or to recall \n",
      "it. That authority shall inform the Commission and the other Member States, without \n",
      "delay, of those measures.  \n",
      "6. The information referred to in paragraph 5 shall include all available details, in \n",
      "particular t he data necessary for the identification of the non -compliant AI system, \n",
      "the origin of the AI system, the nature of the non -compliance alleged and the risk \n",
      "involved, the nature and duration of the national measures taken and the arguments \n",
      "put forward by th e relevant operator. In particular, the market surveillance authorities \n",
      "shall indicate whether the non -compliance is due to one or more of the following:  \n",
      "(a) a failure of the AI system to meet requirements set out in Title III, Chapter 2;  \n",
      "(b) shortcomings  in the harmonised standards or common specifications referred to \n",
      "in Articles 40 and 41 conferring a presumption of conformity.  \n",
      "7. The market surveillance authorities of the Member States other than the market \n",
      "surveillance authority of the Member State ini tiating the procedure shall without \n",
      "delay inform the Commission and the other Member States of any measures adopted \n",
      "and of any additional information at their disposal relating to the non -compliance of \n",
      "the AI system concerned, and, in the event of disagree ment with the notified national \n",
      "measure, of their objections.  \n",
      "8. Where, within three months of receipt of the information referred to in paragraph 5, \n",
      "no objection has been raised by either a Member State or the Commission in respect \n",
      "of a provisional measure taken by a Member State, that measure shall be deemed \n",
      "justified . This is without prejudice to the procedural rights of the concerned operator \n",
      "in accordance with Article 18 of Regulation (EU) 2019/1020.  EN 79  EN 9. The market surveillance authorities of all Member States shall ensure that \n",
      "appropriate restrictive measures are t aken in respect of the product concerned, such \n",
      "as withdrawal of the product from their market, without delay.  \n",
      "Article 66  \n",
      "Union safeguard procedure  \n",
      "1. Where, within three months of receipt of the notification referred to in Article 65(5), \n",
      "objections are rai sed by a Member State against a measure taken by another Member \n",
      "State, or where the Commission considers the measure to be contrary to Union law, \n",
      "the Commission shall without delay enter into consultation with the relevant Member \n",
      "State and operator or oper ators and shall evaluate the national measure. On the basis \n",
      "of the results of that evaluation, the Commission shall decide whether the national \n",
      "measure is justified or not within 9 months from the notification referred to in Article \n",
      "65(5) and notify such d ecision to the Member State concerned.  \n",
      "2. If the national measure is considered justified, all Member States shall take the \n",
      "measures necessary to ensure that the non -compliant AI system is withdrawn from \n",
      "their market, and shall inform the Commission accor dingly. If the national measure \n",
      "is considered unjustified, the Member State concerned shall withdraw the measure.  \n",
      "3. Where the national measure is considered justified and the non -compliance of the AI \n",
      "system is attributed to shortcomings in the harmonised standards or common \n",
      "specifications referred to in Articles 40 and 41 of this Regulation, the Commission \n",
      "shall apply the procedure provided for in Article 11 of Regulation (EU) No \n",
      "1025/2012.  \n",
      "Article 67  \n",
      "Compliant AI systems which present a risk  \n",
      "1. Where, hav ing performed an evaluation under Article 65, the market surveillance \n",
      "authority of a Member State finds that although an AI system is in compliance with \n",
      "this Regulation, it presents a risk to the health or safety of persons, to the compliance \n",
      "with obligati ons under Union or national law intended to protect fundamental rights \n",
      "or to other aspects of public interest protection, it shall require the relevant operator \n",
      "to take all appropriate measures to ensure that the AI system concerned, when placed \n",
      "on the mar ket or put into service, no longer presents that risk, to withdraw the AI \n",
      "system from the market or to recall it within a reasonable period, commensurate with \n",
      "the nature of the risk, as it may prescribe.  \n",
      "2. The provider or other relevant operators shall en sure that corrective action is taken in \n",
      "respect of all the AI systems concerned that they have made available on the market \n",
      "throughout the Union within the timeline prescribed by the market surveillance \n",
      "authority of the Member State referred to in paragrap h 1. \n",
      "3. The Member State shall immediately inform the Commission and the other Member \n",
      "States. That information shall include all available details, in particular the data \n",
      "necessary for the identification of the AI system concerned, the origin and the suppl y \n",
      "chain of the AI system, the nature of the risk involved and the nature and duration of \n",
      "the national measures taken.  \n",
      "4. The Commission shall without delay enter into consultation with the Member States \n",
      "and the relevant operator and shall evaluate the nati onal measures taken. On the basis EN 80  EN of the results of that evaluation, the Commission shall decide whether the measure is \n",
      "justified or not and, where necessary, propose appropriate measures.  \n",
      "5. The Commission shall address its decision to the Member States.  \n",
      "Article 68  \n",
      "Formal non -compliance  \n",
      "1. Where the market surveillance authority of a Member State makes one of the \n",
      "following findings, it shall require the relevant provider to put an end to the non -\n",
      "compliance concerned:  \n",
      "(a) the conformity marking has been aff ixed in violation of Article 49;  \n",
      "(b) the conformity marking has not been affixed;  \n",
      "(c) the EU declaration of conformity has not been drawn up;  \n",
      "(d) the EU declaration of conformity has not been drawn up correctly;  \n",
      "(e) the identification number of the notifie d body, which is involved in the \n",
      "conformity assessment procedure, where applicable, has not been affixed;  \n",
      "2. Where the non -compliance referred to in paragraph 1 persists, the Member State \n",
      "concerned shall take all appropriate measures to restrict or prohibi t the high -risk AI \n",
      "system being made available on the market or ensure that it is recalled or withdrawn \n",
      "from the market.  \n",
      "TITLE  IX \n",
      "CODES  OF CONDUCT  \n",
      "Article 69  \n",
      "Codes of conduct  \n",
      "1. The Commission and the Member States shall encourage and facilitate the drawing \n",
      "up of codes of conduct intended to foster the voluntary application to AI systems \n",
      "other than high -risk AI systems of the requirements set out in Title III, Chapter 2 on \n",
      "the bas is of technical specifications and solutions that are appropriate means of \n",
      "ensuring compliance with such requirements in light of the intended purpose of the \n",
      "systems.  \n",
      "2. The Commission and the Board shall encourage and facilitate the drawing up of \n",
      "codes o f conduct intended to foster the voluntary application to AI systems of \n",
      "requirements related for example to environmental sustainability, accessibility for \n",
      "persons with a disability, stakeholders participation in the design and development of \n",
      "the AI system s and diversity of development teams on the basis of clear objectives \n",
      "and key performance indicators to measure the achievement of those objectives.  \n",
      "3. Codes of conduct may be drawn up by individual providers of AI systems or by \n",
      "organisations representing them or by both, including with the involvement of users \n",
      "and any interested stakeholders and their representative organisations. Codes of \n",
      "conduct may cover one or more AI systems taking into account the similarity of the \n",
      "intended purpose of the relevant sy stems.  EN 81  EN 4. The Commission and the Board shall take into account the specific interests and \n",
      "needs of the small -scale providers and start -ups when encouraging and facilitating \n",
      "the drawing up of codes of conduct.  \n",
      "TITLE  X \n",
      "CONFIDENTIALITY  AND  PENALTIES   \n",
      "Article 70 \n",
      "Confidentiality  \n",
      "1. National competent authorities and notified bodies involved in the application of this \n",
      "Regulation shall respect the confidentiality of information and data obtained in \n",
      "carrying out their tasks and activities in such a manner as to pro tect, in particular:  \n",
      "(a) intellectual property rights, and confidential business information or trade \n",
      "secrets of a natural or legal person, including source code, except the cases \n",
      "referred to in Article 5 of Directive 2016/943 on the protection of undisclosed \n",
      "know -how and business  information (trade secrets) against their unlawful \n",
      "acquisition, use and disclosure apply.  \n",
      "(b) the effective implementation of this Regulation, in particular for the purpose of \n",
      "inspections, investigations or audits; (c) public and national security interes ts;  \n",
      "(c) integrity of criminal or administrative  proceedings.  \n",
      "2. Without prejudice to paragraph 1, information exchanged on a confidential basis \n",
      "between the national competent authorities and between national competent \n",
      "authorities and the Commission shall not be disclosed without the prior consultation \n",
      "of the originating national competent authority and the user when high -risk AI \n",
      "systems referred to in points 1, 6 and 7 of Annex III are used by law enforcement, \n",
      "immigration or asylum authorities, when such d isclosure would jeopardise public and \n",
      "national security interests.  \n",
      "When the law enforcement, immigration or asylum authorities are providers of high -\n",
      "risk AI systems referred to in points 1, 6 and 7 of Annex III, the technical \n",
      "documentation referred to in A nnex IV shall remain within the premises of those \n",
      "authorities. Those authorities shall ensure that the market surveillance authorities \n",
      "referred to in Article 63(5) and (6), as applicable, can, upon request, immediately \n",
      "access the documentation or obtain a copy thereof. Only staff of the market \n",
      "surveillance authority holding the appropriate level of security clearance shall be \n",
      "allowed to access that documentation or any copy thereof.  \n",
      "3. Paragraphs 1 and 2 shall not affect the rights and obligations of the Co mmission, \n",
      "Member States and notified bodies with regard to the exchange of information and \n",
      "the dissemination of warnings, nor the obligations of the parties concerned to provide \n",
      "information under criminal law of the Member States.  \n",
      "4. The Commission and Mem ber States may exchange, where necessary, confidential \n",
      "information with regulatory authorities of third countries with which they have \n",
      "concluded bilateral or multilateral confidentiality arrangements guaranteeing an \n",
      "adequate level of confidentiality.  EN 82  EN Artic le 71  \n",
      "Penalties  \n",
      "1. In compliance with the terms and conditions laid down in this Regulation, Member \n",
      "States shall lay down the rules on penalties, including administrative fines, applicable \n",
      "to infringements of this Regulation and shall take all measures nec essary to ensure \n",
      "that they are properly and effectively implemented. The penalties provided for shall \n",
      "be effective, proportionate, and dissuasive. They shall take into particular account the \n",
      "interests of small -scale providers and start -up and their economi c viability.  \n",
      "2. The Member States shall notify the Commission of those rules and of those measures \n",
      "and shall notify it, without delay, of any subsequent amendment affecting them.  \n",
      "3. The following infringements shall be subject to administrative fines of u p to 30 000 \n",
      "000 EUR or, if the offender is company, up to 6 % of its total worldwide annual \n",
      "turnover for the  preceding financial year, whichever is higher:  \n",
      "(a) non-compliance with the prohibition of the artificial intelligence practices \n",
      "referred to in Arti cle 5;  \n",
      "(b) non-compliance of the AI system with the requirements laid down in Article \n",
      "10. \n",
      "4. The non -compliance of the AI system with any requirements or obligations under \n",
      "this Regulation, other than those laid down in Articles 5 and 10, shall be subject t o \n",
      "administrative fines of up to 20 000 000 EUR or, if the offender is a company, up to \n",
      "4 % of its total worldwide annual turnover for the preceding financial year, \n",
      "whichever is higher.  \n",
      "5. The supply of incorrect, incomplete or misleading information to not ified bodies and \n",
      "national competent authorities in reply to a request shall be subject to administrative \n",
      "fines of up to 10 000 000 EUR or, if the offender is a company, up to 2 % of its total \n",
      "worldwide annual turnover for the preceding financial year, whic hever is higher.  \n",
      "6. When deciding on the amount of the administrative fine in each individual case, all \n",
      "relevant circumstances of the specific situation shall be taken into account and due \n",
      "regard shall be given to the following:  \n",
      "(a) the nature, gravity and  duration of the infringement and of its consequences;  \n",
      "(b) whether administrative fines have been already applied by other market \n",
      "surveillance authorities to the same operator for the same infringement.  \n",
      "(c) the size and market share of the operator committing the infringement;  \n",
      "7. Each Member State shall lay down rules on whether and to what extent \n",
      "administrative fines may be imposed on public authorities and bodies established in \n",
      "that Member State.  \n",
      "8. Depending on the legal system of the Member State s, the rules on administrative \n",
      "fines may be applied in such a manner that the fines are imposed by competent \n",
      "national courts of other bodies as applicable in those Member States.  The application \n",
      "of such rules in those Member States shall have  an equivalent effect.  EN 83  EN Article 72  \n",
      "Administrative fines on Union institutions, agencies and  bodies  \n",
      "1. The European Data Protection Supervisor may impose administrative fines on Union \n",
      "institutions, agencies and bodies falling within the scope of this Regulat ion. When \n",
      "deciding whether to impose an administrative fine and deciding on the amount of the \n",
      "administrative fine in each individual case, all relevant circumstances of the specific \n",
      "situation shall be taken into account and due regard shall be given to the  following:  \n",
      "(a) the nature, gravity and duration of the infringement and of its consequences;  \n",
      "(b) the cooperation with the European Data Protection Supervisor in order to \n",
      "remedy the infringement and mitigate the possible adverse effects of the \n",
      "infringement , including compliance with any of the measures previously \n",
      "ordered by the European Data Protection Supervisor against the Union \n",
      "institution or agency or body concerned with regard to the same subject matter;  \n",
      "(c) any similar previous infringements by the Un ion institution, agency or body;  \n",
      "2. The following infringements shall be subject to administrative fines of up to 500 000 \n",
      "EUR:  \n",
      "(a) non-compliance with the prohibition of the artificial intelligence practices \n",
      "referred to in Article 5;  \n",
      "(b) non-compliance of the AI system with the requirements laid down in Article \n",
      "10. \n",
      "3. The non -compliance of the AI system with any requirements or obligations under \n",
      "this Regulation, other than those laid down in Articles 5 and 10, shall be subject to \n",
      "administrative fines of up to 250 000 EUR.  \n",
      "4. Before taking decisions pursuant to this Article, the European Data Protection \n",
      "Supervisor shall give the Union institution, agency or body  which is the subject of \n",
      "the proceedings conducted by the European Data Protection Supervisor the \n",
      "opportunity of being heard on the matter regarding the possible infringement. The \n",
      "European Data Protection Supervisor shall base his or her decisions only on elements \n",
      "and circumstances on which the parties concerned have been able to comment. \n",
      "Complainants, if any, shall be associated closely with the proceedings.  \n",
      "5. The rights of defense of the parties concerned shall be fully respected in the \n",
      "proceedings. They shall be entitled to have access to the European Data Protection \n",
      "Supervisor’s file, subject to the  legitimate interest of individuals or undertakings in \n",
      "the protection of their personal data or business secrets.  \n",
      "6. Funds collected by imposition of fines in this Article shall be the income of the \n",
      "general budget of the Union.  \n",
      "TITLE  XI \n",
      "DELEGATION  OF POWER  AND  COMMITTEE  PROCEDURE   \n",
      "Article 73  \n",
      "Exercise of the delegation  \n",
      "1. The power to adopt delegated acts is conferred on the Commission subject to the \n",
      "conditions laid down in this Article.  EN 84  EN 2. The delegation of power referred to in Article 4, Article 7(1), Arti cle 11(3), Article \n",
      "43(5) and (6) and Article 48(5) shall be conferred on the Commission for an \n",
      "indeterminate period of time from [ entering into force of the Regulation ]. \n",
      "3. The delegation of power referred to in Article 4, Article 7(1), Article 11(3), Arti cle \n",
      "43(5) and (6) and Article 48(5) may be revoked at any time by the European \n",
      "Parliament or by the Council. A decision of revocation shall put an end to the \n",
      "delegation of power specified in that decision. It shall take effect the day following \n",
      "that of its  publication in the Official Journal of the European Union  or at a later date \n",
      "specified therein. It shall not affect the validity of any delegated acts already in force.  \n",
      "4. As soon as it adopts a delegated act, the Commission shall notify it simultaneously  to \n",
      "the European Parliament and to the Council.  \n",
      "5. Any delegated act adopted pursuant to Article 4, Article 7(1), Article 11(3), Article \n",
      "43(5) and (6) and Article 48(5) shall enter into force only if no objection has been \n",
      "expressed by either the European P arliament or the Council within a period of three \n",
      "months of notification of that act to the European Parliament and the Council or if, \n",
      "before the expiry of that period, the European Parliament and the Council have both \n",
      "informed the Commission that they wil l not object. That period shall be extended by \n",
      "three months at the initiative of the European Parliament or of the Council.  \n",
      "Article 74  \n",
      "Committee procedure  \n",
      "1. The Commission shall be assisted by a committee. That committee shall be a \n",
      "committee within the me aning of Regulation (EU) No 182/2011.  \n",
      "2. Where reference is made to this paragraph, Article 5 of Regulation (EU) No \n",
      "182/2011 shall apply.  \n",
      "TITLE  XII \n",
      "FINAL  PROVISIONS   \n",
      "Article 75  \n",
      "Amendment to Regulation (EC) No 300/2008  \n",
      "In Article 4(3) of Regulation (EC) No 300/2008, the following subparagraph is added:  \n",
      "“When adopting detailed measures related to technical specifications and procedures for \n",
      "approval and use of security equipment concerning Artificial Intelligence systems in the \n",
      "meaning of Regulation (EU) YYY/X X [on Artificial Intelligence]  of the European Parliament \n",
      "and of the Council* , the requirements set out in Chapter 2, Title III of that Regulation shall be \n",
      "taken into account.”  \n",
      "__________  \n",
      "* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”  \n",
      "Artic le 76  \n",
      "Amendment to Regulation (EU) No 167/2013  \n",
      "In Article 17(5) of Regulation (EU) No 167/2013, the following subparagraph is added:  EN 85  EN “When adopting delegated acts pursuant to the first subparagraph concerning artificial \n",
      "intelligence systems which are safet y components in the meaning of Regulation (EU) \n",
      "YYY/XX [on Artificial Intelligence]  of the European Parliament and of the Council*, the \n",
      "requirements set out in Title III, Chapter 2 of that Regulation  shall be taken into account.  \n",
      "__________  \n",
      "* Regulation (EU)  YYY/XX [on Artificial Intelligence] (OJ …).”  \n",
      "Article 77  \n",
      "Amendment to Regulation (EU) No 168/2013  \n",
      "In Article 22(5) of Regulation (EU) No 168/2013, the following subparagraph is added:  \n",
      "“When adopting delegated acts pursuant to the first subparagraph concern ing Artificial \n",
      "Intelligence systems which are safety components in the meaning of Regulation (EU) \n",
      "YYY/XX on [Artificial Intelligence]  of the European Parliament and of the Council*, the \n",
      "requirements set out in Title III, Chapter 2 of that Regulation  shall be taken into account.  \n",
      "__________  \n",
      "* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”  \n",
      "Article 78  \n",
      "Amendment to Directive 2014/90/EU  \n",
      "In Article 8 of Directive 2014/90/EU, the following paragraph is added:  \n",
      "“4. For Artificial Intelligence systems  which are safety components in the meaning of \n",
      "Regulation (EU) YYY/XX [on Artificial Intelligence]  of the European Parliament and of the \n",
      "Council*, when carrying out its activities pursuant to paragraph 1 and when adopting \n",
      "technical specifications and testing standards in accordance with paragraphs 2 and 3, the \n",
      "Commission shall take into account the requirements set out in Title III, Chapter 2 of that \n",
      "Regulation.  \n",
      "__________  \n",
      "* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.  \n",
      "Article 79  \n",
      "Amendment to Directive (EU) 2016/797  \n",
      "In Article 5 of Directive (EU) 2016/797, the following paragraph is added:  \n",
      "“12. When adopting delegated acts pursuant to paragraph 1 and implementing acts pursuant to \n",
      "paragraph 11 concerning Artificial Intelligence systems  which are  safety components in the \n",
      "meaning of Regulation (EU) YYY/XX [on Artificial Intelligence]  of the European Parliament \n",
      "and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation  shall be \n",
      "taken into account.  \n",
      "__________  \n",
      "* Regulatio n (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.  EN 86  EN Article 80  \n",
      "Amendment to Regulation (EU) 2018/858  \n",
      "In Article 5 of Regulation (EU) 2018/858 the following paragraph is added:  \n",
      "“4. When adopting delegated acts pursuant to paragraph 3 concerning Artificial Intelligence \n",
      "systems which are safety components in the meaning of Regulation (EU) YYY/XX [on \n",
      "Artificial Intelligence]  of the European Parliament and of the Council *, the requirements set \n",
      "out in Title III, Chapter 2 of that Regulation  shall be taken into account.  \n",
      "__________  \n",
      "* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.  \n",
      "Article 81  \n",
      "Amendment to Regulation (EU) 2018/1139  \n",
      "Regulation (EU) 2018/1139 is amended as follows:  \n",
      "(1) In Article 17, the following paragraph is added:  \n",
      "“3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph \n",
      "1 concerning Artificial Intelligence systems which are safety components in the meaning of \n",
      "Regulation (EU) YYY/XX [ on Artificial Intelligence ] of the European Parl iament and of the \n",
      "Council*, the requirements set out in Title III, Chapter 2 of that Regulation  shall be taken into \n",
      "account.  \n",
      "__________  \n",
      "* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”  \n",
      "(2) In Article 19, the following paragraph is added:  \n",
      "“4. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \n",
      "Intelligence systems which are safety components in the meaning of Regulation (EU) \n",
      "YYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of th at \n",
      "Regulation  shall be taken into account.”  \n",
      "(3) In Article 43, the following paragraph is added:  \n",
      "“4. When adopting implementing acts pursuant to paragraph 1 concerning Artificial \n",
      "Intelligence systems which are safety components in the meaning of Regulation  (EU) \n",
      "YYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that \n",
      "Regulation  shall be taken into account.”  \n",
      "(4) In Article 47, the following paragraph is added:  \n",
      "“3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \n",
      "Intelligence systems which are safety components in the meaning of Regulation (EU) \n",
      "YYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that \n",
      "Regulation  shall be taken into account.”  \n",
      "(5) In Article  57, the following paragraph is added:  \n",
      "“When adopting those implementing acts concerning Artificial Intelligence systems which are \n",
      "safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence], \n",
      "the requirements set out in Title II I, Chapter 2 of that Regulation  shall be taken into account.”  \n",
      "(6) In Article 58, the following paragraph is added:  EN 87  EN “3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \n",
      "Intelligence systems which are safety components in the  meaning of Regulation (EU) \n",
      "YYY/XX [on Artificial Intelligence] , the requirements set out in Title III, Chapter 2 of that \n",
      "Regulation  shall be taken into account.”.  \n",
      "Article 82  \n",
      "Amendment to Regulation (EU) 2019/2144  \n",
      "In Article 11 of Regulation (EU) 2019/214 4, the following paragraph is added:  \n",
      "“3. When adopting the implementing acts pursuant to paragraph 2,  concerning artificial \n",
      "intelligence systems which are safety components in the meaning of Regulation (EU) \n",
      "YYY/XX [on Artificial Intelligence] of the Europe an Parliament and of the Council*, the \n",
      "requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.  \n",
      "__________  \n",
      "* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.  \n",
      "Article 83  \n",
      "AI systems already placed on the marke t or put into service  \n",
      "1. This Regulation shall not apply to the AI systems which are components of the large -\n",
      "scale IT systems established by the legal acts listed in Annex IX that have been \n",
      "placed on the market or put into service before [12 months after t he date of \n",
      "application of this Regulation referred to in Article 85(2) ], unless the replacement or \n",
      "amendment of those legal acts leads to a significant change in the design or intended \n",
      "purpose of the AI system or AI systems concerned.  \n",
      "The requirements laid  down in this Regulation shall be taken into account, where \n",
      "applicable, in the evaluation of each large -scale IT systems established by the legal \n",
      "acts listed in Annex IX to be undertaken as provided for in those respective acts.  \n",
      "2. This Regulation shall ap ply to the high -risk AI systems, other than the ones referred \n",
      "to in paragraph 1, that have been placed on the market or put into service before \n",
      "[date of application of this Regulation referred to in Article 85(2) ], only if, from that \n",
      "date, those systems are subject to significant changes in their design or intended \n",
      "purpose.  \n",
      "Article 84  \n",
      "Evaluation and review  \n",
      "1. The Commission shall assess the need for amendment of the list in Annex III once a \n",
      "year following the entry into force of this Regulation.  \n",
      "2. By [three years after the date of application of this Regulation referred to in Article \n",
      "85(2) ] and every four years thereafter, the Commission shall submit a report on the \n",
      "evaluation and review of this Regulation to the European Parliament and to the \n",
      "Council. The reports shall be made public.   \n",
      "3. The reports referred to in paragraph 2 shall devote specific attention to the following:  \n",
      "(a) the status of the financial and human resources of the national competent \n",
      "authorities in order to effectively perform the ta sks assigned to them under this \n",
      "Regulation;  EN 88  EN (b) the state of penalties, and notably administrative fines as referred to in Article \n",
      "71(1), applied by Member States to infringements of the provisions of this \n",
      "Regulation.  \n",
      "4. Within [ three years after the date of application of this Regulation referred to in \n",
      "Article 85(2) ] and every four years thereafter, the Commission shall evaluate the \n",
      "impact and effectiveness of codes of conduct to foster the application of the \n",
      "requirements set out in Title III, Chapter 2 and possibly other additional requirements \n",
      "for AI systems other than high -risk AI systems.  \n",
      "5. For the purpose of paragraphs 1 to 4 the Board, the Member States and national \n",
      "competent authorities shall provide the Commission with information on its request.  \n",
      "6. In carrying out the evaluations and reviews referred to in paragraphs 1 to 4 the \n",
      "Commission shall take into account the positions and findings of the Board, of the \n",
      "European Parliament, of the Council, and of other relevant bodies or sources.  \n",
      "7. The Commi ssion shall, if necessary, submit appropriate proposals to amend this \n",
      "Regulation, in particular taking into account developments in technology and in the \n",
      "light of the state of progress in the information society.  \n",
      "Article 85  \n",
      "Entry into force and application  \n",
      "1. This Regulation shall enter into force on the twentieth day following that of its \n",
      "publication in the Official Journal of the European Union . \n",
      "2. This Regulation shall apply from [24 months following the entering into force of the \n",
      "Regulation].  \n",
      "3. By way of derogation from  paragraph 2:  \n",
      "(a) Title III, Chapter 4  and Title VI  shall apply from [three months following the \n",
      "entry into force of this Regulation];  \n",
      "(b) Article 71 shall apply from [twelve months following the entry into force of \n",
      "this Regulation].  \n",
      "This Regulation shall be binding in its entirety and directly applicable in all Member States.  \n",
      "Done at Brussels,  \n",
      "For the European Parliament  For the Council  \n",
      "The President  The President  EN 89  EN LEGISLATIVE FINANCIAL STATEMENT  \n",
      "1. FRAMEWORK  OF THE  PROPOSAL/INITIATIVE   \n",
      " 1.1.  Title of the proposal/initiative  \n",
      " 1.2.  Policy area(s) concerned  \n",
      " 1.3.  The proposal/initiative relates to:  \n",
      " 1.4.  Objective(s)  \n",
      "1.4.1.   General objective(s)  \n",
      " 1.4.2.  Specific objective(s)  \n",
      "1.4.3.  Expected result(s) and impact  \n",
      "1.4.4.  Indicators of performance  \n",
      " 1.5.  Grounds for the proposal/initiative  \n",
      " 1.5.1.   Requirement(s) to be met in the short or long term including a detailed \n",
      "timeline for roll -out of the implementation of the initiative  \n",
      "1.5.2.  Added v alue of Union involvement (it may result from different factors, e.g. \n",
      "coordination gains, legal certainty, greater effectiveness or complementarities). For \n",
      "the purposes of this point 'added value of Union involvement' is the value resulting \n",
      "from Union inte rvention which is additional to the value that would have been \n",
      "otherwise created by Member States alone  \n",
      "1.5.3.   Lessons learned from similar experiences in the past  \n",
      "1.5.4. Compatibility with the Multiannual Financial Framework and possible \n",
      "synergies with other appropriate instruments  \n",
      "1.5.5   Assessment of the different available financing options, including scope for \n",
      "redeployment  \n",
      "1.6.      Duration and financial impact of the proposal/initiative  \n",
      " 1.7.   Management mode(s) planned  \n",
      "2. MANAGEMENT  MEASURES   \n",
      " 2.1.  Monitoring and reporting rules  \n",
      " 2.2.  Management and control system  \n",
      "2.2.1. Justification of the management mode(s), the funding implementation \n",
      "mechanism(s), the payment modalities and the control strategy proposed  \n",
      "2.2.2. Information concerning the risks identified and the internal control system(s) \n",
      "set up to mitigate them  \n",
      "2.2.3. Estimation and justification of the cost -effectiveness of the controls (ratio of \n",
      "\"control costs ÷ value of the related funds managed\"), and assessment of the \n",
      "expected levels  of risk of error (at payment & at closure)  EN 90  EN  2.3. Measures to prevent fraud and irregularities  \n",
      "3. ESTIMATED  FINANCIAL  IMPACT  OF THE  PROPOSAL/INITIATIVE   \n",
      " 3.1. Heading(s) of the multiannual financial framework and expenditure budget \n",
      "line(s) affected  \n",
      " 3.2. Estimated financial impact of the proposal on appropriations   \n",
      " 3.2.1.  Summary of estimated impact on operational appropriations  \n",
      " 3.2.2.  Estimated output funded with operational appropriations  \n",
      " 3.2.3. Summary of estimated impact on administrative appropri ations  \n",
      " 3.2.4.  Compatibility with the current multiannual financial framework  \n",
      " 3.2.5.  Third -party contributions  \n",
      " 3.3. Estimated impact on revenueEN 91  EN LEGISLATIVE FINANCIAL STATEMENT  \n",
      "1. FRAMEWORK  OF THE  PROPOSAL/INITIATIVE   \n",
      "1.1. Title of the proposal/initiative  \n",
      "Regulation of the European Parliament and of the Council Laying Down Harmonised \n",
      "Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain \n",
      "Union Legislative Acts  \n",
      "1.2. Policy area(s) concerned  \n",
      "Communications Networks, Content and Technology;  \n",
      "Internal Market, Industry, Entrepreneurship and SMEs;  \n",
      "The budgetary impact concerns the new tasks entrusted with the Commission, \n",
      "including the support to the EU AI Board;  \n",
      "Activity: Shaping Europe's digital future.  \n",
      "1.3. The proposal/initiative relates to:  \n",
      "X  a new action   \n",
      " a new action following a pilot project/preparatory action64  \n",
      " the extension of an existing action   \n",
      " an action redirected towards a new action   \n",
      "1.4. Objective(s)  \n",
      "1.4.1.  General objective( s)  \n",
      "The general objective of the intervention is to ensure the proper functioning of the \n",
      "single market by creating the conditions for the development and use of trustworthy \n",
      "artificial intelligence in the Union.  \n",
      "1.4.2.  Specific objective(s)  \n",
      "Specific objective No 1  \n",
      "To set requirements specific to AI systems and obligations on all value chain \n",
      "participants in order to ensure that AI systems placed on the market and used are safe \n",
      "and respect existing law on fundamental rights and Union values;  \n",
      "Specific ob jective No 2  \n",
      "To ensure legal certainty to facilitate investment and innovation in AI by making it \n",
      "clear what essential requirements, obligations, as well as conformity and compliance \n",
      "procedures must be followed to place or use an AI system in the Union mar ket; \n",
      "Specific objective No 3  \n",
      "To enhance governance and effective enforcement of existing law on fundamental \n",
      "rights and safety requirements applicable to AI systems by providing new powers, \n",
      "resources and clear rules for relevant authorities on conformity as sessment and ex \n",
      "                                                 \n",
      "64 As referred to in Article 54(2)(a) or (b) of the Financial  Regulation   EN 92  EN post monitoring procedures and the division of governance and supervision tasks \n",
      "between national and EU levels;  \n",
      "Specific objective No 4  \n",
      "To facilitate the development of a single market for lawful, safe and trustworthy AI \n",
      "applications and prevent market fragmentation by taking EU action to set minimum \n",
      "requirement for AI systems to be placed and used in the Union market in compliance \n",
      "with existing law on fundamental rights and safety.  EN 93  EN 1.4.3.  Expected result(s) and impact  \n",
      "Specify the effects  which the proposal/initiative should have on the beneficiaries/groups targeted.  \n",
      "AI suppliers should benefit from a minimal but clear set of requirements, creating \n",
      "legal certainty and ensuring access to the entire single market.  \n",
      "AI users should benefit fro m legal certainty that the high -risk AI systems they buy \n",
      "comply with European laws and values.  \n",
      "Consumers should benefit by reducing the risk of violations of their safety or \n",
      "fundamental rights.  \n",
      "1.4.4.  Indicators of performance  \n",
      "Specify the indicators for mo nitoring implementation of the proposal/initiative.  \n",
      "Indicator 1  \n",
      "Number of serious incidents or AI performances which constitute a serious incident \n",
      "or a breach of fundamental rights obligations (semi -annual) by fields of applications \n",
      "and calculated a) in ab solute terms, b) as share of applications deployed and c) as \n",
      "share of citizens concerned.  \n",
      "Indicator 2  \n",
      "a) Total AI investment in the EU (annual)  \n",
      "b) Total AI investment by Member State (annual)  \n",
      "c) Share of companies using AI (annual)  \n",
      "d) Share of SMEs using AI (annual)  \n",
      "a) and b) will be calculated based on official sources and benchmarked against \n",
      "private estimates  \n",
      "c) and d) will be collected by regular company surveys  \n",
      "1.5. Grounds for the proposal/initiative  \n",
      "1.5.1.  Requirement(s) to be met in the short or lo ng term including a detailed timeline for \n",
      "roll-out of the implementation of the initiative  \n",
      "The Regulation should be fully applicable one year and a half after its adoption. \n",
      "However, elements of the governance structure should be in place before then. In \n",
      "particular, Member States shall have appointed existing authorities and/or established \n",
      "new authorities performing the tasks set out in the legislation earlier, and the EU AI \n",
      "Board should be set -up and effective. By the time of applicability, the European \n",
      "database of AI systems should be fully operative. In parallel to the adoption process, \n",
      "it is therefore necessary to develop the database, so that its development has come to \n",
      "an end when the regulation enters into force.  \n",
      "1.5.2.  Added value of Union involvement  (it may result from different factors, e.g. \n",
      "coordination gains, legal certainty, greater effectiveness or complementarities). For \n",
      "the purposes of this point 'added value of Union involvement' is the value resulting \n",
      "from Union intervention which is additio nal to the value that would have been \n",
      "otherwise created by Member States alone.  \n",
      "An emerging patchy framework of potentially divergent national rules will hamper \n",
      "the seamless provision of AI systems across the EU and is ineffective in ensuring the EN 94  EN safety a nd protection of fundamental rights and Union values across the different \n",
      "Member States. A common EU legislative action on AI could boost the internal \n",
      "market and has great potential to provide European industry with a competitive edge \n",
      "at the global scene a nd economies of scale that cannot be achieved by individual \n",
      "Member States alone.  \n",
      "1.5.3.  Lessons learned from similar experiences in the past  \n",
      "The E -commerce Directive 2000/31/EC provides the core framework for the \n",
      "functioning of the single market and the su pervision of digital services and sets a \n",
      "basic structure for a general cooperation mechanism among Member States, covering \n",
      "in principle all requirements applicable to digital services. The evaluation of the \n",
      "Directive pointed to shortcomings in several aspe cts of this cooperation mechanism, \n",
      "including important procedural aspects such as the lack of clear timeframes for \n",
      "response from Member States coupled with a general lack of responsiveness to \n",
      "requests from their counterparts. This has led over the years to  a lack of trust \n",
      "between Member States in addressing concerns about providers offering digital \n",
      "services cross -border. The evaluation of the Directive showed the need to define a \n",
      "differentiated set of rules and requirements at European level. For this reaso n, the \n",
      "implementation of the specific obligations laid down in this Regulation would \n",
      "require a specific cooperation mechanism at EU level, with a governance structure \n",
      "ensuring coordination of specific responsible bodies at EU level.  \n",
      "1.5.4.  Compatibility wi th the Multiannual Financial Framework and possible synergies \n",
      "with other appropriate instruments  \n",
      "The Regulation Laying Down Harmonised Rules on Artificial Intelligence and \n",
      "Amending Certain Union Legislative Acts defines a new common framework of \n",
      "requiremen ts applicable to AI systems, which goes well beyond the framework \n",
      "provided by existing legislation. For this reason, a new national and European \n",
      "regulatory and coordination function needs to be established with this proposal.  \n",
      "As regards possible synergies with other appropriate instruments, the role of \n",
      "notifying authorities at national level can be performed by national authorities \n",
      "fulfilling similar functions sunder other EU regulations.  \n",
      "Moreover, by increasing trust in AI and thus encouraging investment i n development \n",
      "and adoption of AI, it complements Digital Europe, for which promoting the \n",
      "diffusion of AI is one of five priorities.  \n",
      "1.5.5.  Assessment of the different available financing options, including scope for \n",
      "redeployment  \n",
      "The staff will be redeploye d. The other costs will be supported from the DEP. \n",
      "envelope, given that the objective of this regulation – ensuring trustworthy AI – \n",
      "contributes directly to one key objective of Digital Europe – accelerating AI \n",
      "development and deployment in Europe.  EN 95  EN 1.6. Duration and financial impact of the proposal/initiative  \n",
      " limited duration  \n",
      "–  in effect from [DD/MM]YYYY to [DD/MM]YYYY  \n",
      "–  Financial impact from YYYY to YYYY for commitment appropriations and \n",
      "from YYYY to YYYY for payment appropriations.  \n",
      "X unlimited durat ion \n",
      "– Implementation with a start -up period from one/two (tbc) year,  \n",
      "– followed by full -scale operation.  \n",
      "1.7. Management mode(s) planned65  \n",
      "X Direct management  by the Commission  \n",
      "–  by its departments, including by its staff in the Union delegations;  \n",
      "–  by the executive agencies  \n",
      " Shared management  with the Member States  \n",
      " Indirect management  by entrusting budget implementation tasks to:  \n",
      "–  third countries or the bodies they have designated;  \n",
      "–  international organisations and their agencies (to be specifi ed); \n",
      "–  the EIB and the European Investment Fund;  \n",
      "–  bodies referred to in Articles 70 and 71 of the Financial Regulation;  \n",
      "–  public law bodies;  \n",
      "–  bodies governed by private law with a public service mission to the extent that \n",
      "they provide adequate financial guarantees;  \n",
      "–  bodies governed by the private law of a Member State that are entrusted with \n",
      "the implementation of a public -private partnership and that provide adequate \n",
      "financial guarantees;  \n",
      "–  persons entrusted with the implementation of specific actions in  the CFSP \n",
      "pursuant to Title V of the TEU, and identified in the relevant basic act.  \n",
      "– If more than one management mode is indicated, please provide details in the ‘Comments’ section.  \n",
      "Comments  \n",
      " \n",
      " \n",
      "                                                 \n",
      "65 Details of management modes and references to the Financial Regulation may be found on the \n",
      "BudgWeb site: http://www.cc.cec/budg/man/budgmanag/budgmanag_en.html  EN 96  EN 2. MANAGEMENT  MEASURES   \n",
      "2.1. Monitoring and reporting rules  \n",
      "Specify frequency and conditions.  \n",
      "The Regulation will be reviewed and evaluated five years from the entry into force of \n",
      "the regulation. The Commission will report on the findings of the evaluation to the \n",
      "European Parliament, the Council and the European Economic and Social \n",
      "Committee.  \n",
      "2.2. Management and control system(s)  \n",
      "2.2.1.  Justification of the management mode(s), the funding implementation mechanism(s), \n",
      "the payment modalities and the control strategy proposed  \n",
      "The Regulation establishes a new policy with regard to harmonised rules for the \n",
      "provision of artificial intelligence systems in the internal market while ensuring the \n",
      "respect of safety and fundamental rights. These new rules require a consistency \n",
      "mechanism for the cross -border application of the obligations under this Regulation \n",
      "in the form of a new advisory group coordinating the activities of national \n",
      "authorities.  \n",
      "In order to face these new tasks, it is necessary to appropriately resource the \n",
      "Commission’s services. The enforcement of the new Regulation is estimated to \n",
      "require 10 FTE à regime (5 FTE for the support to the activities of the Board and 5 \n",
      "FTE for the European Data Protection Supervisor acting as a notifying body for AI \n",
      "systems deployed by a body of the European Union).  \n",
      "2.2.2.  Information concerning the risks identified  and the internal control system(s) set up \n",
      "to mitigate them  \n",
      "In order to ensure that the members of the Board have the possibility to make \n",
      "informed analysis on the basis of factual evidence, it is foreseen that the Board \n",
      "should be supported by the administr ative structure of the Commission and that an \n",
      "expert group be created to provide additional expertise where required.  \n",
      "2.2.3.  Estimate and justification of the cost -effectiveness of the controls (ratio of \"control \n",
      "costs ÷ value of the related funds managed\" ), and assessment of the expected levels \n",
      "of risk of error (at payment & at closure)  \n",
      "For the meeting expenditure, given the low value per transaction (e.g. refunding \n",
      "travel costs for a delegate for a meeting), standard control procedures seem \n",
      "sufficient. Re garding the development of the database, contract attribution has a \n",
      "strong internal control system in place in DG CNECT through centralised \n",
      "procurement activities.  \n",
      "2.3. Measures to prevent fraud and irregularities  \n",
      "Specify existing or envisaged prevention and protection measures, e.g. from the Anti -Fraud Strategy.  \n",
      "The existing fraud prevention measures applicable to the Commission will cover the \n",
      "additional appropriations necessary for this Regulation.   \n",
      "EN 97  EN 3. ESTIMATED  FINANCIAL  IMPACT  OF THE  PROPOSAL/INITIATIVE   \n",
      "3.1. Heading(s) of the multiannual financial framework and expenditure budget line(s) affected  \n",
      " Existing budget lines  \n",
      "In order  of multiannual financial framework headings and budget lines.  \n",
      "Heading of \n",
      "multiannual \n",
      "financial \n",
      "framewo rk Budget line  Type of  \n",
      "expenditure  Contribution  \n",
      "Number  \n",
      " Diff./Non -\n",
      "diff.66 from \n",
      "EFTA \n",
      "countries\n",
      "67 \n",
      " from \n",
      "candidate \n",
      "countries68 \n",
      " from third \n",
      "countries  within the \n",
      "meaning of \n",
      "Article  21(2)(b) of \n",
      "the Financial \n",
      "Regulation  \n",
      "7 20 02 06 Administrative expenditure  Non-diff. NO NO NO NO \n",
      "1 02 04 03 DEP Artificial Intelligence  Diff.  YES  NO NO NO \n",
      "1 02 01 30 01 Support expenditure for \n",
      "the Digital Europe programme  Non-diff. YES  NO NO NO \n",
      "3.2. Estimated financial impact of the proposal on appropriations  \n",
      "3.2.1.  Summary of  estimated impact on expenditure on operational appropriations  \n",
      "–  The proposal/initiative does not require the use of operational appropriations  \n",
      "– X The proposal/initiative requires the use of operational appropriations, as explained below:  \n",
      "EUR million (to t hree decimal places)  \n",
      "                                                 \n",
      "66 Diff. = Differentiated appropriations / Non -diff. = Non -differentiated appropriations.  \n",
      "67 EFTA: European Free Trade Association.  \n",
      "68 Candidate countries and, where applicable, potential candidate countries from the Western Balkans.   \n",
      "EN 98  EN Heading of multiannual financial  \n",
      "framework  1  \n",
      " \n",
      "DG: CNECT     Year \n",
      "2022  Year \n",
      "2023  Year \n",
      "2024  Year \n",
      "2025  Year \n",
      "2026  Year \n",
      "202769 TOTAL  \n",
      " Operational appropriations          \n",
      "Budget line70 02 04 03  Commitments  (1a)  1.000       1.000  \n",
      "Payments  (2a)  0.600  0.100  0.100  0.100  0.100   1.000  \n",
      "Budget line  Commitments  (1b)         \n",
      "Payments  (2b)         \n",
      "Appropriations of an administrative nature financed from the envelope of specific \n",
      "programmes71  \n",
      "         \n",
      "Budget line 02 01 30 01   (3)  0.240  0.240  0.240  0.240  0.240   1.200  \n",
      "TOTAL appropriations  \n",
      "for DG CNECT  Commitments  =1a+1b +3   1.240   0.240  0.240  0.240   2.200  \n",
      " Payments  =2a+2b  \n",
      "+3  0.840  0.340  0.340  0.340  0.340   2.200  \n",
      " \n",
      " \n",
      " \n",
      "                                                 \n",
      "69 Indicative and dependent on budg et availability.  \n",
      "70 According to the official budget nomenclature.  \n",
      "71 Technical and/or administrative assistance and expenditure in support of the implementation of EU programmes and/or actions ( former ‘BA’ lines), indirect research, \n",
      "direct research.   \n",
      "EN 99  EN  TOTAL operational appropriations  Commitments  (4)  1.000       1.000  \n",
      "Payments  (5)  0.600  0.100  0.100  0.100  0.100   1.000  \n",
      " TOTAL appropriations of an administrative nature financed from the \n",
      "envelope for specific programmes  (6)  0.240  0.240  0.240  0.240  0.240   1.200  \n",
      "TOTAL appropriations  \n",
      "under HEADING 1  \n",
      "of the multiannual  financial framework  Commitments  =4+ 6   1.240  0.240  0.240  .0.240  0.240   2.200  \n",
      "Payments  =5+ 6   0.840  0.340  0.340  0.340  0.340   2.200  \n",
      "If more than one heading is affected by the proposal / initiative, repeat the section above:  \n",
      " TOTAL operational appropriations (all \n",
      "operational headings)  Commitments  (4)         \n",
      "Payments  (5)         \n",
      " TOTAL appropriations of an administrative nature \n",
      "financed from the envelope for specific programmes (all \n",
      "operational headings)  (6)         \n",
      "TOTAL appropriations  \n",
      "under HEADINGS 1 to 6  \n",
      "of the multiannual financial framework  \n",
      "(Reference amount)  Commitments  =4+ 6          \n",
      "Payments  =5+ 6          \n",
      " \n",
      "   \n",
      "EN 100  EN Heading of multiannual financial  \n",
      "framework  7 ‘Administrative expenditure’  \n",
      "This section should be filled in using the 'budget data of an administrative nature' to be firstly introduced in the Annex  to the Legislative \n",
      "Financial Statement  (Annex V to the internal rules), which is uploaded to DECIDE for interservice consultation purposes.  \n",
      "EUR million (to three decimal places)  \n",
      "   Year \n",
      "2023  Year \n",
      "2024  Year \n",
      "2025  Year \n",
      "2026  Year 2027  After \n",
      "202772 TOTAL  \n",
      "DG: CNECT  \n",
      " Human resources  0.760  0.760  0.760  0.760  0.760  0.760  3.800  \n",
      " Other administrative expenditure  0.010  0.010  0.010  0.010  0.010  0.010  0.050  \n",
      "TOTAL DG CNECT  Appropriations  0.760  0.760  0.760  0.760  0.760  0.760  3.850  \n",
      "European Data Protection Supervisor   \n",
      " Human resources  0.760  0.760  0.760  0.760  0.760  0.760  3.800  \n",
      " Other administrative expenditure         \n",
      "TOTAL EDPS  Appropriations  0.760  0.760  0.760  0.760  0.760  0.760  3.800  \n",
      "TOTAL appropriations  \n",
      "under HEADING  7 \n",
      "of the multiannual financial framework   (Total commitments = Total payments)  1.530  1.530  1.530  1.530  1.530  1.530  7.650  \n",
      "EUR million (to three decimal places)  \n",
      "   Year \n",
      "2022  Year \n",
      "2023  Year \n",
      "2024  Year \n",
      "2025  Year 2026  Year 2027   TOTAL  \n",
      "TOTAL appropriations  Commitments   2.770  1.770  1.770  1.770  1.770   9.850  \n",
      "                                                 \n",
      "72 All figures in this column are indicative and subject to the continuation of the programmes and availability of appropriations   \n",
      "EN 101  EN under HEADINGS 1 to 7  \n",
      "of the multiannual financial framework   Payments   2.370  1.870  1.870  1.870  1.870  9.850   \n",
      "EN 102  EN 3.2.2.  Estimated output funded with operational appropriations  \n",
      "Commitment appropriations in EUR million (to three decimal places)  \n",
      "Indicate objectives \n",
      "and outputs  \n",
      " \n",
      "   Year \n",
      "2022  Year \n",
      "2023  Year \n",
      "2024  Year \n",
      "2025  Year \n",
      "2026  Year \n",
      "2027  After \n",
      "202773 TOTAL  \n",
      " OUTPUTS  \n",
      " Type  \n",
      " Average \n",
      "cost \n",
      "No \n",
      "Cost \n",
      "No \n",
      "Cost \n",
      "No \n",
      "Cost \n",
      "No \n",
      "Cost \n",
      "No \n",
      "Cost \n",
      "No \n",
      "Cost \n",
      "No \n",
      "Cost Tota\n",
      "l No Total \n",
      "cost \n",
      "SPECIFIC OBJECTIVE No 174…                 \n",
      "Database      1 1.000  1  1  1  1  1 0.100  1 1.000  \n",
      "Meetings - Output      10 0.200  10 0.200  10 0.200  10 0.200  10 0.200  10 0.200  50 1.000  \n",
      "Communication \n",
      "activities      2 0.040  2 0.040  2 0.040  2 0.040  2 0.040  2 0.040  10 0.040  \n",
      "Subtotal for specific objective No 1                  \n",
      "SPECIFIC OBJECTIVE No 2 ...                  \n",
      "- Output                    \n",
      "Subtotal for specific objective No 2                  \n",
      "TOTALS    13 0.240  13 0.240  13 0.240  13 0.240  13 0.240  13 0.100  65 2.200  \n",
      "                                                 \n",
      "73 All figures in this column are indicative and subject to the continuation of the programmes and availability of appropriation s \n",
      "74 As described in point 1.4.2. ‘Specific objective(s)…’   \n",
      "EN 103  EN 3.2.3.  Summary of estimated impact on administrative appropriations   \n",
      "–  The proposal/initiative does not require the use of appropriations of an \n",
      "administrative nature  \n",
      "– X The proposal/initiative requires the use of appropriations of an administrative \n",
      "nature, as explained below:  \n",
      "EUR million (to three decimal places)  \n",
      " Year \n",
      "2022  Year \n",
      "2023  Year \n",
      "2024  Year \n",
      "2025  Year \n",
      "2026  Year \n",
      "2027  Yearly after  \n",
      "202775 TOTAL  \n",
      " \n",
      "HEADING 7  \n",
      "of the multiannual \n",
      "financial framework          \n",
      "Human resources   1.520  1.520  1.520  1.520  1.520  1.520  7.600  \n",
      "Other administrative \n",
      "expenditure   0.010  0.010  0.010  0.010  0.010  0.010  0.050  \n",
      "Subtotal HEADING 7  \n",
      "of the multiannual \n",
      "financial framework   1.530  1.530  1.530  1.530  1.530  1.530  7.650  \n",
      " \n",
      "Outside HEADING 776 \n",
      "of the multiannual \n",
      "financial framework  \n",
      "         \n",
      "Human resources          \n",
      "Other expenditure  \n",
      "of an administrative \n",
      "nature   0.240  0.240  0.240  0.240  0.240  0.240  1.20 \n",
      "Subtotal  \n",
      "outside HEADING 7  \n",
      "of the multiannual \n",
      "financial framework   0.240  0.240  0.240  0.240  0.240  0.240  1.20 \n",
      " \n",
      "TOTAL   1.770  1.770  1.770  1.770  1.770  1.770  8.850  \n",
      "The appropriations required for human resources and other expenditure of an administrative nature will be met by \n",
      "appropriations from the DG that are already assigned to management of the action and/or have been redeployed within the \n",
      "DG, together if necessary with any additional alloc ation which may be granted to the managing DG under the annual \n",
      "allocation procedure and in the light of budgetary constraints.  \n",
      "                                                 \n",
      "75 All figures in this column are indicative and subject to the continuation of the programmes and availability of \n",
      "appropriations.  \n",
      "76 Technical and/or administrative assistance and expenditure in support of  the implementation of \n",
      "EU programmes and/or actions (former ‘BA’ lines), indirect research, direct research.   \n",
      "EN 104  EN 3.2.3.1.  Estimated requirements of human resources  \n",
      "–  The proposal/initiative does not require the use of human resources.  \n",
      "– X The proposal/initiative requires the use of human resources, as explained \n",
      "below:  \n",
      "Estimate to be expressed in full time equivalent units  \n",
      " \n",
      ".  Year \n",
      "2023  Year \n",
      "2024  Year \n",
      "2025  2026  2027  After \n",
      "202777  \n",
      " Establishment plan posts (officials and temporary staff)  \n",
      "20 01 02 01  (Headquarters and Commission’s Representation \n",
      "Offices)  10 10 10 10 10 10  \n",
      "20 01 02 03 (Delegations)         \n",
      "01 01 01 01   (Indirect research)         \n",
      " 01 01 01 11 (Direct research)         \n",
      "Other budget lines (specify)         \n",
      " External staff (in Full Time Equivalent unit: FTE)78 \n",
      " \n",
      "20 02 01  (AC, END, INT from the ‘global envelope’)         \n",
      "20 02 03 (AC, AL, END, INT and JPD in the delegations)         \n",
      "XX 01  xx yy zz  79 \n",
      " - at Headquarters  \n",
      "        \n",
      "- in Delegations         \n",
      "01 01 01 02  (AC, END, INT - Indirect research)         \n",
      " 01 01 01 12 (AC, END, INT - Direct research)         \n",
      "Other budget lines (specify)         \n",
      "TOTAL   10 10 10 10 10 10  \n",
      "XX is the policy area or budget title concerned.  \n",
      "The human resources required will be met by staff from the DG who are already assigned to management of the \n",
      "action and/or have been redeployed within the DG, together if necessary with any additional allocatio n which \n",
      "may be granted to the managing DG under the annual allocation procedure and in the light of budgetary \n",
      "constraints.  \n",
      "EDPS is expected to provide half of the resources required.  \n",
      " \n",
      "Description of tasks to be carried out:  \n",
      "Officials and temporary staff  To prepare a total of 13 -16 meetings, draft reports, continue policy work, e.g. \n",
      "regarding future amendments of the list of high -risk AI applications, and maintain \n",
      "relations with Member States’ authorities will require four AD FTE and 1 AST FTE.  \n",
      "For AI system s developed by the EU institutions, the European Data Protection \n",
      "Supervisor is responsible. Based on past experience, it can be estimated that 5 AD FTE \n",
      "are reuqired to fulfill the EDPS responsibilites under the draft legislation.  \n",
      "                                                 \n",
      "77  All figures in this column are indicative and subject to the continuation of the programmes and \n",
      "availability of appropriations.  \n",
      "78 AC = Contract Staff; AL = Local Staff; END = Seconded National Expert; INT = agency staff; JPD \n",
      "= Junior Professionals in Delegations.  \n",
      "79 Sub-ceiling for external staff covered by operational appropriations (former ‘BA’ lines).   \n",
      "EN 105  EN External staff    \n",
      "EN 106  EN 3.2.4.  Compatibility with the current multiannual financial framework  \n",
      "The proposal/initiative:  \n",
      "– X can be fully financed through redeployment within the relevant heading of the \n",
      "Multiannual Financial Framework (MFF).  \n",
      "No reporgramming is needed.   \n",
      "–  requires use of the unallocated margin under the relevant heading of the MFF \n",
      "and/or use of the special instruments as defined in the MFF Regulation.  \n",
      "Explain what is required, specifying the headings and budget lines concerned, the corresponding \n",
      "amounts, and the instruments proposed to be used.   \n",
      "–  requires a revision of the MFF.  \n",
      "Explain what is required, specifying the headings and budget lines concerned and the corresponding \n",
      "amounts.  \n",
      "3.2.5.  Third -party contributions  \n",
      "The proposal/in itiative:  \n",
      "– X does not provide for co -financing by third parties  \n",
      "–  provides for the co -financing by third parties estimated below:  \n",
      "Appropriations in EUR million (to three decimal places)  \n",
      " Year \n",
      "N80 Year \n",
      "N+1 Year \n",
      "N+2 Year \n",
      "N+3 Enter as many years as necessary \n",
      "to show the duration of the \n",
      "impact (see point 1.6)  Total  \n",
      "Specify the co -financing \n",
      "body           \n",
      "TOTAL appropriations \n",
      "co-financed          \n",
      " \n",
      " \n",
      "                                                 \n",
      "80 Year N is the year in which implementatio n of the proposal/initiative starts. Please replace \"N\" by the \n",
      "expected first year of implementation (for instance: 2021). The same for the following years.   \n",
      "EN 107  EN 3.3. Estimated impact on revenue  \n",
      "–  The proposal/initiative has the following financial impact:  \n",
      "–  The proposal/initiative has the following financial impact:  \n",
      "–  on other revenue  \n",
      "–  on other revenue  \n",
      "– Please indicate, if the revenue is assigned to expenditure lines  \n",
      "EUR million (to three decimal places)  \n",
      "Budget revenue line:  Appropriation\n",
      "s available for \n",
      "the current \n",
      "financial year  Impact of the proposal/initiative81 \n",
      "Year \n",
      "N Year \n",
      "N+1 Year \n",
      "N+2 Year \n",
      "N+3 Enter as many years as necessary to show \n",
      "the duration of the impact (see point 1.6)  \n",
      "Article ………….          \n",
      "For assigned revenue, specify the budget expenditure line(s) affected.  \n",
      "  \n",
      "Other remarks (e.g. method/formula used for calculating the impact on revenue or any other \n",
      "information).  \n",
      " \n",
      "                                                 \n",
      "81 As regards traditional own resources (customs duties, sugar levies), the amounts indicated must be net \n",
      "amounts, i.e. gross amounts after deduction of 20  % for collection costs.  \n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    pdf_reader = PdfReader(file_path)\n",
    "    text = ''\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Extract text from the specified PDF\n",
    "text = extract_text_from_pdf('infos/pdf2.pdf')\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T09:38:56.110825300Z",
     "start_time": "2024-05-17T09:38:42.715624700Z"
    }
   },
   "id": "d4605f0de329715"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1:\n",
      "[[None, 'anonymisation or'], ['pseudonymisation of judicial decisions, documents or data, communication between', None], ['personnel,', None]]\n",
      "Table 2:\n",
      "[['Where a high-risk AI system related to products to which the legal acts listed in Annex II,'], ['section A, apply, is placed on the market or put into service together with the product'], ['manufactured in accordance with those legal acts and under the name of the product'], ['manufacturer, the manufacturer of the product shall take the responsibility of the compliance'], ['of the AI system with this Regulation and, as far as the AI system is concerned, have the same'], ['obligations imposed by the present Regulation on the provider.']]\n",
      "Table 3:\n",
      "[[None, 'or it is necessary for the'], ['exercise of the right to freedom of expression and the right to freedom of the arts and', None], ['sciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to', None], ['appropriate safeguards for the rights and freedoms of third parties.', None]]\n",
      "Table 4:\n",
      "[['Heading of\\nmultiannual\\nfinancial\\nframework', 'Budget line', 'Type of\\nexpenditure', 'Contribution', None, None, None], [None, 'Number', 'Diff./Non-\\n66\\ndiff.', 'from\\nEFTA\\ncountries\\n67', 'from\\ncandidate\\n68\\ncountries', 'from third\\ncountries', 'within the\\nmeaning of\\nArticle 21(2)(b) of\\nthe Financial\\nRegulation'], ['7', '20 02 06 Administrative expenditure', 'Non-diff.', 'NO', 'NO', 'NO', 'NO'], ['1', '02 04 03 DEP Artificial Intelligence', 'Diff.', 'YES', 'NO', 'NO', 'NO'], ['1', '02 01 30 01 Support expenditure for\\nthe Digital Europe programme', 'Non-diff.', 'YES', 'NO', 'NO', 'NO']]\n",
      "Table 5:\n",
      "[['DG: CNECT', '', '', '', 'Year\\n2022', 'Year\\n2023', 'Year\\n2024', 'Year\\n2025', 'Year\\n2026', 'Year\\n69\\n2027', 'TOTAL', None], ['\\uf09f Operational appropriations', None, None, None, '', '', '', '', '', '', '', ''], ['70\\nBudget line 02 04 03', None, 'Commitments', '(1a)', '', '1.000', '', '', '', '', '', '1.000'], [None, None, 'Payments', '(2a)', '', '0.600', '0.100', '0.100', '0.100', '0.100', '', '1.000'], ['Budget line', None, 'Commitments', '(1b)', '', '', '', '', '', '', '', ''], [None, None, 'Payments', '(2b)', '', '', '', '', '', '', '', ''], ['Appropriations of an administrative nature financed from the envelope of specific\\n71\\nprogrammes', None, None, None, '', '', '', '', '', '', '', ''], ['Budget line 02 01 30 01', None, '', '(3)', '', '0.240', '0.240', '0.240', '0.240', '0.240', '', '1.200'], ['TOTAL appropriations\\nfor DG CNECT', None, 'Commitments', '=1a+1b +3', '', '1.240', '', '0.240', '0.240', '0.240', '', '2.200'], ['', None, 'Payments', '=2a+2b\\n+3', '', '0.840', '0.340', '0.340', '0.340', '0.340', '', '2.200']]\n",
      "Table 6:\n",
      "[['\\uf09f TOTAL operational appropriations (all\\noperational headings)', None, None, 'Commitments', '(4)', '', '', '', '', '', '', '', ''], [None, None, None, 'Payments', '(5)', '', '', '', '', '', '', '', ''], ['\\uf09f TOTAL appropriations of an administrative nature\\nfinanced from the envelope for specific programmes (all\\noperational headings)', None, None, None, '(6)', '', '', '', '', '', '', '', ''], ['', 'TOTAL appropriations', '', 'Commitments', '=4+ 6', '', '', '', '', '', '', '', ''], [None, 'under HEADINGS 1 to 6', None, None, None, None, None, None, None, None, None, None, None], [None, None, None, 'Payments', '=5+ 6', '', '', '', '', '', '', '', ''], [None, 'of the multiannual financial framework', None, None, None, None, None, None, None, None, None, None, None], [None, '(Reference amount)', None, None, None, None, None, None, None, None, None, None, None]]\n",
      "Table 7:\n",
      "[['\\uf09f Human resources', None, None, None, None, '0.760', '0.760', '0.760', '0.760', '0.760', '0.760'], ['\\uf09f Other administrative expenditure', None, None, None, None, '0.010', '0.010', '0.010', '0.010', '0.010', '0.010'], ['TOTAL DG CNECT', None, None, None, 'Appropriations', '0.760', '0.760', '0.760', '0.760', '0.760', '0.760'], ['European Data Protection Supervisor', None, None, None, None, None, None, None, None, None, None], ['\\uf09f Human resources', None, None, None, None, '0.760', '0.760', '0.760', '0.760', '0.760', '0.760'], ['\\uf09f Other administrative expenditure', None, None, None, None, '', '', '', '', '', ''], ['TOTAL EDPS', None, None, None, 'Appropriations', '0.760', '0.760', '0.760', '0.760', '0.760', '0.760'], ['', 'TOTAL appropriations', '', '(Total commitments = Total payments)', None, '1.530', '1.530', '1.530', '1.530', '1.530', '1.530'], [None, 'under HEADING 7', None, None, None, None, None, None, None, None, None], [None, 'of the multiannual financial framework', None, None, None, None, None, None, None, None, None]]\n",
      "Table 8:\n",
      "[['', 'under HEADINGS 1 to 7', '', 'Payments', '', '2.370', '1.870', '1.870', '1.870', '1.870'], [None, 'of the multiannual financial framework', None, None, None, None, None, None, None, None]]\n",
      "Table 9:\n",
      "[['Indicate objectives\\nand outputs\\n\\uf0f2', '', '', 'Year\\n2022', None, 'Year\\n2023', None, 'Year\\n2024', None, 'Year\\n2025', None, None, None, 'Year\\n2026', None, 'Year\\n2027', None, 'After\\n73\\n2027', None, 'TOTAL', None], ['', 'OUTPUTS', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], ['', 'Type', 'Average\\ncost', 'o\\nN', 'Cost', 'o\\nN', 'Cost', 'o\\nN', 'Cost', '', 'o\\nN', '', 'Cost', 'o\\nN', 'Cost', 'o\\nN', 'Cost', 'o\\nN', 'Cost', 'Tota\\nl No', 'Total\\ncost'], ['74\\nSPECIFIC OBJECTIVE No 1 …', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], ['Database', '', '', '', None, '1 1.000', None, '1', None, '1', None, None, None, '1', None, '1', None, '1 0.100', None, '1', '1.000'], ['Meetings- Output', '', '', '', None, '10 0.200', None, '10 0.200', None, '10 0.200', None, None, None, '10 0.200', None, '10 0.200', None, '10 0.200', None, '50', '1.000'], ['Communication\\nactivities', '', '', '', '', '2', '0.040', '2', '0.040', '2', None, None, '0.040', '2', '0.040', '2', '0.040', '2', '0.040', '10', '0.040'], ['Subtotal for specific objective No 1', None, None, '', '', '', '', '', '', '', None, None, '', '', '', '', '', '', '', '', ''], ['SPECIFIC OBJECTIVE No 2 ...', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], ['- Output', '', '', '', '', '', '', '', '', '', None, None, '', '', '', '', '', '', '', '', ''], ['Subtotal for specific objective No 2', None, None, '', '', '', '', '', '', '', None, None, '', '', '', '', '', '', '', '', ''], ['TOTALS', None, None, '', '', '13', '0.240', '13', '0.240', '13', None, None, '0.240', '13', '0.240', '13', '0.240', '13', '0.100', '65', '2.200']]\n",
      "Table 10:\n",
      "[['', '76\\nOutside HEADING 7', '', '', '', '', '', '', '', '', ''], [None, 'of the multiannual', None, None, None, None, None, None, None, None, None], [None, 'financial framework', None, None, None, None, None, None, None, None, None], ['Human resources', None, None, '', '', '', '', '', '', '', ''], ['Other expenditure\\nof an administrative\\nnature', None, None, '', '0.240', '0.240', '0.240', '0.240', '0.240', '0.240', '1.20'], ['', 'Subtotal', '', '', '0.240', '0.240', '0.240', '0.240', '0.240', '0.240', '1.20'], [None, 'outside HEADING 7', None, None, None, None, None, None, None, None, None], [None, 'of the multiannual', None, None, None, None, None, None, None, None, None], [None, 'financial framework', None, None, None, None, None, None, None, None, None]]\n",
      "Table 11:\n",
      "[['.', None, '', 'Year\\n2023', 'Year\\n2024', 'Year\\n2025', '2026', '2027', 'After\\n202777', ''], ['\\uf09f Establishment plan posts (officials and temporary staff)', None, None, None, None, None, None, None, None, None], ['20 01 02 01 (Headquarters and Commission’s Representation\\nOffices)', None, None, '10', '10', '10', '10', '10', '10', ''], ['20 01 02 03 (Delegations)', None, None, '', '', '', '', '', '', ''], ['01 01 01 01 (Indirect research)', None, None, '', '', '', '', '', '', ''], ['01 01 01 11 (Direct research)', None, None, '', '', '', '', '', '', ''], ['Other budget lines (specify)', None, None, '', '', '', '', '', '', ''], ['78\\n\\uf09f External staff (in Full Time Equivalent unit: FTE)', None, None, None, None, None, None, None, None, None], ['20 02 01 (AC, END, INT from the ‘global envelope’)', None, None, '', '', '', '', '', '', ''], ['20 02 03 (AC, AL, END, INT and JPD in the delegations)', None, None, '', '', '', '', '', '', ''], ['79\\nXX 01 xx yy zz', '- at Headquarters', None, '', '', '', '', '', '', ''], [None, '- in Delegations', None, '', '', '', '', '', '', ''], ['01 01 01 02 (AC, END, INT - Indirect research)', None, None, '', '', '', '', '', '', ''], ['01 01 01 12 (AC, END, INT - Direct research)', None, None, '', '', '', '', '', '', ''], ['Other budget lines (specify)', None, None, '', '', '', '', '', '', ''], ['TOTAL', None, '', '10', '10', '10', '10', '10', '10', '']]\n",
      "Table 12:\n",
      "[['External staff', '']]\n",
      "Table 13:\n",
      "[['', 'Year\\n80\\nN', 'Year\\nN+1', 'Year\\nN+2', 'Year\\nN+3', 'Enter as many years as necessary\\nto show the duration of the\\nimpact (see point 1.6)', None, None, 'Total'], ['Specify the co-financing\\nbody', '', '', '', '', '', '', '', ''], ['TOTAL appropriations\\nco-financed', '', '', '', '', '', '', '', '']]\n",
      "Table 14:\n",
      "[['Budget revenue line:', 'Appropriation\\ns available for\\nthe current\\nfinancial year', '81\\nImpact of the proposal/initiative', None, None, None, None, None, None], [None, None, 'Year\\nN', 'Year\\nN+1', 'Year\\nN+2', 'Year\\nN+3', 'Enter as many years as necessary to show\\nthe duration of the impact (see point 1.6)', None, None], ['Article ………….', '', '', '', '', '', '', '', '']]\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_all_tables_from_pdf(file_path):\n",
    "    tables = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            table = page.extract_table()\n",
    "            if table:\n",
    "                # look if there's any empty cells\n",
    "                cleaned_table = [row for row in table if any(cell for cell in row)]\n",
    "                if cleaned_table:\n",
    "                    tables.append(cleaned_table)\n",
    "    return tables\n",
    "\n",
    "tables = extract_all_tables_from_pdf('infos/pdf2.pdf')\n",
    "\n",
    "#for each table\n",
    "#the only issue here is that sometimes it see texts as tables\n",
    "for i, table in enumerate(tables):\n",
    "    print(f\"Table {i+1}:\")\n",
    "    print(table)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T09:47:16.278831100Z",
     "start_time": "2024-05-17T09:46:34.574749600Z"
    }
   },
   "id": "ba659db60015c5f0"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "'EN   EN \\n \\n \\n EUROPEAN  \\nCOMMISSION   \\nBrussels, 21.4.2021  \\nCOM(2021) 206 final  \\n2021/0106 (COD)  \\n \\nProposal for a  \\nREGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL  \\nLAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE \\n(ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION \\nLEGISLATIVE ACTS  \\n{SEC(2021)  167 final}  - {SWD(2021)  84 final}  - {SWD(2021)  85 final}   EN 1  EN EXPLANATORY MEMORANDUM  \\n1. CONTEXT  OF THE  PROPOSAL  \\n1.1. Reasons for and objectives of the proposal  \\nThis explanatory memorandum accompanies the proposal for a Regulation laying down \\nharmonised rules on artificial intelligence (Artificial Intelligence Act). Artificial Intelligence \\n(AI) is a fast evolving family of technologies that can bring a wide array of economic and \\nsocietal benefits across the entire s pectrum of industries and social activities. By improving \\nprediction, optimising operations and resource allocation, and personalising service delivery, \\nthe use of artificial intelligence can support socially and environmentally beneficial outcomes \\nand pro vide key competitive advantages to companies and the European economy. Such \\naction is especially needed in high -impact sectors, including climate change, environment and \\nhealth, the public sector, finance, mobility, home affairs and agriculture. However, t he same \\nelements and techniques that power the socio -economic benefits of AI can also bring about \\nnew risks or negative consequences for individuals or the society. In light of the speed of \\ntechnological change and possible challenges, the EU is committed to strive for a balanced \\napproach.  It is in the Union interest to preserve the EU’s technological leadership and to \\nensure that Europeans can benefit from new technologies developed and functioning \\naccording to Union values, fundamental rights and principl es. \\nThis proposal delivers on the political commitment by President von der Leyen, who \\nannounced in her political guidelines for the 2019 -2024 Commission “A Union that strives for \\nmore”1, that the Commission would put forward legislation for a coordinated European \\napproach on the human and ethical implications of AI. Following on that announcement, on \\n19 February 2020 the Commission published the White Paper on AI - A European approach \\nto excellence and trust2. The White Paper sets out policy options on how  to achieve the twin \\nobjective of promoting the uptake of AI and of addressing the risks associated with certain \\nuses of such technology. This proposal aims to implement the second objective for the \\ndevelopment of an ecosystem of trust by proposing a legal  framework for trustworthy AI. The \\nproposal is based on EU values and fundamental rights and aims to give people and other \\nusers the confidence to embrace AI -based solutions, while encouraging businesses to develop \\nthem. AI should be a tool for people and be a force for good in society with the ultimate aim \\nof increasing human well -being. Rules for AI available in the Union market or otherwise \\naffecting people in the Union should therefore be human centric, so that people can trust that \\nthe technology is us ed in a way that is safe and compliant with the law, including the respect \\nof fundamental rights. Following the publication of the White Paper, the Commission \\nlaunched a broad stakeholder consultation,  which was met with a great interest by a large \\nnumber of stakeholders who were largely supportive of regulatory intervention to address the \\nchallenges and concerns raised by the increasing use of AI.  \\nThe proposal also responds to explicit requests from the European Parliament (EP) and the \\nEuropean  Council, w hich have repeatedly expressed calls for legislative action to ensure a \\nwell-functioning internal market for artificial intelligence systems (‘AI systems’) where both \\nbenefits and risks of AI are adequately addressed at Union level.  It supports the objecti ve of \\nthe Union being a global leader in the development of secure, trustworthy and ethical artificial \\n                                                 \\n1 https://ec.europa.eu/commission/sites/beta -political/files/political -guidelines -next-commission_en.pdf  \\n2 European Commission, White Paper on Artificial Intelligence - A European approach to excellence and \\ntrust, COM(2020) 65 final, 2020.  EN 2  EN intelligence as stated by the European Council3 and ensures the protection of ethical principles \\nas specifically requested by the European Parliament4.  \\nIn 2017, the European Council called for a ‘sense of urgency to address emerging trends’ \\nincluding ‘issues such as artificial intelligence …, while at the same time ensuring a high \\nlevel of data protection, digital rights and ethical standards’5. In its 20 19 Conclusions on the \\nCoordinated Plan on the development and use of artificial intelligence Made in Europe6, the \\nCouncil further highlighted the importance of ensuring that European citizens’ rights are fully \\nrespected and called for a review of the exist ing relevant legislation to make it fit for purpose \\nfor the new opportunities and challenges raised by AI. The European Council has also called \\nfor a clear determination of the AI applications that should be considered high -risk7.  \\nThe most recent Conclusi ons from 21 October 2020  further called for addressing the opacity, \\ncomplexity, bias, a certain degree of unpredictability and partially autonomous behaviour of \\ncertain AI systems, to ensure their compatibility with fundamental rights and to facilitate the  \\nenforcement of legal rules8. \\nThe European Parliament has also undertaken a considerable amount of work in the area of \\nAI. In October 2020, it adopted a number of resolutions related to AI, including on ethics9, \\nliability10 and copyright11. In 2021, those were followed by resolutions on AI in criminal \\nmatters12 and in education, culture and the audio -visual sector13. The EP Resolution on a \\nFramework of Ethical Aspects of Artificial Intelligence, Robotics and Related Technologies \\nspecifically recommends to the  Commission to propose legislative action to harness the \\nopportunities and benefits of AI, but also to ensure protection of ethical principles. The \\nresolution includes a text of the legislative proposal for a regulation on ethical principles for \\nthe develo pment, deployment and use of AI, robotics and related technologies. In accordance \\nwith the political commitment made by President von der Leyen in her Political Guidelines as \\nregards resolutions adopted by the European Parliament under Article 225 TFEU, th is \\n                                                 \\n3 European Council, Special meeting of the European Council (1 and 2 October 2020) – Conclusions , \\nEUCO 13/20, 2020, p. 6.  \\n4 European Parliament resolution of 20 Oct ober 2020 with recommendations to the Commission on a \\nframework of ethical aspects of artificial intelligence, robotics and related technologies, \\n2020/2012(INL).  \\n5 European Council, European Council meeting (19 October 2017) – Conclusion  EUCO 14/17, 2017, p. \\n8. \\n6 Council of the European Union, Artificial intelligence b) Conclusion s on the coordinated plan on \\nartificial intelligence -Adoption  6177/19, 2019.  \\n7 European Council, Special meeting of the European Council (1and 2 October 2020) – Conclusions  \\nEUCO 13/20, 2020.  \\n8 Council of the European Union, Presidency conclusions - The Charter of Fundamental Rights in the \\ncontext of Artificial Intelligence and Digital Chan ge, 11481/20, 2020.  \\n9 European Parliament resolution of 20 October 2020 on a framework of ethical aspects of artificial \\nintelligence, robotics and related technologies, 2020/2012(INL) . \\n10 European Parliament resolution of 20 October 2020 on a civil liability regime for artificial intelligence, \\n2020/2014(INL).  \\n11 European Parliament resolution of 20 October 2020 on intellectual property rights for the development \\nof artificial intelligence technologies, 2020/2015(INI).  \\n12 European  Parliament Draft Report, Artificial intelligence in criminal law and its use by the police and \\njudicial authorities in criminal matters,  2020/2016(INI) .  \\n13 European Parliament Draft Report, Artificial intelligence in education, culture and the audiovisual \\nsector, 2020/2017(INI) . In that regard, the Commission has adopted the Digital Education Action Plan \\n2021 -2027: Resetting education and training for the digital age, which foresees the development of \\nethical guidelines in AI and Data usage in education – Commission Communicatio n COM(2020) 624 \\nfinal.  EN 3  EN proposal takes into account the aforementioned resolution of the European Parliament in full \\nrespect of proportionality, subsidiarity and better law making principles.  \\nAgainst this political context, the Commission puts forward the proposed regulatory \\nframework on Artificial Intelligence with the following specific objectives : \\n\\uf0b7 ensure that AI systems placed on the Union market and used are safe and respect \\nexisting law on fundamental rights and Union values;  \\n\\uf0b7 ensure legal certainty to facilitate investmen t and innovation in AI;  \\n\\uf0b7 enhance governance and effective enforcement of existing law on fundamental \\nrights and safety requirements applicable to AI systems;  \\n\\uf0b7 facilitate the development of a single market for lawful, safe and trustworthy AI \\napplications and prevent market fragmentation.  \\nTo achieve those objectives, this proposal presents a balanced and proportionate horizontal \\nregulatory approach to AI that is limited to the minimum necessary requirements to address \\nthe risks and problems linked to AI, withou t unduly constraining or hindering technological \\ndevelopment or otherwise disproportionately increasing the cost of placing AI solutions on \\nthe market.  The proposal sets a robust and flexible legal framework. On the one hand, it is \\ncomprehensive and future -proof in its fundamental regulatory choices, including the \\nprinciple -based requirements that AI systems should comply with. On the other hand, it puts \\nin place a proportionate regulatory system centred on a well -defined risk -based regulatory \\napproach that  does not create unnecessary restrictions to trade, whereby legal intervention is \\ntailored to those concrete situations where there is a justified cause for concern or where such \\nconcern can reasonably be anticipated in the near future. At the same time, t he legal \\nframework includes flexible mechanisms that enable it to be dynamically adapted as the \\ntechnology evolves and new concerning situations emerge.  \\nThe proposal sets harmonised rules for the development, placement on the market and use of \\nAI systems i n the Union following a proportionate risk -based approach. It proposes a single \\nfuture -proof definition of AI. Certain particularly harmful AI practices are prohibited as \\ncontravening Union values, while specific restrictions and safeguards are proposed in  relation \\nto certain uses of remote biometric identification systems for the purpose of law enforcement. \\nThe proposal lays down a solid risk methodology to define “high -risk” AI systems that pose \\nsignificant risks to the health and safety or fundamental ri ghts of persons. Those AI systems \\nwill have to comply with a set of horizontal mandatory requirements for trustworthy AI and \\nfollow conformity assessment procedures before those systems can be placed on the Union \\nmarket. Predictable, proportionate and clea r obligations are also placed on providers and users \\nof those systems to ensure safety and respect of existing legislation protecting fundamental \\nrights throughout the whole AI systems’ lifecycle. For some specific AI systems, only \\nminimum transparency obl igations are proposed, in particular when chatbots or ‘deep fakes’ \\nare used.  \\nThe proposed rules will be enforced through a governance system at Member States level, \\nbuilding on already existing structures, and a cooperation mechanism at Union level with the \\nestablishment of a European Artificial Intelligence Board . Additional measu res are also \\nproposed to support innovation, in particular through AI regulatory sandboxes and other \\nmeasures to reduce the regulatory burden and to support Small and Medium -Sized Enterprises \\n(‘SMEs’) and start -ups. EN 4  EN 1.2. Consistency with existing policy pr ovisions in the policy area  \\nThe horizontal nature of the proposal requires full consistency with existing Union legislation \\napplicable to sectors where high -risk AI systems are already used or likely to be used in the \\nnear future.  \\nConsistency is also ensu red with the EU Charter of Fundamental Rights and the existing \\nsecondary Union legislation on data protection, consumer protection, non -discrimination and \\ngender equality. The proposal is without prejudice and complements the General Data \\nProtection Regula tion (Regulation (EU) 2016/679) and the Law Enforcement Directive \\n(Directive (EU) 2016/680) with a set of harmonised rules applicable to the design, \\ndevelopment and use of certain high -risk AI systems and restrictions on certain uses of remote \\nbiometric id entification systems. Furthermore, the proposal complements existing Union law \\non non -discrimination with specific requirements that aim to minimise the risk of algorithmic \\ndiscrimination, in particular in relation to the design and the quality of data set s used for the \\ndevelopment of AI systems complemented with obligations for testing, risk management, \\ndocumentation and human oversight throughout the AI systems’ lifecycle. The proposal is \\nwithout prejudice to the application of Union competition law.  \\nAs regards high-risk AI systems which are safety components of products, this proposal will \\nbe integrated into the existing sectoral safety legislation to ensure consistency, avoid \\nduplications and minimise additional burdens. In particular, as regards  high-risk AI systems \\nrelated to products covered by the New Legislative Framework (NLF) legislation (e.g. \\nmachinery, medical devices, toys), the requirements for AI systems set out in this proposal \\nwill be checked as part of the existing conformity assessment pro cedures under the relevant \\nNLF legislation. With regard to the interplay of requirements, while the safety risks specific \\nto AI systems are meant to be covered by the requirements of this proposal, NLF legislation \\naims at ensuring the overall safety of the  final product and therefore may contain specific \\nrequirements regarding the safe integration of an AI system into the final product. The \\nproposal for a Machinery Regulation, which is adopted on the same day as this proposal fully \\nreflects this approach. As regards high-risk AI systems related to products  covered by relevant \\nOld Approach legislation (e.g. aviation, cars), this proposal would not directly apply. \\nHowever, the ex -ante essential requirements for high -risk AI systems set out in this proposal \\nwill have to be taken into account when adopting relevant implementing or delegated \\nlegislation under those acts.  \\nAs regards AI systems provided or used by regulated credit institutions , the authorities \\nresponsible for the supervision of the Union’s financial  services legislation should be \\ndesignated as competent authorities for supervising the requirements in this proposal to ensure \\na coherent enforcement of the obligations under this proposal and the Union’s financial \\nservices legislation where AI systems ar e to some extent implicitly regulated in relation to the \\ninternal governance system of credit institutions . To further enhance consistency, the \\nconformity assessment procedure and some of the providers’ procedural obligations under this \\nproposal are integr ated into the procedures under Directive 2013/36/EU on access to the \\nactivity of credit institutions and the prudential supervision14.  \\n                                                 \\n14 Directive 2013/36/EU of the European Parliament and of the Council of 26 June 2013 on access to the \\nactivity of credit institutions and the prudential supervision of credit institutions and investment firms, \\namending Directive 2002 /87/EC and repealing Directives 2006/48/EC and 2006/49/EC Text with EEA \\nrelevance, OJ L 176, 27.6.2013, p. 338 –436. EN 5  EN This proposal is also consistent with the applicable Union legislation on services, including on \\nintermediary services re gulated by the e -Commerce Directive 2000/31/EC15 and the \\nCommission’s recent proposal for the Digital Services Act (DSA)16. \\nIn relation to AI systems that are components of large -scale IT systems in the Area of \\nFreedom, Security and Justice managed by the European Union Agency for the Operational \\nManagement of Large -Scale IT Systems (eu -LISA), the proposal will not apply to tho se AI \\nsystems that have been placed on the market or put into service before one year has elapsed \\nfrom the date of application of this Regulation,  unless the replacement or amendment of those \\nlegal acts leads to a significant change in the design or intend ed purpose of the AI system or \\nAI systems concerned . \\n1.3. Consistency with other Union policies  \\nThe proposal is part of a wider comprehensive package of measures that address problems \\nposed by the development and use of AI, as examined in the White Paper o n AI. Consistency \\nand complementarity is therefore ensured with other ongoing or planned initiatives of the \\nCommission that also aim to address those problems, including the revision of sectoral \\nproduct legislation (e.g. the Machinery Directive, the Genera l Product Safety Directive) and \\ninitiatives that address liability issues related to new technologies, including AI systems. \\nThose initiatives will build on and complement this proposal in order to bring legal clarity and \\nfoster the development of an ecosy stem of trust in AI in Europe.  \\nThe proposal is also coherent with the Commission’s overall digital strategy in its \\ncontribution to promoting technology that works for people, one of the three main pillars of \\nthe policy orientation and objectives announced  in the Communication ‘Shaping Europe\\'s \\ndigital future’17. It lays down a coherent, effective and proportionate framework to ensure AI \\nis developed in ways that respect people’s rights and earn their trust, making Europe fit for the \\ndigital age and turning the next ten years into the Digital Decade18. \\nFurthermore, the promotion of AI -driven innovation is closely linked to the Data \\nGovernance Act19, the Open Data Directive20 and other initiatives under the EU strategy \\nfor data21, which will establish trusted mech anisms and services for the re -use, sharing and \\npooling of data that are essential for the development of data -driven AI models of high \\nquality.  \\nThe proposal also strengthens significantly the Union’s role to help shape global norms and \\nstandards and promo te trustworthy AI that is consistent with Union values and interests. It \\nprovides the Union with a powerful basis to engage further with its external partners, \\nincluding third countries, and at international fora on issues relating to AI.  \\n                                                 \\n15 Directive 2000/31/EC of the European Parliament and of the Council of 8 June 2000 on certain legal \\naspects of information society services , in particular electronic commerce, in the Internal Market \\n(\\'Directive on electronic commerce\\'), OJ L 178, 17.7.2000, p. 1 –16. \\n16 See Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL \\non a Single Market For Digital Services (Digital S ervices Act) and amending Directive 2000/31/EC \\nCOM/2020/825 final.  \\n17 Communication from the Commission, Shaping Europe\\'s Digital Future, COM/2020/67 final.  \\n18 2030 Digital Compass: the European way for the Digital Decade . \\n19 Proposal for a Regulation on European data governance (Data Governance Act) COM/2020/767 . \\n20 Directive (EU) 2019/1024 of the European Parliament and of the Council of 20 June 2019 on open data \\nand the re -use of public sector information, PE/28/2019/REV/1, OJ L 172, 26.6.2019, p. 56 –83. \\n21 Commission Communication, A  European strategy for data COM/2020/66 final.  \\n EN 6  EN 2. LEGAL  BASIS,  SUBSIDIARITY  AND  PROPORTIONALITY  \\n2.1. Legal  basis  \\nThe legal basis for the proposal is in the first place Article 114 of the Treaty on the \\nFunctioning of the European Union (TFEU), which provides for the adoption of measures to \\nensure the establishment and f unctioning of the internal market.  \\nThis proposal constitutes a core part of the EU digital single market strategy. The primary \\nobjective of this proposal is to ensure the proper functioning of the internal market by setting \\nharmonised rules in particular on the development, placing on the Union market and the use \\nof products and services making use of AI technologies or provided as stand -alone AI \\nsystems. Some Member States are already considering national rules to ensure that AI is safe \\nand is developed a nd used in compliance with fundamental rights obligations. This will likely \\nlead to two main problems: i) a fragmentation of the internal market on essential elements \\nregarding in particular the requirements for the AI products and services , their marketin g, \\ntheir use, the liability  and the supervision  by public authorities , and ii) the substantial \\ndiminishment of legal certainty for both providers and users of AI systems on how existing \\nand new rules will apply to those systems in the Union. Given the wide  circulation of products \\nand services across borders, these two problems can be best solved through EU harmonizing \\nlegislation.  \\nIndeed, the proposal defines common mandatory requirements applicable to the design and \\ndevelopment of certain AI systems befor e they are placed on the market that will be further \\noperationalised through harmonised technical standards. The proposal also addresses the \\nsituation after AI systems have been placed on the market by harmonising the way in which \\nex-post controls are cond ucted.  \\nIn addition, considering that this proposal contains certain specific rules  on the protection of \\nindividuals with regard to the processing of personal data,  notably restrictions of the use of AI \\nsystems for ‘real -time’ remote biometric identification in publicly accessible spaces for the \\npurpose of law enforcement, it is appropriate to base this regulation, in as far as those specific \\nrules are concerned, on Article 16 of the TFEU.  \\n2.2. Subsidiarity  (for non-exclusive  competence)   \\nThe nature of AI, which often relies on large and varied datasets and which may be embedded \\nin any product or service circulating freely within the internal market, entails that the \\nobjectives of this proposal cannot be effectively achieved by Member States alone. \\nFurthermore, a n emerging patchwork of potentially divergent national rules will hamper the \\nseamless circulation of products and services related to  AI systems across the EU and will be \\nineffective in ensuring the safety and protection of fundamental rights and Union values \\nacross the different Member States. National approaches in addressing the problems will only \\ncreate additional legal uncertainty  and barriers, and will slo w market uptake of AI.   \\nThe objectives of this proposal can be better achieved at Union level to avoid a further \\nfragmentation of the Single Market into potentially contradictory national frameworks \\npreventing the free circulation of goods and services emb edding AI. A solid European \\nregulatory framework for trustworthy AI will also ensure a level playing field and protect all \\npeople, while strengthening Europe’s competitiveness and industrial basis in AI. Only \\ncommon action at Union level can also protect t he Union’s digital sovereignty and leverage \\nits tools and regulatory powers to shape global rules and standards.   EN 7  EN 2.3. Proportionality  \\nThe proposal builds on existing legal frameworks and is proportionate and necessary to \\nachieve its objectives, since it f ollows a risk -based approach and imposes regulatory burdens \\nonly when an AI system is likely to pose high risks to fundamental rights and safety. For \\nother, non -high-risk AI systems, only very limited transparency obligations are imposed, for \\nexample in te rms of the provision of information to flag the use of an AI system when \\ninteracting with humans. For high -risk AI systems, the requirements of high quality data, \\ndocumentation and traceability, transparency, human oversight, accuracy and robustness, are \\nstrictly necessary to mitigate the risks to fundamental rights and safety posed by AI and that \\nare not covered by other existing legal frameworks. Harmonised standards and supporting \\nguidance and compliance tools will assist providers and users in complying  with the \\nrequirements laid down by the proposal  and minimise their costs. The costs incurred by \\noperators are proportionate to the objectives achieved and the economic and reputational \\nbenefits that operators can expect from this proposal.  \\n2.4. Choice  of the instrument  \\nThe choice of a regulation as a legal instrument is justified by the need for a uniform \\napplication of the new rules, such as definition of AI, the prohibition of certain harmful AI -\\nenabled practices and the classification of certain AI syst ems. The direct applicability of a \\nRegulation, in accordance with Article 288 TFEU, will reduce legal fragmentation and \\nfacilitate the development of a single market for lawful, safe and trustworthy AI systems. It \\nwill do so, in particular, by introducing a harmonised set of core requirements with regard to \\nAI systems classified as high -risk and obligations for providers and users of those systems, \\nimproving the protection of fundamental rights and providing legal certainty for operators and \\nconsumers alike . \\nAt the same time, the provisions of the regulation are not overly prescriptive and leave room \\nfor different levels of Member State action for elements that do not undermine the objectives \\nof the initiative, in particular the internal organisation of the market surveillance system and \\nthe uptake of measures to foster innovation.  \\n3. RESULTS  OF EX-POST  EVALUATIONS,  STAKEHOLDER  \\nCONSULTATIONS  AND  IMPACT  ASSESSMENTS  \\n3.1. Stakeholder  consultation  \\nThis proposal is the result of extensive consultation with all major stakeholders, in which the \\ngeneral principles and minimum standards for consultation of interested parties by the \\nCommission were applied.  \\nAn online public consultation  was launched on 1 9 February 2020 along with the publication \\nof the White Paper on Artificial Intelligence and ran until 14 June 2020. The objective of that \\nconsultation was to collect views and opinions on the White Paper. It targeted all interested \\nstakeholders from the p ublic and private sectors, including governments, local authorities, \\ncommercial and non -commercial organisations, social partners, experts, academics and \\ncitizens. After analysing all the responses received, the Commission published a summary \\noutcome and t he individual responses on its website22. \\nIn total, 1215 contributions were received, of which 352 were from companies or business \\norganisations/associations, 406 from individuals (92%individuals from EU ), 152 on behalf of \\n                                                 \\n22 See all consultation results here.  EN 8  EN academic/research institutions, a nd 73 from public authorities. Civil society’s voices were \\nrepresented by 160 respondents (among which 9 consumers’ organisations, 129 non -\\ngovernmental organisations and 22 trade unions), 72 respondents contributed as ‘others’. Of \\nthe 352 business and indu stry representatives, 222 were companies and business \\nrepresentatives, 41.5% of which were micro, small and medium -sized enterprises. The rest \\nwere business associations. Overall, 84% of business and industry replies came from the EU -\\n27. Depending on the q uestion, between 81 and 598 of the respondents used the free text \\noption to insert comments. Over 450 position papers were submitted through the EU Survey \\nwebsite, either in addition to questionnaire answers (over 400) or as stand -alone contributions \\n(over  50). \\nOverall, there is a general agreement amongst stakeholders on a need for action. A large \\nmajority of stakeholders agree that legislative gaps exist or that new legislation is needed. \\nHowever, several stakeholders warn the Commission to avoid duplicat ion, conflicting \\nobligations and overregulation. There were many comments underlining the importance of a \\ntechnology neutral and proportionate regulatory framework.  \\nStakeholders mostly requested a narrow, clear and precise definition for AI. Stakeholders a lso \\nhighlighted that besides the clarification of the term of AI, it is important to define ‘risk’, \\n‘high -risk’, ‘low -risk’, ‘remote biometric identification’ and ‘harm’.  \\nMost of the respondents are explicitly in favour of the risk -based approach. Using a risk-based \\nframework was considered a better option than blanket regulation of all AI systems. The types \\nof risks and threats should be based on a sector -by-sector and case -by-case approach. Risks \\nalso should be calculated taking into account the impact on  rights and safety.  \\nRegulatory sandboxes could be very useful for the promotion of AI  and are welcomed by \\ncertain stakeholders, especially the Business Associations.  \\nAmong those who formulated their opinion on the enforcement models, more than 50%, \\nespeci ally from the business associations, were in favour of a combination of an ex -ante risk \\nself-assessment and an ex -post enforcement for high -risk AI systems.  \\n3.2. Collection  and use of expertise  \\nThe proposal builds on two years of analysis and close involve ment of stakeholders, including \\nacademics, businesses, social partners, non -governmental organisations, Member States and \\ncitizens. The preparatory work started in 2018 with the setting up of a High -Level Expert \\nGroup on AI (HLEG) which had an inclusive an d broad composition of 52 well -known \\nexperts tasked to advise the Commission on the implementation of the Commission’s Strategy \\non Artificial Intelligence. In April 2019, the Commission supported23 the key requirements set \\nout in the HLEG ethics guidelines for Trustworthy AI24, which had been revised to take into \\naccount more than 500 submissions from stakeholders. The key requirements reflect a \\nwidespread and common approach, as evidenced by a plethora of ethical codes and principles \\ndeveloped by many privat e and public organisations in Europe and beyond, that AI \\ndevelopment and use should be guided by certain essential value -oriented principles. The \\nAssessment List for Trustworthy Artificial Intelligence (ALTAI)25 made those requirements \\noperational in a pilo ting process with over 350 organisations.  \\n                                                 \\n23 European Commission, Building Trust in Human -Centric Artificial Intelligence , COM(2019) 168.  \\n24 HLEG, Ethics Guidelines for Trustworthy AI , 2019.  \\n25 HLEG, Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self -assessment , 2020.  EN 9  EN In addition, the AI Alliance26 was formed as a platform for approximately 4000 stakeholders \\nto debate the technological and societal implications of AI, culminating in a yearly AI \\nAssembly.  \\nThe White Paper  on AI f urther developed this inclusive approach, inciting comments from \\nmore than 1250 stakeholders, including over 450 additional position papers. As a result, the \\nCommission published an Inception Impact Assessment, which in turn attracted more than \\n130 comment s27. Additional stakeholder workshops and events were also organised the \\nresults of which support the analysis in the impact assessment and the policy choices made in \\nthis proposal28. An external study  was also procured to feed into the impact assessment.  \\n3.3. Impact  assessment  \\nIn line with its “Better Regulation” policy, the Commission conducted an impact assessment \\nfor this proposal examined by the Commission\\'s Regulatory Scrutiny Board. A meeting with \\nthe Regulatory Scrutiny Board was held on 16 December 2020, which was follow ed by a \\nnegative opinion. After substantial revision of the impact assessment to address the comments \\nand a resubmission of the impact assessment, the Regulatory Scrutiny Board issued a positive \\nopinion on 21 March 2021. The opinions of the Regulatory Scru tiny Board, the \\nrecommendations and an explanation of how they have been taken into account are presented \\nin Annex 1 of the impact assessment.  \\nThe Commission examined different policy options to achieve the general objective of the \\nproposal, which is to ensure the proper functioning of the single market  by creating the \\nconditions for the development and use of trustworthy AI in the Union.  \\nFour policy options of different degrees of regulatory intervention were assessed:  \\n\\uf0b7 Option 1 : EU legislative instrument setting up a voluntary labelling scheme;  \\n\\uf0b7 Option 2 : a sectoral, “ad -hoc” approach;  \\n\\uf0b7 Option 3 : Horizontal EU legislative instrument following a proportionate risk -\\nbased approach;  \\n\\uf0b7 Option 3+ : Horizontal EU legislative instrument following a proportionate risk -\\nbased approach + codes of conduct for non -high-risk AI systems;  \\n\\uf0b7 Option 4 : Horizontal EU legislative instrument establishing mandatory \\nrequirements for all AI systems, irrespective of the risk they pose.  \\nAccording to the Commission\\'s established methodology, each policy option was evaluated \\nagainst economic and societal impacts, with a particular focus on impacts on fundamental \\nrights. The preferred option is option 3+, a regulatory framework for high -risk AI systems \\nonly, with the possibility for all providers of non -high-risk AI systems to follow a code of \\nconduct. The requirements will concern data, documentation and traceability, provision of \\ninformation and transparency, human oversight and robustness and accuracy and would be \\nmandatory for high -risk AI systems. Companies that introduced codes of conduct for other AI \\nsystems would do so voluntarily.  \\n                                                 \\n26 The AI Alliance is  a multi -stakeholder forum launched in June 2018, AI Alliance \\nhttps://ec.europ a.eu/digital -single -market/en/european -ai-alliance  \\n27 European Commission, Inception Impact Assessment For  a Proposal for a legal act of the European \\nParliament and the Council laying down requirements for Artificial Intelligence .  \\n28 For details of all the consultations that have been carried out see Annex 2 of the impact assessment.  EN 10  EN The preferred option was considered suitable to address in the most effective way the \\nobjectives of this proposal. By requiring a restricted yet effective set of actions from AI \\ndevelopers and users, the preferred option limits the risks of violation of fundamental rights \\nand safety of people and foster effective supervision and enforcement, by targeting the \\nrequirements only to systems where there is a high risk that such violations could occur. As a \\nresult, that option keeps compliance costs to a minimum, thus avoiding an unnecessary \\nslowing of uptake due to higher prices and compliance costs. In order to ad dress possible \\ndisadvantages for SMEs, this option includes several provisions to support their compliance \\nand reduce their costs, including creation of regulatory sandboxes and obligation to consider \\nSMEs interests when setting fees related to conformity assessment.  \\nThe preferred option will increase people’s trust in AI, companies will gain in legal certainty, \\nand Member States will see no reason to take unilateral action that could fragment the single \\nmarket. As a result of higher demand due to higher t rust, more available offers due to legal \\ncertainty, and the absence of obstacles to cross -border movement of AI systems, the single \\nmarket for AI will likely flourish. The European Union will continue to develop a fast -\\ngrowing AI ecosystem of innovative se rvices and products embedding AI technology or \\nstand -alone AI systems, resulting in increased digital autonomy.  \\nBusinesses or public authorities that develop or use AI applications that constitute a high risk \\nfor the safety or fundamental rights of citizen s would have to comply with specific \\nrequirements and obligations. Compliance with these requirements would  imply costs \\namounting to approximately EUR € 6000 to EUR € 7000 for the supply of an average high -\\nrisk AI system of around EUR € 170000 by 2025. For  AI users, there would also be the \\nannual cost for the time spent on ensuring human oversight where this is appropriate, \\ndepending on the use case. Those have been estimated at approximately EUR € 5000 to EUR \\n€ 8000 per year. Verification costs could amoun t to another EUR € 3000 to EUR € 7500 for \\nsuppliers of high -risk AI. Businesses or public authorities that develop or use any AI \\napplications not classified as high risk would only have minimal obligations of information. \\nHowever, they could choose to join  others and together adopt a code of conduct to follow \\nsuitable requirements,  and to ensure that their AI systems are trustworthy. In such a case, \\ncosts would be at most as high as for high -risk AI systems, but most probably lower.  \\nThe impacts of the poli cy options on different categories of stakeholders (economic operators/ \\nbusiness; conformity assessment bodies, standardisation bodies and other public bodies; \\nindividuals/citizens; researchers) are explained in detail in Annex 3 of the Impact assessment \\nsupporting this proposal.  \\n3.4. Regulatory  fitness  and simplification  \\nThis proposal lays down obligation that will apply to providers and users of high -risk AI \\nsystems. For providers who develop and place such systems on the Union market, it will \\ncreate lega l certainty and ensure that no obstacle to the cross -border provision of AI -related \\nservices and products emerge. For companies using AI, it will promote trust among their \\ncustomers. For national public administrations, it will promote public trust in the use of AI \\nand strengthen enforcement mechanisms (by introducing a European coordination \\nmechanism, providing for appropriate capacities, and facilitating audits of the AI systems \\nwith new requirements for documentation, traceability and transparency). More over, the \\nframework will envisage specific measures supporting innovation, including regulatory \\nsandboxes and specific measures supporting small -scale users and providers of high -risk AI \\nsystems to comply with the new rules . \\nThe proposal also specifically aims at strengthening Europe’s competitiveness and industrial \\nbasis in AI. Full consistency is ensured with existing sectoral Union legislation applicable to EN 11  EN AI systems (e.g. on products and services) that will bring further clarity and simplify the \\nenforc ement of the new rules.  \\n3.5. Fundamental  rights  \\nThe use of AI with its specific characteristics (e.g. opacity, complexity, dependency on data, \\nautonomous behaviour) can adversely affect a number of fundamental rights enshrined in the \\nEU Charter of Fundamen tal Rights (‘the Charter’) . This proposal seeks to ensure a high level \\nof protection for those fundamental rights and aims to address various sources of risks \\nthrough a clearly defined risk -based approach. With a set of requirements for trustworthy AI \\nand proportionate obligations on all value chain participants, the proposal will enhance and \\npromote the protection of the rights  protected by the Charter: the right  to human dignity \\n(Article 1), respect for private life and protection of personal data (Articl es 7 and 8), non -\\ndiscrimination (Article 21) and equality between women and men (Article 23). It aims to \\nprevent a chilling effect on the rights to freedom of expression (Article 11) and freedom of \\nassembly (Article 12), to ensure protection of the right t o an effective remedy and to a fair \\ntrial, the rights of defence and the presumption of innocence (Articles 47 and 48), as well as \\nthe general principle of good administration. Furthermore, as applicable in certain domains, \\nthe proposal will positively aff ect the rights of a number of special groups, such as the \\nworkers’ rights to fair and just working conditions (Article 31), a high level of consumer \\nprotection (Article 28), the rights of the child (Article 24) and the integration of persons with \\ndisabilit ies (Article 26). The right to a high level of environmental protection and the \\nimprovement of the quality of the environment (Article 37) is also relevant, including in \\nrelation to the health and safety of people. The obligations for ex ante testing, risk  \\nmanagement and human oversight will also facilitate the respect of other fundamental rights \\nby minimising the risk of erroneous or biased AI -assisted decisions in critical areas such as \\neducation and training, employment, important services, law enforceme nt and the  judiciary. In \\ncase infringements of fundamental rights still happen, effective redress for affected persons \\nwill be made possible by ensuring transparency and traceability of the AI systems coupled \\nwith strong ex post controls.  \\nThis proposal imp oses some restrictions on the freedom to conduct business (Article 16) and \\nthe freedom of art and science (Article 13) to ensure compliance with overriding reasons of \\npublic interest such as health, safety, consumer protection and the protection of other \\nfundamental rights  (‘responsible innovation’) when high -risk AI technology is developed and \\nused. Those restrictions are proportionate and limited to the minimum necessary to prevent \\nand mitigate serious safety risks and likely infringements of fundamental rights.  \\nThe increased transparency obligations will also not disproportionately affect the right to \\nprotection of intellectual property (Article 17(2)), since they will be limited only to the \\nminimum necessary information for individuals to exercise their right to an effective remedy \\nand to the necessary transparency towards supervision and enforcement authorities, in line \\nwith their mandates. Any disclosure of information will be carried out in compliance with \\nrelevant legislation in the field, including Directive 2016/943 on the protection of undisclosed \\nknow -how and business information (trade secrets) against their unlawful acquisition, use and \\ndisclosure . When public authorities and notified bodies need to be given access to confidential \\ninformation or source code to examine compliance with substantial obligations, they are \\nplaced under binding confidentiality obligations.  \\n4. BUDGETARY  IMPLICATIONS  \\nMember States will have to designate supervisory authorities in charge of implementing the \\nlegislative requ irements. Their supervisory function could build on existing arrangements, for EN 12  EN example regarding conformity assessment bodies or market surveillance, but would require \\nsufficient technological expertise and human and financial resources. Depending on the p re-\\nexisting structure in each Member State, this could amount to 1 to 25 Full Time Equivalents \\nper Member State.  \\nA detailed overview of the costs involved is provided in the ‘financial statement’ linked to \\nthis proposal.  \\n5. OTHER  ELEMENTS  \\n5.1. Implementati on plans  and monitoring,  evaluation  and reporting  arrangements  \\nProviding for a robust monitoring and evaluation mechanism is crucial to ensure that the \\nproposal will be effective in achieving its specific objectives. The Commission will be in \\ncharge of mon itoring the effects of the proposal. It will establish a system for registering \\nstand -alone high -risk AI applications in a public EU -wide database. This registration will also \\nenable competent authorities, users and other interested people to verify if the  high-risk AI \\nsystem complies with the requirements  laid down in the proposal  and to exercise enhanced \\noversight over those AI systems posing high risks to fundamental rights. To feed this \\ndatabase, AI providers will be obliged to provide meaningful inform ation about their systems \\nand the conformity assessment carried out on those systems .  \\nMoreover, AI providers will be obliged to inform national competent authorities about serious \\nincidents or malfunctioning that constitute a breach of fundamental rights obligations as soon \\nas they become aware of them, as well as any recalls or withdrawals of AI systems from the \\nmarket. National competent authorities will then investigate the incidents/or malfunctioning, \\ncollect all the necessary information and regularly  transmit it to the Commission with \\nadequate metadata. The Commission will complement this information on the incidents by a \\ncomprehensive analysis of the overall market for AI.  \\nThe Commission will publish a report evaluating and reviewing the proposed AI  framework \\nfive years following the date on which it becomes applicable.  \\n5.2. Detailed  explanation  of the specific  provisions  of the proposal  \\n5.2.1.  SCOPE  AND  DEFINITIONS  (TITLE  I) \\nTitle I defines the subject matter of the regulation and the scope of application of the new \\nrules that cover the placing on the market, putting into service and use of AI systems . It also \\nsets out the definitions used throughout the instrument. The definition of AI system in the \\nlegal framework aims to be as technology neutral and future proof as possible, taking into \\naccount the fast technological and market developments related to AI. In order to provide the \\nneeded legal certainty, Title I is complemented by Ann ex I, which contains a detailed list of \\napproaches and techniques for the development of AI to be adapted by the Commission in line \\nwith new technological developments. Key participants across the AI value chain are also \\nclearly defined such as providers a nd users of AI systems that cover both public and private \\noperators to ensure a level playing field.  \\n5.2.2.  PROHIBITED ARTIFICIAL INTELLIGENCE PRACTICES (TITLE II)  \\nTitle II  establishes a list of prohibited AI. The regulation follows a risk -based approach,  \\ndifferentiating between uses of AI that create (i) an unacceptable risk, (ii) a high risk, and (iii) \\nlow or minimal risk. The list of prohibited practices in Title II comprises all those AI systems \\nwhose use is considered unacceptable as contravening Unio n values, for instance by violating \\nfundamental rights. The prohibitions covers practices that have a significant potential to \\nmanipulate persons  through subliminal techniques beyond their consciousness or exploit EN 13  EN vulnerabilities of specific vulnerable gro ups such as children or persons with disabilities in \\norder to materially distort their behaviour in a manner that is likely to cause them or another \\nperson psychological or physical harm. Other manipulative or exploitative practices affecting \\nadults that m ight be facilitated by AI systems could be covered by the existing data \\nprotection, consumer protection and digital service legislation that guarantee that natural \\npersons are properly informed and have free choice not to be subject to profiling or other \\npractices that might affect their behaviour. The proposal also prohibits AI -based social \\nscoring for general purposes done by public authorities. Finally, the use of ‘real time’ remote \\nbiometric identification systems in publicly accessible spaces for the purpose of law \\nenforcement is also prohibited unless certain limited exceptions apply.  \\n5.2.3.  HIGH -RISK AI SYSTEMS (TITLE III)  \\nTitle III contains specific rules for AI systems that create a high risk to the health and safety \\nor fundamental rights of natural persons. In line with a risk -based approach, those high -risk \\nAI systems are permitted on the European market subject to compliance with certain \\nmandatory requirements and an ex -ante conformity assessment. The classification of an AI \\nsystem as hi gh-risk is based on the intended purpose of the AI system, in line with existing \\nproduct safety legislation. Therefore, the classification as high -risk does not only depend on \\nthe function performed by the AI system, but also on the specific purpose and mo dalities for \\nwhich that system is used.  \\nChapter 1 of Title III sets the classification rules and identifies two main categories of high -\\nrisk AI systems:  \\n\\uf0b7 AI systems intended to be used as safety component of products that are subject to \\nthird party ex -ante conformity assessment;  \\n\\uf0b7 other stand -alone AI systems with mainly fundamental rights implications that are \\nexplicitly listed in Annex III.  \\nThis list of high -risk AI systems in Annex III contains a limited number of AI systems whose \\nrisks have already materi alised or are likely to materialise in the near future. To ensure that \\nthe regulation can be adjusted to emerging uses and applications of AI, the Commission may \\nexpand the list of high -risk AI systems used  within certain pre -defined areas, by applying a \\nset of criteria and risk assessment methodology.  \\nChapter 2 sets out the legal requirements for high -risk AI systems in relation to data and data \\ngovernance, documentation and recording keeping, transparency and provision of information \\nto users, human over sight, robustness, accuracy and security. The proposed minimum \\nrequirements are already state -of-the-art for many diligent operators and the result of two \\nyears of preparatory work, derived from the Ethics Guidelines of the HLEG29, piloted by \\nmore than 350 organisations30. They are also largely consistent with other international \\nrecommendations and principles, which ensures that the proposed AI framework is \\ncompatible with those adopted by the EU’s international trade partners . The precise technical \\nsolution s to achieve compliance with those requirements may be provided by standards or by \\nother technical specifications or otherwise be developed in accordance with general \\nengineering or scientific knowledge at the discretion of the provider of the AI system. T his \\nflexibility is particularly important, because it allows providers of AI systems to choose the \\n                                                 \\n29 High -Level Expert Group on Artificial Intelligence, Ethics Guidelines for Trustworthy AI , 2019.  \\n30 They were also endorsed by the Commission in its 2019 Communication on human -centric approach to \\nAI. EN 14  EN way to meet their requirements, taking into account the state -of-the-art and technological and \\nscientific progress in this field.  \\nChapter 3 places a clear se t of horizontal obligations on providers of high -risk AI systems. \\nProportionate obligations are also placed on users and other participants across the AI value \\nchain (e.g., importers, distributors, authorized representatives).  \\nChapter 4 sets the framework for notified bodies to be involved as independent third parties in \\nconformity assessment procedures, while Chapter 5 explains in detail the conformity \\nassessment procedures to be followed for each type of high -risk AI system.  The conformity \\nassessment app roach aims to minimise the burden for economic operators as well as for \\nnotified bodies, whose capacity needs to be progressively ramped up over time. AI systems \\nintended to be used as safety components of products that are regulated under the New \\nLegislat ive Framework legislation (e.g. machinery, toys, medical devices, etc.) will be subject \\nto the same ex -ante and ex -post compliance and enforcement mechanisms of the products of \\nwhich they are a component. The key difference is that the ex -ante and ex -post mechanisms \\nwill ensure compliance not only with the requirements established by sectorial legislation, but \\nalso with the requirements established by this regulation.  \\nAs regards stand -alone high -risk AI systems that are referred to in Annex III, a new \\ncomp liance and enforcement system will be established. This follows the model of the New \\nLegislative Framework legislation implemented through internal control checks by the \\nproviders with the exception of remote biometric identification systems that would be subject \\nto third party conformity assessment. A comprehensive ex -ante conformity assessment \\nthrough internal checks, combined with a strong ex -post enforcement, could be an effective \\nand reasonable solution for those systems, given the early phase of the r egulatory intervention \\nand the fact the AI sector is very innovative and expertise for auditing is only now being \\naccumulated. An assessment through internal checks for ‘stand -alone’ high -risk AI systems \\nwould require a full, effective and properly documen ted ex ante compliance with all \\nrequirements of the regulation and compliance with robust quality and risk management \\nsystems and post -market monitoring. After the provider has performed the relevant \\nconformity assessment, it should register those stand -alone high -risk AI systems in an EU \\ndatabase that will be managed by the Commission to increase public transparency and \\noversight and strengthen ex post supervision by competent authorities. By contrast, for \\nreasons of consistency with the existing product s afety legislation, the conformity assessments \\nof AI systems that are safety components of products will follow a system with third party \\nconformity assessment procedures already established under the relevant sectoral product \\nsafety legislation. New ex ant e re-assessments of the conformity will be needed in case of \\nsubstantial modifications to the AI systems (and notably changes which go beyond what is \\npre-determined by the provider in its technical documentation and checked at the moment of \\nthe ex -ante con formity assessment).  \\n5.2.4.  TRANSPARENCY OBLIGATIONS FOR CERTAIN AI SYSTEMS (TITLE IV)  \\nTitle IV  concerns certain AI systems to take account of the specific risks of manipulation they \\npose. Transparency obligations will apply for systems that (i) interact with humans, (ii) are \\nused to detect emotions or determine association with (social) categories based on biometric \\ndata, or (iii) generate or manipulate content (‘deep fakes’). When persons interact with an AI \\nsystem or their emotions or characteristics ar e recognised through automated means, people \\nmust be informed of that circumstance. If an AI system is used to generate or manipulate \\nimage, audio or video content that appreciably resembles  authentic content, there should be an \\nobligation to disclose that  the content is generated through automated means, subject to EN 15  EN exceptions for legitimate purposes (law enforcement, freedom of expression). This allows \\npersons to make informed choices or step back from a given situation.  \\n5.2.5.  MEASURES IN SUPPORT OF INNOV ATION (TITLE V)  \\nTitle V contributes to the objective to create a legal framework that is innovation -friendly, \\nfuture -proof and resilient to disruption. To that end, it encourages national competent \\nauthorities to set up regulatory sandboxes and sets a bas ic framework in terms of governance, \\nsupervision and liability. AI regulatory sandboxes establish a controlled environment to test \\ninnovative technologies for a limited time on the basis of a testing plan agreed with the \\ncompetent authorities. Title V also  contains measures to reduce the regulatory burden on \\nSMEs and start -ups.  \\n5.2.6.  GOVERNANCE AND IMPLEMENTATION (TITLES VI, VII AND VII)  \\nTitle VI sets up the governance systems at Union and national level. At Union level, the \\nproposal establishes a European Artificial Intelligence Board (the ‘Board’), composed of \\nrepresentatives from the Member States and the Commission. The Board will facilitate a \\nsmooth, effective and harmonised implementation of this regulation by contributing to the \\neffective cooperation of the national supervisory authorities and the Commission and \\nproviding advice and expertise to the Commission. It will also collect and share best practices \\namong the Member States.  \\nAt national level, Member States will have to designate one or more national competent \\nauthorities and, among them, the national supervisory authority, for the purpose of \\nsupervising the application and implementati on of the regulation. The European Data \\nProtection Supervisor will act as the competent authority for the supervision of the Union \\ninstitutions, agencies and bodies when they fall within the scope of this regulation.  \\nTitle VII  aims to facilitate the monito ring work of the Commission and national authorities \\nthrough the establishment of an EU -wide database for stand -alone high -risk AI systems with \\nmainly fundamental rights implications. The database will be operated by the Commission \\nand provided with data b y the providers of the AI systems, who will be required to register \\ntheir systems before placing them on the market or otherwise putting them into service.  \\nTitle VIII sets out the monitoring and reporting obligations for providers of AI systems with \\nregard  to post -market monitoring and reporting and investigating on AI -related incidents and \\nmalfunctioning. Market surveillance authorities would also control the market and investigate \\ncompliance with the obligations and requirements for all high -risk AI syste ms already placed \\non the market. Market surveillance authorities would have all powers under Regulation (EU) \\n2019/1020 on market surveillance. Ex -post enforcement should ensure that once the AI \\nsystem has been put on the market, public authorities have the  powers and resources to \\nintervene in case AI systems generate unexpected risks, which warrant rapid action. They will \\nalso monitor compliance of operators with their relevant obligations under the regulation. The \\nproposal does not foresee the automatic cr eation of any additional bodies or authorities at \\nMember State level. Member States may therefore appoint (and draw upon the expertise of) \\nexisting sectorial authorities, who would be entrusted also with the powers to monitor and \\nenforce the provisions of the regulation.  \\nAll this is without prejudice to the existing system and allocation of powers of ex -post \\nenforcement of obligations regarding fundamental rights in the Member States. When \\nnecessary for their mandate, existing supervision and enforcement a uthorities will also have \\nthe power to request and access any documentation maintained following this regulation and, \\nwhere needed, request market surveillance authorities to organise testing of the high -risk AI \\nsystem through technical means.  EN 16  EN 5.2.7.  CODES  OF CONDUCT (TITLE IX)  \\nTitle IX creates a framework for the creation of codes of conduct, which aim to encourage \\nproviders of non -high-risk AI systems to apply voluntarily the mandatory requirements for \\nhigh-risk AI systems (as laid out in Title III). Pro viders of non -high-risk AI systems may \\ncreate and implement the codes of conduct themselves. Those codes may also include \\nvoluntary commitments related, for example, to environmental sustainability, accessibility for \\npersons with disability, stakeholders’ participation in the design and development of AI \\nsystems, and diversity of development teams.  \\n5.2.8.  FINAL PROVISIONS (TITLES X, XI AND XII)  \\nTitle X  emphasizes the obligation of all parties to respect the confidentiality of information \\nand data  and sets  out rules for the exchange of information obtained during the \\nimplementation of the regulation. Title X also includes measures to ensure the effective \\nimplementation of the regulation through effective, proportionate, and dissuasive penalties for \\ninfringe ments of the provisions . \\nTitle XI sets out rules for the exercise of delegation and implementing powers. The proposal \\nempowers the Commission to adopt, where appropriate, implementing acts to ensure uniform \\napplication of the regulation or delegated acts t o update or complement the lists in Annexes I \\nto VII.  \\nTitle XII  contains an obligation for the Commission to assess regularly the need for an update \\nof Annex III and to prepare regular reports on the evaluation and review of the regulation. It \\nalso lays d own final provisions, including a differentiated transitional period for the initial \\ndate of the applicability of the regulation to facilitate the smooth implementation for all \\nparties concerned.  EN 17  EN 2021/0106 (COD)  \\nProposal for a  \\nREGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL  \\nLAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE \\n(ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION \\nLEGISLATIVE ACTS  \\nTHE EUROPEAN PARLIAMENT AND THE COUNCI L OF THE EUROPEAN UNION,  \\nHaving regard to the Treaty on the Functioning of the European Union, and in particular \\nArticles 16 and 114 thereof,  \\nHaving regard to the proposal from the European Commission,  \\nAfter transmission of the draft legislative act to the  national parliaments,  \\nHaving regard to the opinion of the European Economic and Social Committee31, \\nHaving regard to the opinion of the Committee of the Regions32, \\nActing in accordance with the ordinary legislative procedure,  \\nWhereas:  \\n(1) The purpose of thi s Regulation is to improve the functioning of the internal market by \\nlaying down a uniform legal framework in particular for the development, marketing \\nand use of artificial intelligence in conformity with Union values. This Regulation \\npursues a number of overriding reasons of public interest, such as a high level of \\nprotection of health, safety and fundamental rights, and it ensures the free movement \\nof AI -based goods and services cross -border, thus preventing Member States from \\nimposing restrictions on th e development, marketing and use of AI systems, unless \\nexplicitly authorised by this Regulation.  \\n(2) Artificial intelligence systems (AI systems) can be easily deployed in multiple sectors \\nof the economy and society, including cross border, and circulate throughout the \\nUnion. Certain Member States have already explored the adoption of national rules to \\nensure that artificial intelligence is safe and is developed and used in compliance with \\nfundamental rights obligations. Differing national rules may lead t o fragmentation of \\nthe internal market and decrease legal certainty for operators that develop or use AI \\nsystems. A consistent and high level of protection throughout the Union should \\ntherefore be ensured, while divergences hampering the free circulation o f AI systems \\nand related products and services within the internal market should be prevented, by \\nlaying down uniform obligations for operators and guaranteeing the uniform \\nprotection of overriding reasons of public interest and of rights of persons throug hout \\nthe internal market based on Article 114 of the Treaty on the Functioning of the \\nEuropean Union (TFEU). To the extent that this Regulation contains specific rules on \\nthe protection of individuals with regard to the processing of personal data  concerni ng \\n                                                 \\n31 OJ C […], […], p. […].  \\n32 OJ C […], […], p. […].  EN 18  EN restrictions of the use of AI systems for ‘real -time’ remote biometric identification in \\npublicly accessible spaces for the purpose of law enforcement, it is appropriate to base \\nthis Regulation, in as far as those specific rules are concerned, on Articl e 16 of the \\nTFEU. In light of  those specific rules and the recourse to Article 16 TFEU, it is \\nappropriate  to consult the European Data Protection Board.  \\n(3) Artificial intelligence is a fast evolving family of technologies that can contribute to a \\nwide arr ay of economic and societal benefits across the entire spectrum of industries \\nand social activities. By improving prediction, optimising operations and resource \\nallocation, and personalising digital solutions available for individuals and \\norganisations, th e use of artificial intelligence can provide key competitive advantages \\nto companies and support socially and environmentally beneficial outcomes, for \\nexample in healthcare, farming, education and training, infrastructure management, \\nenergy, transport and logistics, public services, security, justice, resource and energy \\nefficiency, and climate change mitigation and adaptation.  \\n(4) At the same time, depending on the circumstances regarding its specific application \\nand use, artificial intelligence may genera te risks and cause harm to public interests \\nand rights that are protected by Union law. Such harm might be material or \\nimmaterial.  \\n(5) A Union legal framework laying down harmonised rules on artificial intelligence is \\ntherefore needed to foster the develop ment, use and uptake of artificial intelligence in \\nthe internal market that at the same time meets a high level of protection of public \\ninterests, such as health and safety and the protection of fundamental rights, as \\nrecognised and protected by Union law.  To achieve that objective, rules regulating the \\nplacing on the market and putting into service of certain AI systems should be laid \\ndown, thus ensuring the smooth functioning of the internal market and allowing those \\nsystems to benefit from the principle of free movement of goods and services. By \\nlaying down those rules, this Regulation supports the objective of the Union of being a \\nglobal leader in the development of secure, trustworthy and ethical artificial \\nintelligence, as stated by the European Counci l33, and it ensures the protection of \\nethical principles, as specifically requested by the European Parliament34. \\n(6) The notion of AI system should be clearly defined to ensure legal certainty, while \\nproviding the flexibility to accommodate future technolog ical developments. The \\ndefinition should be based on the key functional characteristics of the software, in \\nparticular the ability, for a given set of human -defined objectives, to generate outputs \\nsuch as content, predictions, recommendations, or decisions  which influence the \\nenvironment with which the system interacts, be it in a physical or digital dimension. \\nAI systems can be designed to operate with varying levels of autonomy and be used on \\na stand -alone basis or as a component of a product, irrespectiv e of whether the system \\nis physically integrated into the product (embedded) or serve the functionality of the \\nproduct without being integrated therein (non -embedded). The definition of AI system \\nshould be complemented by a list of specific techniques and approaches used for its \\ndevelopment, which should be kept up -to–date in the light of market and technological \\n                                                 \\n33 European Council, Special meeting of the European Council (1 and 2 October 2020) – Conclusions, \\nEUCO 13/20, 2020, p. 6.  \\n34 European Parliament resolution of 20 October 2020 with recommendations to the Commissio n on a \\nframework of ethical aspects of artificial intelligence, robotics and related technologies, \\n2020/2012(INL).  EN 19  EN developments through the adoption of delegated acts by the Commission to amend that \\nlist. \\n(7) The notion of biometric data used in this Regulation  is in line with and should be \\ninterpreted consistently with the notion of biometric data as defined in Article 4(14) of \\nRegulation (EU) 2016/679 of the European Parliament and of the Council35, Article \\n3(18) of Regulation (EU) 2018/1725 of the European Par liament and of the Council36 \\nand Article 3(13) of Directive (EU) 2016/680 of the European Parliament and of the \\nCouncil37.   \\n(8) The notion of remote biometric identification system as used in this Regulation should \\nbe defined functionally, as  an AI system  intended for the identification of natural \\npersons at a distance through the comparison of a person’s biometric data with the \\nbiometric data contained in a reference database, and without prior knowledge whether \\nthe targeted person will be present and can  be identified, irrespectively of the \\nparticular technology, processes or types of biometric data used. Considering their \\ndifferent characteristics and manners in which they are used, as well as the different \\nrisks involved, a distinction should be made be tween ‘real -time’ and ‘post’ remote \\nbiometric identification systems. In the case of ‘real -time’ systems, the capturing of \\nthe biometric data, the comparison and the identification occur all instantaneously, \\nnear-instantaneously or in any event without a s ignificant delay. In this regard, there \\nshould be no scope for circumventing the rules of this Regulation on the ‘real -time’ \\nuse of the AI systems in question by providing for minor delays. ‘Real -time’ systems \\ninvolve the use of ‘live’ or ‘near -‘live’ mate rial, such as video footage, generated by a \\ncamera or other device with similar functionality. In the case of ‘post’ systems, in \\ncontrast, the biometric data have already been captured and the comparison and \\nidentification occur only after a significant de lay. This involves material, such as \\npictures or video footage generated by  closed circuit television cameras or private \\ndevices, which has been generated before the use of the system in respect of the \\nnatural persons concerned.  \\n(9) For the purposes of thi s Regulation the notion of publicly accessible space should be \\nunderstood as referring to any physical place that is accessible to the public, \\nirrespective of whether the place in question is privately or publicly owned. Therefore, \\nthe notion does not cove r places that are private in nature and normally not freely \\naccessible for third parties, including law enforcement authorities, unless those parties \\nhave been specifically invited or authorised, such as homes, private clubs, offices, \\nwarehouses and factor ies.  Online spaces are not covered either, as they are not \\nphysical spaces. However, the mere fact that certain conditions for accessing a \\n                                                 \\n35 Regulation  (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the \\nprotection of natural persons with regard to  the processing of personal data and on the free movement of \\nsuch data, and repealing Directive 95/46/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, \\np. 1).  \\n36 Regulation (EU) 2018/1725 of the European Parliament and of the Council of 23 Octobe r 2018 on the \\nprotection of natural persons with regard to the processing of personal data by the Union institutions, \\nbodies, offices and agencies and on the free movement of such data, and repealing Regulation (EC) No \\n45/2001 and Decision No 1247/2002/EC (OJ L 295, 21.11.2018, p. 39)  \\n37 Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the \\nprotection of natural persons with regard to the processing of personal data by competent authorities for \\nthe purposes of the prevention, investigation, detection or prosecution of criminal offences or the \\nexecution of criminal penalties, and on the free movement of such data, and repealing Council \\nFramework Decision 2008/977/JHA (Law Enforcement Directive) ( OJ L 119, 4.5.2016, p . 89).  EN 20  EN particular space may apply, such as admission tickets or age restrictions, does not \\nmean that the space is not publi cly accessible within the meaning of this Regulation. \\nConsequently, in addition to public spaces such as streets, relevant parts of \\ngovernment buildings and most transport infrastructure, spaces such as cinemas, \\ntheatres, shops and shopping centres are nor mally also publicly accessible. Whether a \\ngiven space is accessible to the public should however be determined on a case -by-\\ncase basis, having regard to the specificities of the individual situation at hand.   \\n(10) In order to ensure a level playing field a nd an effective protection of rights and \\nfreedoms of individuals across the Union, the rules established by this Regulation \\nshould apply to providers of AI systems in a non -discriminatory manner, irrespective \\nof whether they are established within the Unio n or in a third country, and to users of \\nAI systems established within the Union.  \\n(11) In light of their digital nature, certain AI systems should fall within the scope of this \\nRegulation even when they are neither placed on the market, nor put into servic e, nor \\nused in the Union. This is the case for example of an operator established in the Union \\nthat contracts certain services to an operator established outside the Union in relation \\nto an activity to be performed by an AI system that would qualify as hig h-risk and \\nwhose effects impact natural persons located in the Union. In those circumstances, the \\nAI system used by the operator outside the Union could process data lawfully \\ncollected in and transferred from the Union, and provide to the contracting opera tor in \\nthe Union the output of that AI system resulting from that processing, without that AI \\nsystem being placed on the market, put into service or used in the Union. To prevent \\nthe circumvention of this Regulation and to ensure an effective protection of  natural \\npersons located in the Union, this Regulation should also apply to providers and users \\nof AI systems that are established in a third country, to the extent the output produced \\nby those systems is used in the Union. Nonetheless, to take into accoun t existing \\narrangements and special needs for cooperation with foreign partners with whom \\ninformation and evidence is exchanged, this Regulation should not apply to public \\nauthorities of a third country and international organisations when acting in the \\nframework of international agreements concluded at national or European level for law \\nenforcement and judicial cooperation with the Union or with its Member States. Such \\nagreements have been concluded bilaterally between Member States and third \\ncountries or between the European Union, Europol and other EU agencies and third \\ncountries and international organisations.  \\n(12) This Regulation should also apply to Union institutions, offices, bodies and agencies \\nwhen acting as a provider or user of an AI system. AI  systems exclusively developed \\nor used for military purposes should be excluded from the scope of this Regulation \\nwhere that use falls under the exclusive remit of the Common Foreign and Security \\nPolicy regulated under Title V of the Treaty on the European  Union (TEU).  This \\nRegulation should be without prejudice to the provisions regarding the liability of \\nintermediary service providers set out in Directive 2000/31/EC of the European \\nParliament and of the Council [as amended by the Digital Services Act].  \\n(13) In order to ensure a consistent and high level of protection of public interests as \\nregards health, safety and fundamental rights, common normative standards for all \\nhigh-risk AI systems should be established. Those standards should be consistent with  \\nthe Charter of fundamental rights of the European Union (the Charter) and should be \\nnon-discriminatory and in line with the Union’s international trade commitments.  EN 21  EN (14) In order  to introduce a proportionate and effective set of binding rules for AI syste ms, \\na clearly defined risk -based approach should be followed. That approach should tailor \\nthe type and content of such rules to the intensity and scope of the risks that AI \\nsystems can generate. It is therefore necessary to prohibit certain artificial inte lligence \\npractices, to lay down requirements for high -risk AI systems and obligations for the \\nrelevant operators, and to lay down transparency obligations for certain AI systems.  \\n(15) Aside from the many beneficial uses of artificial intelligence, that tec hnology can also \\nbe misused and provide novel and powerful tools for manipulative, exploitative and \\nsocial control practices. Such practices are particularly harmful and should be \\nprohibited because they contradict Union values of respect for human dignity , \\nfreedom, equality, democracy and the rule of law and Union fundamental rights, \\nincluding the right to non -discrimination, data protection and privacy  and the rights of \\nthe child . \\n(16) The placing on the market, putting into service or use of certain AI s ystems intended \\nto distort human  behaviour, whereby physical or psychological harms are likely to \\noccur, should be forbidden. Such AI systems deploy subliminal components \\nindividuals cannot perceive or exploit vulnerabilities of children and people due to \\ntheir age, physical or mental incapacities. They do so with the intention to materially \\ndistort the behaviour of a person and in a manner that causes or is likely to cause harm \\nto that or another person. The intention may not be presumed if the distortion of \\nhuman behaviour results from factors external to the AI system which are outside of \\nthe control of the provider or the user. Research for legitimate purposes in relation to \\nsuch AI systems should not be stifled by the prohibition, if such research does not \\namount to use of the AI system in human -machine relations that exposes natural \\npersons to harm and such research is carried out in accordance with recognised ethical \\nstandards for scientific research.  \\n(17) AI systems  providing social scoring of natural  persons for general purpose by public \\nauthorities or on their behalf may lead to discriminatory outcomes and the exclusion of \\ncertain groups. They may violate the right to dignity and non -discrimination and the \\nvalues of equality and justice. Such AI syst ems evaluate or classify the trustworthiness \\nof natural persons based on their social behaviour in multiple contexts or known or \\npredicted personal or personality characteristics. The social score obtained from such \\nAI systems may lead to the detrimental o r unfavourable treatment of natural persons or \\nwhole groups thereof in social contexts, which are unrelated to the context in which \\nthe data was originally generated or collected or to a detrimental treatment that is \\ndisproportionate or unjustified to the gravity of their social behaviour.  Such AI \\nsystems should be therefore prohibited.  \\n(18) The use of AI systems for ‘real -time’ remote biometric identification of natural \\npersons in publicly accessible spaces for the purpose of law enforcement is considered \\nparticularly intrusive in the rights and freedoms of the concerned persons, to the extent \\nthat it may affect the private life of a large part of the population, evoke a feeling of \\nconstant surveillance and indirectly dissuade the exercise of the freedom of  assembly \\nand other fundamental rights. In addition, the immediacy of the impact and the limited \\nopportunities for further checks or corrections  in relation to the use of such systems \\noperating in ‘real -time’ carry heightened risks for the rights and freed oms of the \\npersons that are concerned by law enforcement activities.  \\n(19) The use of those systems for the purpose of law enforcement should therefore be \\nprohibited, except in three exhaustively listed and narrowly defined situations, where EN 22  EN the use is stri ctly necessary to achieve a substantial public interest, the importance of \\nwhich outweighs the risks. Those situations involve the search for potential victims of \\ncrime, including missing children; certain threats to the life or physical safety of \\nnatural persons or of a terrorist attack; and the detection, localisation, identification or \\nprosecution of perpetrators or suspects of the criminal offences referred to in Council \\nFramework Decision 2002/584/JHA38 if those criminal offences are punishable in the \\nMember State concerned by a custodial sentence or a detention order for a maximum \\nperiod of at least three years and as they are defined in the law of that Member State . \\nSuch threshold for the custodial sentence or detention order in accordance with \\nnationa l law contributes to ensure that the offence should be serious enough to \\npotentially justify the use of ‘real -time’ remote biometric identification systems. \\nMoreover, of the 32 criminal offences listed in the Council Framework Decision \\n2002/584/JHA, some a re in practice likely to be more relevant than others, in that the \\nrecourse to ‘real-time’ remote biometric identification  will foreseeably be necessary \\nand proportionate to highly varying degrees for the practical pursuit of the detection, \\nlocalisation, i dentification or prosecution of a perpetrator or suspect of the different \\ncriminal offences listed and having regard to the likely differences in  the seriousness, \\nprobability and scale of the harm or possible negative consequences.  \\n(20) In order to ensure that those systems are used in a responsible and proportionate \\nmanner, it is also important to establish that, in each of those three exhaustively listed \\nand narrowly defined situations, certain elements should be taken into account, in \\nparticular as regar ds the nature of the situation giving rise to the request and the \\nconsequences of the use for the rights and freedoms of all persons concerned  and the \\nsafeguards and conditions provided for with the use. In addition, the use of ‘real -time’ \\nremote biometric identification systems in publicly accessible spaces for the purpose \\nof law enforcement should be subject to appropriate limits in time and space, hav ing \\nregard in particular to the evidence or indications regarding the threats, the victims or \\nperpetrator.  The reference database of persons should be appropriate for each use case \\nin each of the three situations mentioned above.  \\n(21) Each use of a ‘real -time’ remote biometric identification system in publicly accessible \\nspaces for the purpose of law enforcement should be subject to an express and specific \\nauthorisation by a judicial authority or by an independent administrative authority of a \\nMember State.  Such authorisation should in principle be obtained prior to the use, \\nexcept in duly justified situations of urgency, that is, situations where the need to use \\nthe systems in question is such as to make it effectively and objectively impossible to \\nobtain a n authorisation before commencing the use. In such situations of urgency, the \\nuse should be restricted to the absolute minimum necessary and be subject to \\nappropriate safeguards and conditions, as determined in national law and specified in \\nthe context of each individual urgent use case by the law enforcement authority itself. \\nIn addition, the law enforcement authority should in such situations seek to obtain an \\nauthorisation as soon as possible, whilst providing the reasons for not having been able \\nto requ est it earlier.  \\n(22) Furthermore, it is appropriate to provide, within the exhaustive framework set by this \\nRegulation that such use in the territory of a Member State in accordance with this \\nRegulation should only be possible where and in as far as the M ember State in \\nquestion has decided to expressly provide for the possibility to authorise such use in its \\n                                                 \\n38 Council Framework Decision 2002/584/JHA of 13 June 2002 on the European arrest warrant and the \\nsurrender procedures between Member States ( OJ L 190, 18.7.2002, p. 1).  EN 23  EN detailed rules of national law. Consequently, Member States remain free under this \\nRegulation not to provide for such a possibility at all or to only provide for such a \\npossibility in respect of some of the objectives capable of justifying authorised use \\nidentified in this Regulation.  \\n(23) The use of AI systems for ‘real -time’ remote biometric identification of natural \\npersons in publicly accessible spa ces for the purpose of law enforcement necessarily \\ninvolves the processing of biometric data. The rules of this Regulation that prohibit, \\nsubject to certain exceptions, such use, which are based on Article 16 TFEU, should \\napply as lex specialis  in respect of the rules on the processing of biometric data \\ncontained in Article 10 of Directive (EU) 2016/680, thus regulating such use and the \\nprocessing of biometric data involved in an exhaustive manner. Therefore, such use \\nand processing should only be possible in as far as it is compatible with the framework \\nset by this Regulation, without there being scope, outside that framework, for the \\ncompetent authorities, where they act for purpose of law enforcement, to use such \\nsystems and process such data in connectio n thereto on the grounds listed in Article 10 \\nof Directive (EU) 2016/680. In this context, this Regulation is not intended to provide \\nthe legal basis for the processing of personal data under Article 8 of Directive \\n2016/680. However , the use of ‘real -time’  remote biometric identification systems in \\npublicly accessible spaces for purposes other than law enforcement, including by \\ncompetent authorities, should not be covered by the specific framework regarding such \\nuse for the purpose of law enforcement set by  this Regulation. Such use for purposes \\nother than law enforcement should therefore not be subject to the requirement of an \\nauthorisation under this Regulation  and the applicable detailed rules of national law \\nthat may give effect to it.  \\n(24) Any processin g of biometric data and other personal data involved in the use of AI \\nsystems for biometric identification, other than in connection to the use of ‘real -time’ \\nremote biometric identification systems in publicly accessible spaces for the purpose \\nof law enfo rcement as regulated by this Regulation, including where those systems are \\nused by competent authorities in publicly accessible spaces for other purposes than \\nlaw enforcement, should continue to comply with all requirements resulting from \\nArticle 9(1) of R egulation (EU) 2016/679, Article 10(1) of Regulation (EU) \\n2018/1725 and Article 10 of Directive (EU) 2016/680, as applicable.   \\n(25) In accordance with Article 6a of Protocol No 21 on the position of the United \\nKingdom and Ireland in respect of the area of freedom, security and justice, as \\nannexed to the TEU and to the TFEU, Ireland is not bound by the rules laid down in \\nArticle 5(1), point (d), (2) and (3) of this Regulation adopted on the basis of Article 16 \\nof the TFEU which relate to the processing of pe rsonal data by the Member States \\nwhen carrying out activities falling within the scope of Chapter 4 or Chapter 5 of Title \\nV of Part Three of the TFEU, where Ireland is not bound by the rules governing the \\nforms of judicial cooperation in criminal matters o r police cooperation which require \\ncompliance with the provisions laid down on the basis of Article 16 of the TFEU.   \\n(26) In accordance with Articles 2 and 2a of Protocol No 22 on the position of Denmark, \\nannexed to the TEU and TFEU, Denmark is not bound b y rules laid down in Article \\n5(1), point (d), (2) and (3) of this Regulation adopted on the basis of Article 16 of the \\nTFEU, or subject to their application, which relate to the processing of personal data \\nby the Member States when carrying out activities falling within the scope of Chapter \\n4 or Chapter 5 of Title V of Part Three of the TFEU.   EN 24  EN (27) High -risk AI systems should only be placed on the Union market or put into service if \\nthey comply with certain mandatory requirements. Those requirements should ensure \\nthat high -risk AI systems available in the Union or whose output is otherwise used in \\nthe Union do not pose unacceptable risks to important Union public interests  as \\nrecognised and protected by Union law . AI systems identified as high -risk should be  \\nlimited to those that have a significant harmful impact on the health, safety and \\nfundamental rights of persons in the Union and such limitation minimises any \\npotential restriction to international trade, if any.  \\n(28) AI systems could produce adverse outc omes to health and safety of persons, in \\nparticular when such systems operate as components of products. Consistently with \\nthe objectives of Union harmonisation legislation to facilitate the free movement of \\nproducts in the internal market and to ensure th at only safe and otherwise compliant \\nproducts find their way into the market, it is important that the safety risks that may be \\ngenerated by a product as a whole due to its digital components, including AI systems, \\nare duly prevented and mitigated. For ins tance, increasingly autonomous robots, \\nwhether in the context of manufacturing or personal assistance and care should be able \\nto safely operate and performs their functions in complex environments. Similarly, in \\nthe health sector where the stakes for life and health are particularly high, increasingly \\nsophisticated diagnostics systems and systems supporting human decisions should be \\nreliable and accurate. The extent of the adverse impact caused by the AI system on the \\nfundamental rights protected by the Cha rter is of particular relevance when classifying \\nan AI system as high -risk. Those rights include the right to human dignity, respect for \\nprivate and family life, protection of personal data, freedom of expression and \\ninformation, freedom of assembly and of  association, and non -discrimination, \\nconsumer protection, workers’ rights,  rights of persons with disabilities, right to an \\neffective remedy and to a fair trial, right of defence and the presumption of innocence, \\nright to good administration.  In addition to those rights, it is important to highlight that \\nchildren have specific rights as enshrined in Article 24 of the EU Charter and in the \\nUnited Nations Convention on the Rights of the Child (further elaborated in the \\nUNCRC General Comment No. 25 as regards  the digital environment), both of which \\nrequire consideration of the children’s vulnerabilities and provision of such protection \\nand care as necessary for their well -being.  The fundamental right to a high level of \\nenvironmental protection enshrined in the  Charter and implemented in Union policies \\nshould also be considered when assessing the severity of the harm that an AI system \\ncan cause, including in relation to the health and safety of persons.  \\n(29) As regards high -risk AI systems that are safety compon ents of products or systems, or \\nwhich are themselves products or systems falling within the scope of Regulation (EC) \\nNo 300/2008 of the European Parliament and of the Council39, Regulation (EU) No \\n167/2013  of the European Parliament and of the Council40, Reg ulation (EU) No \\n168/2013  of the European Parliament and of the Council41, Directive 2014/90/EU  of \\n                                                 \\n39 Regulation (EC) No 300/2008 of the European Parliament and of the Council of 11 March 2008 on \\ncommon rules in the field of civil aviation security and repealing Regulation (EC) No 2320/2002 (OJ L \\n97, 9.4.2008, p. 72).  \\n40 Regulation (EU) No 167/2013 of the European Parliament and of the Council of 5 February 2013 on the \\napproval and market surveillance of agricultural and forestry vehicles (OJ L 60, 2.3.2013, p. 1).  \\n41 Regulation (EU) No 168/2013 of the European Parliament and of the Council of 15 January 2013 on the \\napproval and market surveillance of two - or three -wheel vehicles and quadricycles (OJ L 60, 2.3.2013, \\np. 52).  EN 25  EN the European Parliament and of the Council42, Directive (EU) 2016/797  of the \\nEuropean Parliament and of the Council43, Regulation (EU) 2018/858  of the European \\nParliament and of the Council44, Regulation (EU) 2018/1139 of the European \\nParliament and of the Council45, and Regulation (EU) 2019/2144  of the European \\nParliament and of the Council46, it is appropriate to amend those acts to ensure that the \\nCommission take s into account, on the basis of the technical and regulatory \\nspecificities of each sector, and without interfering with existing governance, \\nconformity assessment and enforcement mechanisms and authorities established \\ntherein, the mandatory requirements fo r high -risk AI systems laid down in this \\nRegulation when adopting any relevant future delegated or implementing acts on the \\nbasis of those acts.  \\n(30) As regards AI systems that are safety components of products, or which are \\nthemselves products, falling wi thin the scope of certain Union harmonisation \\nlegislation, it is appropriate to classify them as high -risk under this Regulation if the \\nproduct in question undergoes the conformity assessment procedure with a third -party \\nconformity assessment body pursuant  to that relevant Union harmonisation legislation. \\nIn particular, such products are machinery, toys, lifts, equipment and protective \\nsystems intended for use in potentially explosive atmospheres, radio equipment, \\npressure equipment, recreational craft equi pment, cableway installations,  appliances \\nburning gaseous fuels, medical devices, and in vitro diagnostic medical devices.  \\n(31) The classification of an AI system as high -risk pursuant to this Regulation should not \\nnecessarily mean that the product whose s afety component is the AI system, or the AI \\nsystem itself as a product, is considered ‘high -risk’ under the criteria established in the \\nrelevant Union harmonisation legislation that applies to the product. This is notably \\nthe case for Regulation (EU) 2017/ 745 of the European Parliament and of the \\n                                                 \\n42 Directive 2014/90/EU of the European Parliament and of the Council of 23 July 2014 on marine \\nequipment and repealing Council Directive 96/98/EC (OJ L 257, 28.8.2014, p. 146).  \\n43 Directive (EU) 2016/797 of the Europ ean Parliament and of the Council of 11 May 2016 on the \\ninteroperability of the rail system within the European Union (OJ L 138, 26.5.2016, p. 44).  \\n44 Regulation (EU) 2018/858 of the European Parliament and of the Council of 30 May 2018 on the \\napproval and market surveillance of motor vehicles and their trailers, and of systems, components and \\nseparate technical units intended for such vehicles, amending Regulations (EC) No 715/2007 and (EC) \\nNo 595/2009 and repealing Directive 2007/46/EC (OJ L 151, 14.6.2018 , p. 1).  \\n45 Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common \\nrules in the field of civil aviation and establishing a European Union Aviation Safety Agency, and \\namending Regulations (EC) No 2111/2005, (EC) No 1 008/2008, (EU) No 996/2010, (EU) No 376/2014 \\nand Directives 2014/30/EU and 2014/53/EU of the European Parliament and of the Council, and \\nrepealing Regulations (EC) No 552/2004 and (EC) No 216/2008 of the European Parliament and of the \\nCouncil and Council R egulation (EEC) No 3922/91 (OJ L 212, 22.8.2018, p. 1).  \\n46 Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November 2019 on \\ntype-approval requirements for motor vehicles and their trailers, and systems, components and separate \\ntechnical units intended for such vehicles, as regards their general safety and the protection of vehicle \\noccupants and vulnerable road users, amending Regulation (EU) 2018/858 of the European Parliament \\nand of the Council and repealing Regulations (EC) No  78/2009, (EC) No 79/2009 and (EC) No \\n661/2009 of the European Parliament and of the Council and Commission Regulations (EC) No \\n631/2009, (EU) No 406/2010, (EU) No 672/2010, (EU) No 1003/2010, (EU) No 1005/2010, (EU) No \\n1008/2010, (EU) No 1009/2010, (EU) N o 19/2011, (EU) No 109/2011, (EU) No 458/2011, (EU) No \\n65/2012, (EU) No 130/2012, (EU) No 347/2012, (EU) No 351/2012, (EU) No 1230/2012 and (EU) \\n2015/166 (OJ L 325, 16.12.2019, p. 1).  EN 26  EN Council47 and Regulation (EU) 2017/746 of the European Parliament and of the \\nCouncil48, where a third -party conformity assessment is provided for medium -risk and \\nhigh-risk products.   \\n(32) As regards stand -alone AI sy stems, meaning high -risk AI systems other than those that \\nare safety components of products, or which are themselves products, it is appropriate \\nto classify them as high -risk if, in the light of their intended purpose, they pose a high \\nrisk of harm to the health and safety or the fundamental rights of persons, taking into \\naccount both the severity of the possible harm and its probability of occurrence and \\nthey are used in a number of specifically pre -defined areas specified in the Regulation. \\nThe identifica tion of those systems is based on the same methodology and criteria \\nenvisaged also for any future amendments of the list of high -risk AI systems.  \\n(33) Technical inaccuracies of AI systems intended for the remote biometric identification \\nof natural persons can lead to biased results and entail discriminatory effects. This is \\nparticularly relevant when it comes to age, ethnicity, sex or disabilities. Therefore, \\n‘real-time’ and ‘post’ remote biometric identification systems should be classified as \\nhigh-risk. In view of the risks that they pose, both types of remote biometric \\nidentification systems should be subject to specific requirements on logging \\ncapabilities and human oversight.   \\n(34) As regards the management and operation of critical infrastructure, it i s appropriate to \\nclassify as high -risk the AI systems intended to be used as safety components in the \\nmanagement and operation of road traffic and the supply of water, gas, heating and \\nelectricity, since their failure or malfunctioning may put at risk the life and health of \\npersons at large scale and lead to appreciable disruptions in the ordinary conduct of \\nsocial and economic activities.  \\n(35) AI systems used in education or vocational training, notably for determining access or \\nassigning persons to educa tional and vocational training institutions or to evaluate \\npersons on tests as part of or as a precondition for their education should be considered \\nhigh-risk, since they may determine the educational and professional course of a \\nperson’s life and therefor e affect their ability to secure their livelihood. When \\nimproperly designed and used, such systems may violate the right to education and \\ntraining as well as the right not to be discriminated against and perpetuate historical \\npatterns of discrimination.  \\n(36) AI systems used in employment, workers management and access to self -employment, \\nnotably for the recruitment and selection of persons, for making decisions on \\npromotion and termination and for task allocation, monitoring or evaluation of persons \\nin work -related contractual relationships, should also be classified as high -risk, since \\nthose systems may appreciably impact future career prospects and livelihoods of these \\npersons. Relevant work -related contractual relationships should involve employees \\nand pe rsons providing services through platforms as referred to in the Commission \\nWork Programme 2021. Such persons should in principle not be considered users \\nwithin the meaning of this Regulation. Throughout the recruitment process and in the \\n                                                 \\n47 Regulation (EU) 2017/745 of the European Parliament and of the Council  of 5 April 2017 on medical \\ndevices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No \\n1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (OJ L 117, 5.5.2017, p. 1).  \\n48 Regulation (EU) 2017/746 of the Europea n Parliament and of the Council of 5 April 2017 on in vitro \\ndiagnostic medical devices and repealing Directive 98/79/EC and Commission Decision 2010/227/EU \\n(OJ L 117, 5.5.2017, p. 176).  EN 27  EN evaluation, promo tion, or retention of persons in work -related contractual \\nrelationships, such systems may perpetuate historical patterns of discrimination, for \\nexample against women, certain age groups, persons with disabilities, or persons of \\ncertain racial or ethnic ori gins or sexual orientation. AI systems used to monitor the \\nperformance and behaviour of these persons may also impact their rights to data \\nprotection and privacy.  \\n(37) Another area in which the use of AI systems deserves special consideration is the \\nacces s to and enjoyment of certain essential private and public services and benefits \\nnecessary for people  to fully participate in society or to improve one’s standard of \\nliving . In particular, AI systems used to evaluate the credit score or creditworthiness of  \\nnatural persons should be classified as high -risk AI systems, since they determine \\nthose persons’ access to financial resources or essential services such as housing, \\nelectricity, and telecommunication services. AI systems used for this purpose may lead \\nto discrimination of persons or groups and perpetuate historical patterns of \\ndiscrimination, for example based on racial or ethnic origins, disabilities, age, sexual \\norientation, or create new forms of discriminatory impacts. Considering the very \\nlimited sc ale of the impact and the available alternatives on the market, it is \\nappropriate to exempt AI systems for the purpose of creditworthiness assessment and \\ncredit scoring when put into service by small -scale providers for their own use. \\nNatural persons apply ing for or receiving public assistance benefits and services from \\npublic authorities are typically dependent on those benefits and services and in a \\nvulnerable position in relation to the responsible authorities. If AI systems are used for \\ndetermining whet her such benefits and services should be denied, reduced, revoked or \\nreclaimed by authorities, they may have a significant impact on persons’ livelihood \\nand may infringe their fundamental rights, such as the right to social protection, non -\\ndiscrimination, human dignity or an effective remedy. Those systems should therefore \\nbe classified as high -risk. Nonetheless, this Regulation should not hamper the \\ndevelopment and use of innovative approaches in the public administration, which \\nwould stand to benefit from  a wider use of compliant and safe AI systems, provided \\nthat those systems do not entail a high risk to legal and natural persons. Finally, AI \\nsystems used to dispatch or establish priority in the dispatching of emergency first \\nresponse services should als o be classified as high -risk since they make decisions in \\nvery critical situations for the life and health of persons and their property.  \\n(38) Actions by law enforcement authorities involving certain uses of AI systems are \\ncharacterised by a significant de gree of power imbalance and may lead to surveillance, \\narrest or deprivation of a natural person’s liberty as well as other adverse impacts on \\nfundamental rights guaranteed in the Charter. In particular, if the AI system is not \\ntrained with high quality dat a, does not meet adequate requirements in terms of its \\naccuracy or robustness, or is not properly designed and tested before being put on the \\nmarket or otherwise put into service, it may single out people in a discriminatory or \\notherwise incorrect or unjus t manner. Furthermore, the exercise of important \\nprocedural fundamental rights, such as the right to an effective remedy and to a fair \\ntrial as well as the right of defence and the presumption of innocence, could be \\nhampered, in particular, where such AI s ystems are not sufficiently transparent, \\nexplainable and documented.  It is therefore appropriate to classify as high -risk a \\nnumber of AI systems intended to be used in the law enforcement context where \\naccuracy, reliability and transparency is particularly  important to avoid adverse \\nimpacts, retain public trust and ensure accountability and effective redress. In view of \\nthe nature of the activities in question and the risks relating thereto, those high -risk AI \\nsystems should include in particular AI systems  intended to be used by law EN 28  EN enforcement authorities for individual risk assessments, polygraphs and similar tools \\nor to detect the emotional state of natural person, to detect ‘deep fakes’,  for the \\nevaluation of the reliability of evidence in criminal proc eedings, for predicting the \\noccurrence or reoccurrence of an actual or potential criminal offence based on \\nprofiling of natural persons, or assessing personality traits and characteristics or past \\ncriminal behaviour of natural persons or groups, for profil ing in the course of \\ndetection, investigation or prosecution of criminal offences, as well as for  crime \\nanalytics regarding natural persons.  AI systems specifically intended to be used for \\nadministrative proceedings by tax and customs authorities should no t be considered \\nhigh-risk AI systems used by law enforcement authorities for the purposes of \\nprevention, detection, investigation and prosecution of criminal offences.   \\n(39) AI systems used in migration, asylum and border control management affect people \\nwho are often in particularly vulnerable position and who are dependent on the \\noutcome of the actions of the competent public authorities. The accuracy, non -\\ndiscriminatory nature and transparency of the AI systems used in those contexts are \\ntherefore partic ularly important to guarantee the respect of the fundamental rights of \\nthe affected persons, notably their rights to free movement, non -discrimination, \\nprotection of private life and personal data, international protection and good \\nadministration. It is th erefore appropriate to classify as high -risk AI systems intended \\nto be used by the competent public authorities charged with tasks in the fields of \\nmigration, asylum and border control management as polygraphs and similar tools or \\nto detect the emotional s tate of a natural person; for assessing certain risks posed by \\nnatural persons entering the territory of a Member State or applying for visa or \\nasylum;  for verifying the authenticity of the relevant documents of natural persons; for \\nassisting competent pub lic authorities for the examination of applications for asylum, \\nvisa and residence permits and associated complaints with regard to the objective to \\nestablish the eligibility of the natural persons applying for a status.  AI systems in the \\narea of migration , asylum and border control management covered by this Regulation \\nshould comply with the relevant procedural requirements set by the Directive \\n2013/32/EU  of the European Parliament and of the Council49, the Regulation (EC) No \\n810/2009 of the European Parliament and of the Council50 and other relevant \\nlegislation.  \\n(40) Certain AI systems  intended for the administration of justice and democratic processes  \\nshould be classified as high -risk, considering their potentially significant impact on \\ndemocracy, rul e of law, individual freedoms as well as the right to an effective remedy \\nand to a fair trial. In particular, to address the risks of potential biases, errors and \\nopacity, it is appropriate to qualify as high -risk AI systems intended to assist judicial \\nauthorities in researching and interpreting facts and the law and in applying the law to \\na concrete set of facts. Such qualification should not extend, however, to AI systems \\nintended for purely ancillary administrative activities that do not affect the actua l \\nadministration of justice in individual cases, such as anonymisation or \\npseudonymisation of judicial decisions, documents or data, communication between \\npersonnel, administrative tasks or allocation of resources.  \\n                                                 \\n49  Directive 2013/32/EU of the European Parliament and of the Council of 26  June 2013 on common \\nprocedures for granting and withdrawing international protection ( OJ L 180, 29.6.2013, p. 60).  \\n50  Regulation (EC) No  810/2009 of the European Parliament and of the Council of 13  July 2009 \\nestablishing a Community Code on Visas (Vi sa Code) ( OJ L 243, 15.9.2009, p.  1). \\n EN 29  EN (41) The fact that an AI system is classi fied as high risk under this Regulation should not \\nbe interpreted as indicating that the use of the system is necessarily lawful under other \\nacts of Union law or under national law compatible with Union law, such as on the \\nprotection of personal data, on t he use of polygraphs and similar tools or other systems \\nto detect the emotional state of natural persons . Any such use should continue to occur \\nsolely in accordance with the applicable requirements resulting from the Charter and \\nfrom the applicable acts of  secondary Union law and national law. This Regulation \\nshould not be understood as providing for the legal ground for processing of personal \\ndata, including special categories of personal data,  where relevant .  \\n(42) To mitigate the risks from high -risk AI systems placed or otherwise put into service on \\nthe Union market for users and affected persons, certain mandatory requirements \\nshould apply, taking into account the intended purpose of the use of the system and \\naccording to the risk management system to be established by the provider.  \\n(43) Requirements should apply to high -risk AI systems as regards the quality of data sets \\nused, technical documentation and record -keeping, transparency and the provision of \\ninformati on to users, human oversight, and robustness, accuracy and cybersecurity. \\nThose requirements are necessary to effectively mitigate the risks for health, safety \\nand fundamental rights, as applicable in the light of the intended purpose of the \\nsystem, and no  other less trade restrictive measures are reasonably available, thus \\navoiding unjustified restrictions to trade.  \\n(44) High data quality is essential for the performance of many AI systems, especially \\nwhen techniques involving the training of models are u sed, with a view to ensure that \\nthe high -risk AI system performs as intended and safely and it does not become the \\nsource of discrimination prohibited by Union law. High quality training, validation \\nand testing data sets require the implementation of appro priate data governance and \\nmanagement practices. Training, validation and testing data sets should be sufficiently \\nrelevant, representative and free of errors and complete in view of the intended \\npurpose of the system. They should also have the appropriate  statistical properties, \\nincluding as regards the persons or groups of persons on which the high -risk AI \\nsystem is intended to be used. In particular, training, validation and testing data sets \\nshould take into account, to the extent required in the light of their intended purpose, \\nthe features, characteristics or elements that are particular to the specific geographical, \\nbehavioural or functional setting or context within which the AI system is intended to \\nbe used.  In order to protect the right of others f rom the discrimination that might result \\nfrom the bias in AI systems, the providers shouldbe able to process also special \\ncategories of personal data, as a matter of substantial public interest, in order to ensure \\nthe bias monitoring, detection and correct ion in relation to high -risk AI systems.  \\n(45) For the development of high -risk AI systems, certain actors, such as providers, \\nnotified bodies and other relevant entities, such as digital innovation hubs, testing \\nexperimentation facilities and researchers, should be able to access and use high \\nquality datasets within their respective fields of activities which are related to this \\nRegulation. European common data spaces established by the Commission and the \\nfacilitation of data sharing between businesses and with government in the public \\ninterest will be instrumental to provide trustful, accountable and non -discriminatory \\naccess to high quality data for the training, validation and testing of AI systems. For \\nexample, in health, the European health data space w ill facilitate non -discriminatory \\naccess to health data and the training of artificial intelligence algorithms on those \\ndatasets, in a privacy -preserving, secure, timely, transparent and trustworthy manner, \\nand with an appropriate institutional governance.  Relevant competent authorities, EN 30  EN including sectoral ones, providing or supporting the access to data may also support \\nthe provision of high -quality data for the training, validation and testing of AI systems.  \\n(46) Having information on how high -risk AI sys tems have been developed and how they \\nperform throughout their lifecycle is essential to verify compliance with the \\nrequirements under this Regulation. This requires keeping records and the availability \\nof a technical documentation, containing information which is necessary to assess the \\ncompliance of the AI system with the relevant requirements. Such information should \\ninclude the general characteristics, capabilities and limitations of the system, \\nalgorithms, data, training, testing and validation process es used as well as \\ndocumentation on the relevant risk management system. The technical documentation \\nshould be kept up to date.  \\n(47) To address the opacity that may make certain AI systems incomprehensible to or too \\ncomplex for natural persons, a certain d egree of transparency should be required for \\nhigh-risk AI systems. Users should be able to interpret the system output and use it \\nappropriately. High -risk AI systems should therefore be accompanied by relevant \\ndocumentation and instructions of use and incl ude concise and clear information, \\nincluding in relation to possible risks to fundamental rights and discrimination, where \\nappropriate.  \\n(48) High -risk AI systems should be designed and developed in such a way that natural \\npersons can oversee their function ing. For this purpose, appropriate human oversight \\nmeasures should be identified by the provider of the system before its placing on the \\nmarket  or putting into service . In particular, where appropriate, such measures should \\nguarantee that the system is sub ject to in -built operational constraints that cannot be \\noverridden by the system itself and is responsive to the human operator, and that the \\nnatural persons to whom human oversight has been assigned have the necessary \\ncompetence, training and authority to  carry out that role.   \\n(49) High -risk AI systems should perform consistently throughout their lifecycle and meet \\nan appropriate level of accuracy, robustness and cybersecurity in accordance with the \\ngenerally acknowledged state of the art. The level of acc uracy and accuracy metrics \\nshould be communicated to the users.  \\n(50) The technical robustness is a key requirement for high -risk AI systems. They should \\nbe resilient against risks connected to the limitations of the system (e.g. errors, faults, \\ninconsiste ncies, unexpected situations) as well as against malicious actions that may \\ncompromise the security of the AI system and result in harmful or otherwise \\nundesirable behaviour. Failure to protect against these risks could lead to safety \\nimpacts or negatively  affect the fundamental rights, for example due to erroneous \\ndecisions or wrong or biased outputs generated by the AI system.  \\n(51) Cybersecurity plays a crucial role in ensuring that AI systems are resilient against \\nattempts to alter their use, behaviour, performance or compromise their security \\nproperties by malicious third parties exploiting the system’s vulnerabilities. \\nCyberattacks against AI systems can leverage AI specific assets, such as training data \\nsets (e.g. data poisoning) or trained models (e.g . adversarial attacks), or exploit \\nvulnerabilities in the AI system’s digital assets or the underlying ICT infrastructure. \\nTo ensure a level of cybersecurity appropriate to the risks, suitable measures should \\ntherefore be taken by the providers of high -risk AI systems, also taking into account as \\nappropriate the underlying ICT infrastructure.  EN 31  EN (52) As part of Union harmonisation legislation, rules applicable to the placing on the \\nmarket, putting into service and use of high -risk AI systems should be laid dow n \\nconsistently with Regulation (EC) No 765/2008 of the European Parliament and of the \\nCouncil51 setting out the requirements for accreditation and the market surveillance of \\nproducts, Decision No 768/2008/EC of the European Parliament and of the Council52 \\non a common framework for the marketing of products and Regulation (EU) \\n2019/1020 of the European Parliament and of the Council53 on market surveillance \\nand compliance of products  (‘New Legislative Framework for the marketing of \\nproducts’).  \\n(53) It is appropr iate that a specific natural or legal person, defined as the provider, takes \\nthe responsibility for the placing on the market or putting into service of a high -risk AI \\nsystem, regardless of whether that natural or legal person is the person who designed \\nor developed the system.  \\n(54) The provider should establish a sound quality management system, ensure the \\naccomplishment of the required conformity assessment procedure, draw up the \\nrelevant documentation and establish a robust post -market monitoring system. Public \\nauthorities which pu t into service high -risk AI systems for their own use may adopt \\nand implement the rules for the quality management system as part of the quality \\nmanagement system adopted at a national or regional level, as appropriate, taking into \\naccount the specificitie s of the sector and the competences and organisation of the \\npublic authority in question.  \\n(55) Where a high -risk AI system that is a safety component of a product which is covered \\nby a relevant New Legislative Framework sectorial legislation is not placed  on the \\nmarket or put into service independently from the product, the manufacturer of the \\nfinal product as defined under the relevant New Legislative Framework legislation \\nshould comply with the obligations of the provider established in this Regulation a nd \\nnotably ensure that the AI system embedded in the final product complies with the \\nrequirements of this Regulation.  \\n(56) To enable enforcement of this Regulation and create a level -playing field for \\noperators, and taking into account the different forms of making available of digital \\nproducts, it is important to ensure that, under all circumstances, a person established in \\nthe Union can provide authorities with all the necessary information on the compliance \\nof an AI system. Therefore, prior to making the ir AI systems available in the Union, \\nwhere an importer cannot be identified, providers established outside the Union shall, \\nby written mandate, appoint an authorised representative established in the Union.  \\n(57) In line with New Legislative Framework pri nciples, specific obligations for relevant \\neconomic operators, such as importers and distributors, should be set to ensure legal \\ncertainty and facilitate regulatory compliance by those relevant operators.  \\n                                                 \\n51 Regulation (EC) No 765/2008 of the European Parliament and of the Council of 9 July 2008 setting out \\nthe requirements for accreditation and market surveillance relating to the marketing of products and \\nrepealing Reg ulation (EEC) No 339/93 (OJ L 218, 13.8.2008, p. 30).  \\n52 Decision No 768/2008/EC of the European Parliament and of the Council of 9 July 2008 on a common \\nframework for the marketing of products, and repealing Council Decision 93/465/EEC (OJ L 218, \\n13.8.2008 , p. 82).  \\n53 Regulation (EU) 2019/1020 of the European Parliament and of the Council of 20 June 2019 on market \\nsurveillance and compliance of products and amending Directive 2004/42/EC and Regulations (EC) No \\n765/2008 and (EU) No 305/2011 (Text with EEA rel evance) (OJ L 169, 25.6.2019, p. 1 –44). EN 32  EN (58) Given the nature of AI systems and the risks t o safety and fundamental rights possibly \\nassociated with their use, including as regard the need to ensure proper monitoring of \\nthe performance of an AI system in a real -life setting, it is appropriate to set specific \\nresponsibilities for users. Users shou ld in particular use high -risk AI systems in \\naccordance with the instructions of use and certain other obligations should be \\nprovided for with regard to monitoring of the functioning of the AI systems and with \\nregard to record -keeping, as appropriate.   \\n(59) It is appropriate to envisage that the user of the AI system should be the natural or \\nlegal person, public authority, agency or other body under whose authority the AI \\nsystem is operated except where the use is made in the course of a personal non -\\nprofes sional activity.  \\n(60) In the light of the complexity of the artificial intelligence value chain, relevant third \\nparties, notably the ones involved in the sale and the supply of software, software tools \\nand components, pre -trained models and data, or provid ers of network services, should \\ncooperate, as appropriate, with providers and users to enable their compliance with the \\nobligations under this Regulation and with competent authorities established under this \\nRegulation.  \\n(61) Standardisation should play a k ey role to provide technical solutions to providers to \\nensure compliance with this Regulation. Compliance with harmonised standards as \\ndefined in Regulation (EU) No 1025/2012 of the European Parliament and of the \\nCouncil54 should be a means for providers to  demonstrate conformity with the \\nrequirements of this Regulation. However, the Commission could adopt common \\ntechnical specifications in areas where no harmonised standards exist or where they \\nare insufficient.   \\n(62) In order to ensure a high level of trus tworthiness of high -risk AI systems, those \\nsystems should be subject to a conformity assessment prior to their placing on the \\nmarket or putting into service.  \\n(63) It is appropriate that, in order to minimise the burden on operators and avoid any \\npossible d uplication, for high -risk AI systems related to products which are covered by \\nexisting Union harmonisation legislation following the New Legislative Framework \\napproach, the compliance of those AI systems with the requirements of this Regulation \\nshould be a ssessed as part of the conformity assessment already foreseen under that \\nlegislation. The applicability of the requirements of this Regulation should thus not \\naffect the specific logic, methodology or general structure of conformity assessment \\nunder the re levant specific New Legislative Framework legislation. This approach is \\nfully reflected in the interplay between this Regulation and the [Machinery \\nRegulation]. While safety risks of AI systems ensuring safety functions in machinery \\nare addressed by the re quirements of this Regulation, certain specific requirements in \\nthe [Machinery Regulation] will ensure the safe integration of the AI system into the \\noverall machinery, so as not to compromise the safety of the machinery as a whole. \\n                                                 \\n54 Regulation (EU) No 1025/2012 of the European Parliament and of the Council of 25 October 2012 on \\nEuropean standardisation, amending Council Directives 89/686/EEC and 93/15/EEC and Directives \\n94/9/EC, 94/25/EC, 95/1 6/EC, 97/23/EC, 98/34/EC, 2004/22/EC, 2007/23/EC, 2009/23/EC and \\n2009/105/EC of the European Parliament and of the Council and repealing Council Decision \\n87/95/EEC and Decision No 1673/2006/EC of the European Parliament and of the Council (OJ L 316, \\n14.11. 2012, p. 12).  EN 33  EN The [Machinery Regulati on] applies the same definition of AI system as this \\nRegulation.  \\n(64) Given the more extensive experience of professional pre -market certifiers in the field \\nof product safety and the different nature of risks involved, it is appropriate to limit, at \\nleast in an initial phase of application of this Regulation, the scope of application of \\nthird -party conformity assessment for high -risk AI systems other than those related to \\nproducts. Therefore, the conformity assessment of such systems should be carried out \\nas a general rule by the provider under its own responsibility, with the only exception \\nof AI systems intended to be used for the remote biometric identification of persons, \\nfor which the involvement of a notified body in the conformity assessment should b e \\nforeseen, to the extent they are not prohibited.  \\n(65) In order to carry out third -party conformity assessment for AI systems intended to be \\nused for the remote biometric identification of persons, notified bodies should be \\ndesignated under this Regulati on by the national competent authorities, provided they \\nare compliant with a set of requirements, notably on independence, competence and \\nabsence of conflicts of interests.  \\n(66) In line with the commonly established notion of substantial modification for products \\nregulated by Union harmonisation legislation, it is appropriate that an AI system \\nundergoes a new conformity assessment whenever a change occurs which may affect \\nthe compliance of the system with this Regulation or when the intended purpose of the  \\nsystem changes. In addition, as regards AI systems which continue to ‘learn’ after \\nbeing placed on the market or put into service (i.e. they automatically adapt how \\nfunctions are carried out), it is necessary to provide rules establishing that changes to \\nthe algorithm and its performance that have been pre -determined by the provider and \\nassessed at the moment of the conformity assessment should not constitute a \\nsubstantial modification.   \\n(67) High -risk AI systems should bear the CE marking to indicate thei r conformity with \\nthis Regulation so that they can move freely within the internal market. Member States \\nshould not create unjustified obstacles to the placing on the market or putting into \\nservice of high -risk AI systems that comply with the requirements laid down in this \\nRegulation and bear the CE marking.  \\n(68) Under certain conditions, rapid availability of innovative technologies may be crucial \\nfor health and safety of persons and for society as a whole. It is thus appropriate that \\nunder exceptional rea sons of public security or protection of life and health of natural \\npersons and the protection of industrial and commercial property, Member States \\ncould authorise the placing on the market or putting into service of AI systems which \\nhave not undergone a c onformity assessment.  \\n(69) In order to facilitate the work of the Commission and the Member States in the \\nartificial intelligence field as well as to increase the transparency towards the public, \\nproviders of high -risk AI systems other than those related t o products falling within \\nthe scope of relevant existing Union harmonisation legislation, should be required to \\nregister their high -risk AI system in a EU database , to be established and managed by \\nthe Commission. The Commission should be the controller of  that database, in \\naccordance with Regulation (EU) 2018/1725 of the European Parliament and of the EN 34  EN Council55. In order to ensure the full functionality of the database, when deployed, the \\nprocedure for setting the database should include the elaboration of functional \\nspecifications by the Commission and an independent audit report.   \\n(70) Certain AI systems intended to interact with natural persons or to generate content \\nmay pose specific risks of impersonation or deception irrespective of whether they \\nqualif y as high -risk or not. In certain circumstances, the use of these systems should \\ntherefore be subject to specific transparency obligations without prejudice to the \\nrequirements and obligations for high -risk AI systems. In particular, natural persons \\nshould  be notified that they are interacting with an AI system, unless this is obvious \\nfrom the circumstances and the context of use. Moreover, natural persons should be \\nnotified when they are exposed to an emotion recognition system or a biometric \\ncategorisatio n system. Such information and notifications should be provided in \\naccessible formats for persons with disabilities. Further, users, who use an AI system \\nto generate or manipulate image, audio or video content that appreciably resembles \\nexisting persons, p laces or events and would falsely appear to a person to be authentic, \\nshould disclose that the content has been artificially created or manipulated by \\nlabelling the artificial intelligence output accordingly and disclosing its artificial \\norigin.  \\n(71) Artificial intelligence is a rapidly developing family of technologies that requires \\nnovel forms of regulatory oversight and a safe space for experimentation, while \\nensuring responsible innovation and integration of appropriate safeguards and risk \\nmitigati on measures. To ensure a legal framework that is innovation -friendly, future -\\nproof and resilient to disruption, national competent authorities from one or more \\nMember States should be encouraged to establish artificial intelligence regulatory \\nsandboxes to facilitate the development and testing of innovative AI systems under \\nstrict regulatory oversight before these systems are placed on the market or otherwise \\nput into service.  \\n(72) The objectives of the regulatory sandboxes should be to foster AI innovatio n by \\nestablishing a controlled experimentation and testing environment in the development \\nand pre -marketing phase with a view to ensuring compliance of the innovative AI \\nsystems  with this Regulation and other relevant Union and Member States legislation; \\nto enhance legal certainty for innovators and the competent authorities’ oversight and \\nunderstanding of the opportunities, emerging risks and the impacts of AI use, and to \\naccelerate access to markets, including by removing barriers for small and medium \\nenterprises (SMEs) and start -ups. To ensure uniform implementation across the Union \\nand economies of scale, it is appropriate to establish common rules for the regulatory \\nsandboxes’ implementation and a framework for cooperation between the relevant \\nauthorit ies involved in the supervision of the sandboxes. This Regulation should \\nprovide the legal basis for the use of personal data collected for other purposes for \\ndeveloping certain AI systems in the public interest within the AI regulatory sandbox, \\nin line wi th Article 6(4) of Regulation (EU) 2016/679, and Article 6 of Regulation \\n(EU) 2018/1725, and without prejudice to Article 4(2) of Directive (EU) 2016/680. \\nParticipants in the sandbox should ensure appropriate safeguards and cooperate with \\nthe competent aut horities, including by following their guidance and acting \\n                                                 \\n55 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the \\nprotection of natural persons with regard to the processing of personal data and on the free movement of \\nsuch data, and repealing Directive 95/46 /EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, \\np. 1).  EN 35  EN expeditiously and in good faith to mitigate any high -risks to safety and fundamental \\nrights that may arise during the development and experimentation in the sandbox. The \\nconduct of the participants  in the sandbox should be taken into account when \\ncompetent authorities decide whether to impose an administrative fine under Article \\n83(2) of Regulation 2016/679 and Article 57 of Directive 2016/680.  \\n(73) In order to promote and protect innovation, it is  important that the interests of small -\\nscale providers and users of AI systems are taken into particular account. To this \\nobjective, Member States should develop initiatives, which are targeted at those \\noperators, including on awareness raising and informa tion communication. Moreover, \\nthe specific interests and needs of small -scale providers shall be taken into account \\nwhen Notified Bodies set conformity assessment fees.  Translation costs related to \\nmandatory documentation and communication with authorities  may constitute a \\nsignificant cost for providers and other operators, notably those of a smaller scale. \\nMember States should possibly ensure that one of the languages determined and \\naccepted by them for relevant providers’ documentation and for communicati on with \\noperators is one which is broadly understood by the largest possible number of cross -\\nborder users.  \\n(74) In order to minimise the risks to implementation resulting from lack of knowledge and \\nexpertise in the market as well as to facilitate complian ce of providers and notified \\nbodies with their obligations under this Regulation, the AI -on demand platform, the \\nEuropean Digital Innovation Hubs and the Testing and Experimentation Facilities \\nestablished by the Commission and the Member States at national  or EU level should \\npossibly contribute to the implementation of this Regulation. Within their respective \\nmission and fields of competence, they may provide in particular technical and \\nscientific support to providers and notified bodies.  \\n(75) It is approp riate that the Commission facilitates, to the extent possible, access to \\nTesting and Experimentation Facilities to bodies, groups or laboratories established or \\naccredited pursuant to any relevant Union harmonisation legislation and which fulfil \\ntasks in t he context of conformity assessment of products or devices covered by that \\nUnion harmonisation legislation. This is notably the case for expert panels, expert \\nlaboratories and reference laboratories in the field of medical devices pursuant to \\nRegulation (E U) 2017/745 and Regulation (EU) 2017/746.  \\n(76) In order to facilitate a smooth, effective and harmonised implementation of this \\nRegulation a European Artificial Intelligence Board should be established. The Board \\nshould be responsible for a number of advi sory tasks, including issuing opinions, \\nrecommendations, advice or guidance on matters related to the implementation of this \\nRegulation, including on technical specifications or existing standards regarding the \\nrequirements established in this Regulation a nd providing advice to and assisting the \\nCommission on specific questions related to artificial intelligence.  \\n(77) Member States hold a key role in the application and enforcement of this Regulation. \\nIn this respect, each Member State should designate one  or more national competent \\nauthorities for the purpose of supervising the application and implementation of this \\nRegulation. In order to increase organisation efficiency on the side of Member States \\nand to set an official point of contact vis -à-vis the pu blic and other counterparts at \\nMember State and Union levels, in each Member State one national authority should \\nbe designated as national supervisory authority.  \\n(78) In order to ensure that providers of high -risk AI systems can take into account the \\nexper ience on the use of high -risk AI systems for improving their systems and the EN 36  EN design and development process or can take any possible corrective action in a timely \\nmanner, all providers should have a post -market monitoring system in place. This \\nsystem is al so key to ensure that the possible risks emerging from AI systems which \\ncontinue to ‘learn’ after being placed on the market or put into service can be more \\nefficiently and timely addressed. In this context, providers should also be required to \\nhave a syst em in place to report to the relevant authorities any serious incidents or any \\nbreaches to national and Union law protecting fundamental rights resulting from the \\nuse of their AI systems.   \\n(79) In order to ensure an appropriate and effective enforcement of  the requirements and \\nobligations set out by this Regulation, which is Union harmonisation legislation, the \\nsystem of market surveillance and compliance of products established by Regulation \\n(EU) 2019/1020 should apply in its entirety. Where necessary for their mandate, \\nnational public authorities or bodies, which supervise the application of Union law \\nprotecting fundamental rights, including equality bodies, should also have access to \\nany documentation created under this Regulation.  \\n(80) Union legislation on financial services includes internal governance and risk \\nmanagement rules and requirements which are applicable to regulated financial \\ninstitutions in the course of provision of those services, including when they make use \\nof AI systems. In order to ens ure coherent application and enforcement of the \\nobligations under this Regulation and relevant rules and requirements of the Union \\nfinancial services legislation, the authorities responsible for the supervision and \\nenforcement of the financial services leg islation,  including where applicable the \\nEuropean Central Bank , should be designated as competent authorities for the purpose \\nof supervising the implementation of this Regulation, including for market \\nsurveillance activities, as regards AI systems provided  or used by regulated and \\nsupervised financial institutions. To further enhance the consistency between this \\nRegulation and the rules applicable to credit institutions regulated under Directive \\n2013/36/EU of the European Parliament and of the Council56, it is also appropriate to \\nintegrate the conformity assessment procedure and some of the providers’ procedural \\nobligations in relation to risk management, post marketing monitoring and \\ndocumentation into the existing obligations and procedures under Directive \\n2013/36/EU. In order to avoid overlaps, limited derogations should also be envisaged \\nin relation to the quality management system of providers and the monitoring \\nobligation placed on users of high -risk AI systems to the extent that these apply to \\ncredit in stitutions regulated by Directive 2013/36/EU.  \\n(81) The development of AI systems other than high -risk AI systems in accordance with \\nthe requirements of this Regulation may lead to a larger uptake of trustworthy artificial \\nintelligence in the Union. Provide rs of non -high-risk AI systems should be encouraged \\nto create codes of conduct intended to foster the voluntary application of the \\nmandatory requirements applicable to high -risk AI systems. Providers should also be \\nencouraged to apply on a voluntary basis additional requirements related, for example, \\nto environmental sustainability, accessibility to persons with disability, stakeholders’ \\nparticipation in the design and development of AI systems, and diversity of the \\ndevelopment teams. The Commission may dev elop initiatives, including of a sectorial \\n                                                 \\n56 Directive 2013/36/EU of the European Parliament and of the Council of 26 June 2013 on access to the \\nactivity of credit institutions and the prudential supervision of credit institutions  and investment firms, \\namending Directive 2002/87/EC and repealing Directives 2006/48/EC and 2006/49/EC (OJ L 176, \\n27.6.2013, p. 338).  EN 37  EN nature, to facilitate the lowering of technical barriers hindering cross -border exchange \\nof data for AI development, including on data access infrastructure, semantic and \\ntechnical interoperability of different ty pes of data.  \\n(82) It is important that AI systems related to products that are not high -risk in accordance \\nwith this Regulation and thus are not required to comply with the requirements set out \\nherein are nevertheless safe when placed on the market or put into service. To \\ncontribute to this objective,  the Directive 2001/95/EC of the European Parliament and \\nof the Council57  would apply as a safety net.  \\n(83) In order to ensure trustful and constructive cooperation of competent authorities on \\nUnion and nationa l level, all parties involved in the application of this Regulation \\nshould respect the confidentiality of information and data obtained in carrying out \\ntheir tasks.  \\n(84) Member States should take all necessary measures to ensure that the provisions of thi s \\nRegulation are implemented, including by laying down effective, proportionate and \\ndissuasive penalties for their infringement. For certain specific infringements, Member \\nStates should take into account the margins and criteria set out in this Regulation.  The \\nEuropean Data Protection Supervisor should have the power to impose fines on Union \\ninstitutions, agencies and bodies falling within the scope of this Regulation.  \\n(85) In order to ensure that the regulatory framework can be adapted where necessary, the  \\npower to adopt acts in accordance with Article 290 TFEU should be delegated to the \\nCommission to amend the techniques and approaches referred to in Annex I to define \\nAI systems, the Union harmonisation legislation listed in Annex II, the high -risk AI \\nsystems listed in Annex III, the provisions regarding technical documentation listed in \\nAnnex IV, the content of the EU declaration of conformity in Annex V, the provisions \\nregarding the conformity assessment procedures in Annex VI and VII and the \\nprovisions e stablishing the high -risk AI systems to which the conformity assessment \\nprocedure based on assessment of the quality management system and assessment of \\nthe technical documentation should apply. It is of particular importance that the \\nCommission carry out appropriate consultations during its preparatory work, including \\nat expert level, and that those consultations be conducted in accordance with the \\nprinciples laid down in the Interinstitutional Agreement of 13 April 2016 on Better \\nLaw-Making58. In particula r, to ensure equal participation in the preparation of \\ndelegated acts, the European Parliament and the Council receive all documents at the \\nsame time as Member States’ experts, and their experts systematically have access to \\nmeetings of Commission expert g roups dealing with the preparation of delegated acts.  \\n(86) In order to ensure uniform conditions for the implementation of this Regulation, \\nimplementing powers should be conferred on the Commission. Those powers should \\nbe exercised in accordance with Regul ation (EU) No 182/2011 of the European \\nParliament and of the Council59. \\n(87) Since the objective of this Regulation cannot be sufficiently achieved by the Member \\nStates and can rather, by reason of the scale or effects of the action, be better achieved \\n                                                 \\n57  Directive 2001/95/EC of the European Parliament and of the Council of 3 December 2001 on general \\nproduct safety (OJ L 11, 15.1.2002, p. 4).  \\n58 OJ L 123, 12.5.2016, p. 1.  \\n59 Regulation (EU) No 182/2011 of the European Parliament and of the Council of 16 February 2011 \\nlaying down the rules and general principles concerning mechanisms for control by the Member States \\nof the Commission\\'s exercise of implementing powers (OJ L 55, 28.2.2011, p.13).  EN 38  EN at Union level, the Union may adopt measures in accordance with the principle of \\nsubsidiarity as set out in Article 5 TEU. In accordance with the principle of \\nproportionality as set out in that Article, this Regulation does not go beyond what is \\nnecessary in o rder to achieve that objective.  \\n(88) This Regulation should apply from … [ OP – please insert the date established in Art. \\n85]. However, the infrastructure related to the governance and the conformity \\nassessment system should be operational before that date , therefore the provisions on \\nnotified bodies and governance structure should apply from … [OP – please insert the \\ndate – three months following the entry into force of this Regulation ]. In addition, \\nMember States should lay down and notify to the Commissi on the rules on penalties, \\nincluding administrative fines, and ensure that they are properly and effectively \\nimplemented by the date of application of this Regulation. Therefore the provisions on \\npenalties should apply from [ OP – please insert the date – twelve months following the \\nentry into force of this Regulation ]. \\n(89) The European Data Protection Supervisor and the European Data Protection Board \\nwere consulted in accordance with Article 42(2) of Regulation (EU) 2018/1725 and \\ndelivered an opinion on [ …]”.  \\nHAVE ADOPTED THIS REGULATION:  \\nTITLE  I \\nGENERAL  PROVISIONS  \\nArticle 1  \\nSubject matter  \\nThis Regulation lays down:  \\n(a) harmonised rules for the placing on the market, the putting into service and the \\nuse of artificial intelligence systems (‘AI systems’) in the Union;  \\n(a) prohibitions of certain artificial intelligence practices;  \\n(b) specific requirements for high -risk AI systems and obligations for operators of \\nsuch systems;  \\n(c) harmonised transparency rules for AI systems intended to interact with natural \\npersons, emotion recognition systems and biometric categorisation systems, \\nand AI systems used to generate or manipulate image, audio or video content;  \\n(d) rules on market monitoring and surveillance.  \\nArticle 2  \\nScope  \\n1. This Regulation applies to:  \\n(a) providers placing on the market or putting into service AI systems in the \\nUnion, irrespective of whether those providers are established within the Union \\nor in a third country;  \\n(b) users of AI systems located within the Union;  EN 39  EN (c) providers and users of AI  systems that are located in a third country, where the \\noutput produced by the system is used in the Union;  \\n2. For high -risk AI systems that are safety components of products or systems, or which \\nare themselves products or systems, falling within the scope  of the following acts, \\nonly Article 84 of this Regulation shall apply:  \\n(a) Regulation (EC) 300/2008;  \\n(b) Regulation  (EU) No 167/2013;  \\n(c) Regulation (EU) No 168/2013;  \\n(d) Directive 2014/90/EU;  \\n(e) Directive (EU) 2016/797;  \\n(f) Regulation (EU) 2018/858;  \\n(g) Regulation (EU) 201 8/1139;  \\n(h) Regulation (EU) 2019/2144.  \\n3. This Regulation shall not apply to AI systems developed or used exclusively for \\nmilitary purposes.  \\n4. This Regulation shall not apply to public authorities in a third country nor to \\ninternati onal organisations falling within the scope of this Regulation pursuant to \\nparagraph 1, where those authorities or organisations use AI systems in the \\nframework of international agreements for law enforcement and judicial cooperation \\nwith the Union or with  one or more Member States.  \\n5. This Regulation shall not affect the application of the provisions on the liability of \\nintermediary service providers set out in Chapter II, Section IV of Directive \\n2000/31/EC of the European Parliament and of the Council60 [as to be replaced by  \\nthe corresponding provisions of the Digital Services Act ]. \\nArticle 3  \\nDefinitions  \\nFor the purpose of this Regulation, the following definitions apply:  \\n(1) ‘artificial intelligence system’ (AI system) means software that is developed with  one \\nor more of the techniques and approaches listed in Annex I and can, for a given set of \\nhuman -defined objectives, generate outputs such as content, predictions, \\nrecommendations, or decisions influencing the environments they interact with;  \\n(1) ‘provider’ m eans a natural or legal person, public authority, agency or other body \\nthat develops an AI system or that has an AI system developed with a view to \\nplacing it on the market or putting it into service under its own name or trademark, \\nwhether for payment or free of charge;  \\n                                                 \\n60 Directive 2000/31/EC of the European Parliament and of the Council of 8 June 2000 on certain legal \\naspects of information society services, in particular electronic commerce, in the  Internal Market \\n(\\'Directive on electronic commerce\\') (OJ L 178, 17.7.2000, p. 1).  EN 40  EN (3) ‘small -scale provider’ means a provider that is a micro or small enterprise within the \\nmeaning of Commission Recommendation 2003/361/EC61; \\n(4) ‘user’ means any natural or legal person, public authority, agency or other body \\nusing an AI s ystem under its authority, except where the AI system is used in the \\ncourse of a personal non -professional activity;  \\n(5) ‘authorised representative’ means any natural or legal person established in the \\nUnion who has received a written mandate from a provid er of an AI system to, \\nrespectively, perform and carry out on its behalf the obligations and procedures \\nestablished by this Regulation;  \\n(6) ‘importer’ means any natural or legal person established in the Union that places on \\nthe market or puts into service  an AI system that bears the name or trademark of a \\nnatural or legal person established outside the Union;  \\n(7) ‘distributor’ means any natural or legal person in the supply chain, other than the \\nprovider or the importer, that makes an AI system available o n the Union market \\nwithout affecting its properties;  \\n(8) ‘operator’ means the provider, the user, the authorised representative, the importer \\nand the distributor;  \\n(9) ‘placing on the market’ means the first making available of an AI system on the \\nUnion market;  \\n(10) ‘making available on the market’ means any supply of an AI system for distribution \\nor use on the Union market in the course of a commercial activity, whether in return \\nfor payment or free of charge;  \\n(11) ‘putting into service’ means the supply  of an AI system for first use directly to the \\nuser or for own use on the Union market for its intended purpose;  \\n(12) ‘intended purpose’ means the use for which an AI system is intended by the provider, \\nincluding the specific context and conditions of use,  as specified in the information \\nsupplied by the provider in the instructions for use, promotional or sales materials \\nand statements, as well as in the technical documentation;  \\n(13) ‘reasonably foreseeable misuse’ means the use of an AI system in a way tha t is not in \\naccordance with its intended purpose, but which may result from reasonably \\nforeseeable human behaviour or interaction with other systems;  \\n(14) ‘safety component of a product or system’ means a component of a product or of a \\nsystem which fulfils  a safety function for that product or system or the failure or \\nmalfunctioning of which endangers the health and safety of persons or property;  \\n(15) ‘instructions for use’ means the information provided by the provider to inform the \\nuser of in particular a n AI system’s intended purpose and proper use, inclusive of the \\nspecific geographical, behavioural or functional setting within which the high -risk AI \\nsystem is intended to be used;  \\n(16) ‘recall of an AI system’ means any measure aimed at achieving the ret urn to the \\nprovider of an AI system made available to users;  \\n                                                 \\n61 Commission Recommendation of 6 May 2003 concerning the definition of micro, small and medium -\\nsized enterprises (OJ L 124, 20.5.2003, p. 36).  EN 41  EN (17) ‘withdrawal of an AI system’ means any measure aimed at preventing the \\ndistribution, display and offer of an AI system;  \\n(18) ‘performance of an AI system’ means the ability of an AI system t o achieve its \\nintended purpose;  \\n(19) ‘notifying authority’ means the national authority responsible for setting up and \\ncarrying out the necessary procedures for the assessment, designation and \\nnotification of conformity assessment bodies and for their monitoring;  \\n(20) ‘conformity assessment’ means the process of verifying whether the requirements set \\nout in Title III, Chapter 2 of this Regulation relating to an AI system have been \\nfulfilled;  \\n(21) ‘conformity assessment body’ means a body that performs t hird-party conformity \\nassessment activities, including testing, certification and inspection;  \\n(22) ‘notified body’ means a conformity assessment body designated in accordance with \\nthis Regulation and other relevant Union harmonisation legislation;  \\n(23) ‘substantial modification’ means a change to the AI system following its placing on \\nthe market or putting into service which affects the compliance of the AI system with \\nthe requirements set out in Title III, Chapter 2 of this Regulation or results in a \\nmodif ication to the intended purpose for which the AI system has been assessed;  \\n(24) ‘CE marking of conformity’ (CE marking) means a marking by which a provider \\nindicates that an AI system is in conformity with the requirements set out in Title III, \\nChapter 2 o f this Regulation and other applicable Union legislation harmonising the \\nconditions for the marketing of products (‘Union harmonisation legislation’) \\nproviding for its affixing;  \\n(25) ‘post -market monitoring’ means all activities carried out by providers of  AI systems \\nto proactively collect and review experience gained from the use of AI systems they \\nplace on the market or put into service for the purpose of identifying any need to \\nimmediately apply any necessary corrective or preventive actions;  \\n(26) ‘marke t surveillance authority’ means the national authority carrying out the \\nactivities and taking the measures pursuant to Regulation (EU) 2019/1020;  \\n(27) ‘harmonised standard’ means a European standard as defined in Article 2(1)(c) of \\nRegulation (EU) No 1025/ 2012;  \\n(28) ‘common specifications’ means a document, other than a standard, containing \\ntechnical solutions providing a means to, comply with certain requirements and \\nobligations established under this Regulation;  \\n(29) ‘training data’ means data used for tr aining an AI system through fitting its learnable \\nparameters, including the weights of a neural network;  \\n(30) ‘validation data’ means data used for providing an evaluation of the trained AI \\nsystem and for tuning its non -learnable parameters and its learnin g process, among \\nother things, in order to prevent overfitting; whereas the validation dataset can be a \\nseparate dataset or part of the training dataset, either as a fixed or variable split;  \\n(31) ‘testing data’ means data used for providing an independent evaluation of the trained \\nand validated AI system in order to confirm the expected performance of that system \\nbefore its placing on the market or putting into service;  EN 42  EN (32) ‘input data’ means data provided to or directly acquired by an AI system on the ba sis \\nof which the system produces an output;  \\n(33) ‘biometric data’ means personal data resulting from specific technical processing \\nrelating to the physical, physiological or behavioural characteristics of a natural \\nperson, which allow or confirm the unique  identification of that natural person, such \\nas facial images or dactyloscopic data;  \\n(34) ‘emotion recognition system’ means an AI system for the purpose of identifying or \\ninferring emotions or intentions of natural persons on the basis of their biometric \\ndata;  \\n(35) ‘biometric categorisation system’ means an AI system for the purpose of assigning \\nnatural persons to specific categories, such as sex, age, hair colour, eye colour, \\ntattoos, ethnic origin or sexual or political orientation, on the basis of their  biometric \\ndata;  \\n(36) ‘remote biometric identification system’ means  an AI system  for the purpose of \\nidentifying natural persons at a distance through the comparison of a person’s \\nbiometric data with the biometric data contained in a reference database, an d without \\nprior knowledge of the user of the AI system whether the person will be present and \\ncan be identified ;  \\n(37) ‘‘real -time’ remote biometric identification system’ means a remote biometric \\nidentification system whereby the capturing of biometric da ta, the comparison and \\nthe identification all occur without a significant delay. This comprises not only \\ninstant identification, but also limited short delays in order to avoid circumvention.  \\n(38) ‘‘post’ remote biometric identification system’ means a remote biometric \\nidentification system other than a ‘real -time’ remote biometric identification system;  \\n(39) ‘publicly accessible space’ means any physical place accessible to the public, \\nregardless of whether certain conditions for access may apply;  \\n(40) ‘law enforcement authority’ means:   \\n(a) any public authority competent for the prevention, investigation, detection or \\nprosecution of criminal offences or the execution of criminal penalties, \\nincluding the safeguarding against and the prevention of threat s to public \\nsecurity; or  \\n(b) any other body or entity entrusted by Member State law to exercise public \\nauthority and public powers for the purposes of the prevention, investigation, \\ndetection or prosecution of criminal offences or the execution of criminal  \\npenalties, including the safeguarding against and the prevention of threats to \\npublic security;  \\n(41) ‘law enforcement’ means activities carried out by law enforcement authorities for the \\nprevention, investigation, detection or prosecution of criminal offences or the \\nexecution of criminal penalties, including the safeguarding against and the \\nprevention of threats to public security;  \\n(42) ‘national supervisory authority’ means the authority to which a Member State assigns \\nthe responsibility for the imple mentation and application of this Regulation, for \\ncoordinating the activities entrusted to that Member State, for acting as the single \\ncontact point for the Commission, and for representing the Member State at the \\nEuropean Artificial Intelligence Board;  EN 43  EN (43) ‘national competent authority’ means the national supervisory authority, the \\nnotifying authority and the market surveillance authority;  \\n(44) ‘serious incident’ means any incident that directly or indirectly leads, might have led \\nor might lead to any of  the following:  \\n(a) the death of a person or serious damage to a person’s health,  to property or the \\nenvironment,  \\n(b) a serious and irreversible disruption of the management and operation of \\ncritical infrastructure.  \\nArticle 4  \\nAmendments to Annex I  \\nThe Com mission is empowered to adopt delegated acts in accordance with Article 73 to \\namend the list of techniques and approaches listed in Annex I, in order to update that list to \\nmarket and technological developments on the basis of characteristics that are simi lar to the \\ntechniques and approaches listed therein.  \\nTITLE  II \\nPROHIBITED  ARTIFICIAL  INTELLIGENCE  PRACTICES  \\nArticle 5  \\n1. The following artificial intelligence practices shall be prohibited:  \\n(a) the placing on the market, putting into service or use of an A I system that \\ndeploys subliminal techniques beyond a person’s consciousness in order to \\nmaterially distort a person’s behaviour in a manner that causes or is likely to \\ncause that person or another person physical or psychological harm;  \\n(b) the placing on t he market, putting into service or use of an AI system that \\nexploits any of the vulnerabilities of a specific group of persons due to their \\nage, physical or mental disability, in order to materially distort the behaviour of \\na person pertaining to that grou p in a manner that causes or is likely to cause \\nthat person or another person physical or psychological harm;  \\n(c) the placing on the market, putting into service or use of AI systems by public \\nauthorities or on their behalf for the evaluation or classifica tion of the \\ntrustworthiness of natural persons over a certain period of time based on their \\nsocial behaviour or known or predicted personal or personality characteristics, \\nwith the social score leading to either or both of the following:  \\n(i) detrimental or  unfavourable treatment of certain natural persons or whole \\ngroups thereof in social contexts which are unrelated to the contexts in \\nwhich the data was originally generated or collected;  \\n(ii) detrimental or unfavourable treatment of certain natural person s or whole \\ngroups thereof that is unjustified or disproportionate to their social \\nbehaviour or its gravity;  \\n(d) the use of  ‘real-time’ remote biometric identification systems in publicly \\naccessible spaces for the purpose of law enforcement, unless and in a s far as \\nsuch use is strictly necessary for one of the following objectives : EN 44  EN (i) the targeted search for specific potential victims of crime, including \\nmissing children;  \\n(ii) the prevention of a specific, substantial and imminent  threat to the life or \\nphysical safety of natural persons or of a terrorist attack;  \\n(iii) the detection, localisation, identification or prosecution of a perpetrator \\nor suspect of a criminal offence referred to  in Article 2(2) of Council \\nFramework Decision 2002/584/JHA62 and punish able in the Member \\nState concerned by a custodial sentence or a detention order for a \\nmaximum period of at least three years, as determined by the law of that \\nMember State.  \\n2. The use of ‘real -time’ remote biometric identification systems in publicly acces sible \\nspaces for the purpose of law enforcement for any of the objectives referred to in \\nparagraph 1 point d) shall take into account the following elements:  \\n(a) the nature of the situation giving rise to the possible use, in particular the \\nseriousness, pr obability and scale of the harm caused in the absence of the use \\nof the system;  \\n(b) the consequences of the use of the system for the rights and freedoms of all \\npersons concerned, in particular the seriousness, probability and scale of those \\nconsequences.  \\nIn addition, the use of ‘real -time’ remote biometric identification systems in publicly \\naccessible spaces for the purpose of law enforcement for any of the objectives \\nreferred to in paragraph 1 point d) shall comply with necessary and proportionate \\nsafegu ards and conditions in relation to the use, in particular as regards the temporal, \\ngeographic and personal limitations.  \\n3. As regards paragraphs 1, point (d) and 2, each individual use for the purpose of law \\nenforcement of a ‘real-time’ remote biometric identification system in publicly \\naccessible spaces shall be subject to a prior authorisation granted by a judicial \\nauthority or by an independent administrative authority of the Member State in \\nwhich the use is to take place,  issued upon a reasoned request and in accordance with \\nthe detailed rules of national law referred to in paragraph 4. However, in a duly \\njustified situation of urgency, the use of the system may be commenced without an \\nauthorisation and the authorisation m ay be requested only during or after the use.   \\nThe competent judicial or administrative authority shall only grant the authorisation \\nwhere it is satisfied, based on objective evidence or clear indications presented to it, \\nthat the use of the ‘real -time’ r emote biometric identification system at issue is \\nnecessary for and proportionate to achieving one of the objectives specified in \\nparagraph 1, point (d), as identified in the request. In deciding on the request, the \\ncompetent judicial or administrative aut hority shall  take into account the elements \\nreferred to in paragraph 2.  \\n4. A Member State may decide to provide for the possibility to fully or partially \\nauthorise the use of ‘real -time’ remote biometric identification systems in publicly \\naccessible space s for the purpose of law enforcement within the limits and under the \\n                                                 \\n62 Council Framework Decision 2002/584/JHA  of 13 June 2002 on the European arrest warrant and the \\nsurrender procedures between Member States ( OJ L 190, 18.7.2002, p. 1) . \\n EN 45  EN conditions listed in paragraphs 1, point (d), 2 and 3. That Member State shall lay \\ndown in its national law the necessary detailed rules for the request, issuance and \\nexercise of, as well  as supervision relating to, the authorisations referred to in \\nparagraph 3. Those rules shall also specify in respect of which of the objectives listed \\nin paragraph 1, point (d), including which of the criminal offences referred to in \\npoint (iii) thereof, the competent authorities may be authorised to use those systems \\nfor the purpose of law enforcement.  \\nTITLE  III \\nHIGH -RISK  AI SYSTEMS  \\nCHAPTER 1 \\nCLASSIFICATION  OF AI SYSTEMS  AS HIGH -RISK  \\nArticle 6  \\nClassification rules for high -risk AI systems  \\n1. Irrespective of whether an AI system is placed on the market or put into service \\nindependently from the products referred to in points (a) and (b), that AI system shall \\nbe considered high -risk where both of the following conditions are fulfilled:  \\n(a) the AI system is i ntended to be used as a safety component of a product, or is \\nitself a product, covered by the Union harmonisation legislation listed in Annex \\nII;  \\n(b) the product  whose safety component is the AI system, or the AI system itself as \\na product, is required to  undergo a third -party conformity assessment with a \\nview to the placing on the market or putting into service of that product \\npursuant to the Union harmonisation legislation listed in Annex II.  \\n2. In addition to the high -risk AI systems referred to in para graph 1, AI systems \\nreferred to in Annex III shall also be considered high -risk. \\nArticle 7  \\nAmendments to Annex III  \\n1. The Commission is empowered to adopt delegated acts in accordance with Article 73 \\nto update the list in Annex III by adding high -risk AI s ystems where both of the \\nfollowing conditions are fulfilled:  \\n(a) the AI systems are intended to be used in any of the areas listed in points 1 to 8 \\nof Annex III;  \\n(b) the AI systems pose a risk of harm to the health and safety, or a risk of adverse \\nimpact o n fundamental rights, that is, in respect of its severity and probability \\nof occurrence, equivalent to or greater than the risk of harm or of adverse \\nimpact posed by the high -risk AI systems already referred to in Annex III.  \\n2. When assessing for the purpo ses of paragraph 1 whether an AI system poses a risk of \\nharm to the health and safety or a risk of adverse impact on fundamental rights that is \\nequivalent to or greater than the risk of harm posed by the high -risk AI systems EN 46  EN already referred to in Annex II I, the Commission shall take into account the \\nfollowing criteria:  \\n(a) the intended purpose of the AI system;  \\n(b) the extent to which an AI system has been used or is likely to be used;  \\n(c) the extent to which the use of an AI system has already caused harm  to the \\nhealth and safety or adverse impact on the fundamental rights or has given rise \\nto significant concerns in relation to the materialisation of such harm or \\nadverse impact, as demonstrated by reports or documented allegations \\nsubmitted to national co mpetent authorities;  \\n(d) the potential extent of such harm or such adverse impact, in particular in terms \\nof its intensity and its ability to affect a plurality of persons;  \\n(e) the extent to which potentially harmed or adversely impacted persons are \\ndepend ent on the outcome produced with an AI system, in particular because \\nfor practical or legal reasons it is not reasonably possible to opt -out from that \\noutcome;  \\n(f) the extent to which potentially harmed or adversely impacted persons are in a \\nvulnerable pos ition in relation to the user of an AI system, in particular due to \\nan imbalance of power, knowledge, economic or social circumstances, or age;  \\n(g) the extent to which the outcome produced with an AI system is easily \\nreversible, whereby outcomes having an impact on the health or safety of \\npersons shall not be considered as easily reversible;  \\n(h) the extent to which existing Union legislation provides for:  \\n(i) effective measures of redress in relation to the risks posed by an AI \\nsystem, with the exclusion of  claims for damages;  \\n(ii) effective measures to prevent or substantially minimise those risks.  \\nCHAPTER 2 \\nREQUIREMENTS FOR HIG H-RISK AI SYSTEMS  \\nArticle 8  \\nCompliance with the requirements  \\n1. High -risk AI systems shall comply with the requirements established  in this Chapter.  \\n2. The intended purpose of the high -risk AI system and the risk management system \\nreferred to in Article 9 shall be taken into account when ensuring compliance with \\nthose requirements.  \\nArticle 9  \\nRisk management system  \\n1. A risk management  system shall be established, implemented, documented and \\nmaintained in relation to high -risk AI systems.  \\n2. The risk management system shall consist of a continuous iterative process run \\nthroughout the entire lifecycle of a high -risk AI system, requiring regular systematic \\nupdating. It shall comprise the following steps:  EN 47  EN (a) identification and analysis of the known and foreseeable risks associated with \\neach high -risk AI system;  \\n(b) estimation and evaluation of the risks that may emerge when the high -risk A I \\nsystem is used in accordance with its intended purpose and under conditions of \\nreasonably foreseeable misuse;  \\n(c) evaluation of other possibly arising risks based on the analysis of data gathered \\nfrom the post -market monitoring system referred to in Art icle 61;  \\n(d) adoption of suitable risk management measures in accordance with the \\nprovisions of the following paragraphs.  \\n3. The risk management measures referred to in paragraph 2, point (d) shall give due \\nconsideration to the effects and possible interac tions resulting from the combined \\napplication of the requirements set out in this Chapter 2. They shall take into account \\nthe generally acknowledged state of the art, including as reflected in relevant \\nharmonised standards or common specifications.  \\n4. The risk management measures referred to in paragraph 2, point (d) shall be such that \\nany residual risk associated with each hazard as well as the overall residual risk of \\nthe high -risk AI systems is judged acceptable, provided that the high -risk AI system \\nis used in accordance with its intended purpose or under conditions of reasonably \\nforeseeable misuse. Those residual risks shall be communicated to the user.  \\nIn identifying the most appropriate risk management measures, the following shall be \\nensured:  \\n(a) elimination or reduction of risks as far as possible through adequate design and \\ndevelopment;  \\n(b) where appropriate, implementation of adequate mitigation and control \\nmeasures in relation to risks that cannot be eliminated;  \\n(c) provision of adequate informati on pursuant to Article 13, in particular as \\nregards the risks referred to in paragraph 2, point (b) of this Article, and, where \\nappropriate, training to users.  \\nIn eliminating or reducing risks related to the use of the high -risk AI system, due \\nconsideratio n shall be given to the technical knowledge, experience, education, \\ntraining to be expected by the user and the environment in which the system is \\nintended to be used.  \\n5. High -risk AI systems shall be tested for the purposes of identifying the most \\nappropr iate risk management measures. Testing shall ensure that high -risk AI \\nsystems perform consistently for their intended purpose and they are in compliance \\nwith the requirements set out in this Chapter.  \\n6. Testing procedures shall be suitable to achieve the i ntended purpose of the AI system \\nand do not need to go beyond what is necessary to achieve that purpose.  \\n7. The testing of the high -risk AI systems shall be performed, as appropriate, at any \\npoint in time throughout the development process, and, in any eve nt, prior to the \\nplacing on the market or the putting into service. Testing shall be made against \\npreliminarily defined metrics and probabilistic thresholds that are appropriate to the \\nintended purpose of the high -risk AI system.  EN 48  EN 8. When implementing the r isk management system described in paragraphs 1 to 7, \\nspecific consideration shall be given to whether the high -risk AI system is likely to \\nbe accessed by or have an impact on children.  \\n9. For credit institutions regulated by Directive 2013/36/EU, the aspe cts described in \\nparagraphs 1 to 8 shall be part of the risk management procedures established by \\nthose institutions pursuant to Article 74 of that Directive.  \\nArticle 10  \\nData and data governance  \\n1. High -risk AI systems which make use of techniques involvin g the training of models \\nwith data shall be developed on the basis of training, validation and testing data sets \\nthat meet the quality criteria referred to in paragraphs 2 to 5.  \\n2. Training, validation and testing data sets shall be subject to appropriate data \\ngovernance and management practices. Those practices shall concern in particular,  \\n(a) the relevant design choices;  \\n(b) data collection;  \\n(c) relevant data preparation processing operations, such as annotation, labelling, \\ncleaning, enrichment and aggregation ; \\n(d) the formulation of relevant assumptions, notably with respect to the \\ninformation that the data are supposed to measure and represent;  \\n(e) a prior assessment of  the availability, quantity and suitability of the data sets \\nthat are needed;  \\n(f) examination in view of possible biases;  \\n(g) the identification of any possible data gaps or shortcomings, and how those \\ngaps and shortcomings can be addressed.  \\n3. Training, validation and testing data sets shall be relevant, representative, free of \\nerrors and complete. They shall have the appropriate statistical properties, including, \\nwhere applicable, as regards the persons or groups of persons on which the high -risk \\nAI syst em is intended to be used. These characteristics of the data sets may be met at \\nthe level of individual data sets or a combination thereof.  \\n4. Training, validation and testing data sets shall take into account, to the extent \\nrequired by the intended purpos e, the characteristics or elements that are particular to \\nthe specific geographical, behavioural or functional setting within which the high -\\nrisk AI system is intended to be used.  \\n5. To the extent that it is strictly necessary for the purposes of ensuring  bias monitoring, \\ndetection and correction in relation to the high -risk AI systems, the providers of such \\nsystems may process special categories of personal data  referred to in Article 9(1) of \\nRegulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680  and Article 10(1) \\nof Regulation (EU) 2018/1725, subject to appropriate safeguards for the fundamental \\nrights and freedoms of natural persons, including technical limitations on the re -use \\nand use of state -of-the-art security and privacy -preserving measure s, such as \\npseudonymisation, or encryption where anonymisation may significantly affect the \\npurpose pursued.  EN 49  EN 6. Appropriate data governance and management practices shall apply for the \\ndevelopment of high -risk AI systems other than those which make use of techniques \\ninvolving the training of models in order to ensure that those high -risk AI systems \\ncomply with paragraph 2.  \\nArticle 11  \\nTechnical documentation  \\n1. The technical documentation of a high -risk AI system shall be drawn up before that \\nsystem is pla ced on the market or put into service and shall be kept up -to date.  \\nThe technical documentation shall be drawn up in such a way to demonstrate that the \\nhigh-risk AI system complies with the requirements set out in this Chapter and \\nprovide national competen t authorities and notified bodies with all the necessary \\ninformation to assess the compliance of the AI system with those requirements. It \\nshall contain, at a minimum, the elements set out in Annex IV.  \\n2. Where a high -risk AI system related to a product, t o which the legal acts listed in \\nAnnex II, section A apply, is placed on the market or put into service one single \\ntechnical documentation shall be drawn up containing all the information set out in \\nAnnex IV as well as the information required under those legal acts.  \\n3. The Commission is empowered to adopt delegated acts in accordance with Article 73 \\nto amend Annex IV where necessary to ensure that, in the light of technical progress, \\nthe technical documentation provides all the necessary information to ass ess the \\ncompliance of the system with the requirements set out in this Chapter.  \\nArticle 12  \\nRecord -keeping  \\n1. High -risk AI systems shall be designed and developed with capabilities enabling the \\nautomatic recording of events (‘logs’) while the high -risk AI s ystems is operating. \\nThose logging capabilities shall conform to recognised standards or common \\nspecifications.  \\n2. The logging capabilities shall ensure a level of traceability of the AI system’s \\nfunctioning throughout its lifecycle that is appropriate to the intended purpose of the \\nsystem.  \\n3. In particular, logging capabilities shall enable the monitoring of the operation of the \\nhigh-risk AI system with respect to the occurrence of situations that may result in the \\nAI system presenting a risk within the m eaning of Article 65(1) or lead to a \\nsubstantial modification, and facilitate the post -market monitoring referred to in \\nArticle 61.  \\n4. For high -risk AI systems referred to in paragraph 1, point (a) of Annex III, the \\nlogging capabilities shall provide, at a minimum:  \\n(a) recording of the period of each use of the system (start date and time and end \\ndate and time of each use);  \\n(b) the reference database against which input data has been checked by the \\nsystem;  \\n(c) the input data for which the search has led to a match;  EN 50  EN (d) the identification of the natural persons involved in the verification of the \\nresults, as referred to in Article 14 (5).  \\nArticle 13  \\nTransparency and provision of information to users  \\n1. High -risk AI systems shall be designed and developed in such a way to ensure that \\ntheir operation is sufficiently transparent to enable users to interpret the system’s \\noutput and use it appropriately. An appropriate type and degree of transparency shall \\nbe ensured, with a view to achieving compliance with th e relevant obligations of the \\nuser and of the provider set out  in Chapter 3 of this Title . \\n2. High -risk AI systems shall be accompanied by instructions for use in an appropriate \\ndigital format or otherwise that include concise, complete, correct and clear \\ninformation that is relevant, accessible and comprehensible to users.  \\n3. The information referred to in paragraph 2 shall specify:  \\n(a) the identity and the contact details of the provider and, where applicable, of its \\nauthorised representative;  \\n(b) the cha racteristics, capabilities and limitations of performance of the high -risk \\nAI system, including:  \\n(i) its intended purpose;  \\n(ii) the level of accuracy, robustness and cybersecurity referred to in Article \\n15 against which the high -risk AI system has been tested and validated \\nand which can be expected, and any known and foreseeable \\ncircumstances that may have an impact on that expected level of \\naccuracy, robustness and cybersecurity;  \\n(iii) any known or foreseeable circumstance, related to the use of the hig h-\\nrisk AI system in accordance with its intended purpose or under \\nconditions of reasonably foreseeable misuse, which may lead to risks to \\nthe health and safety or fundamental rights;  \\n(iv) its performance as regards the persons or groups of persons on which  the \\nsystem is intended to be used;  \\n(v) when appropriate, specifications for the input data,  or any other relevant \\ninformation in terms of the training, validation and testing data sets used, \\ntaking into account the intended purpose of the AI system . \\n(c) the changes to the high -risk AI system and its performance which have been \\npre-determined by the provider at the moment of the initial conformity \\nassessment, if any;   \\n(d) the human oversight measures referred to in Article 14, including the technical \\nmeasur es put in place to facilitate the interpretation of the outputs of AI \\nsystems by the users;  \\n(e) the expected lifetime of the high -risk AI system and any necessary \\nmaintenance and care measures to ensure the proper functioning of that AI \\nsystem, including a s regards software updates.  EN 51  EN Article 14  \\nHuman oversight  \\n1. High -risk AI systems shall be designed and developed in such a way, including with \\nappropriate human -machine interface tools, that they can be effectively overseen by \\nnatural persons during the peri od in which the AI system is in use.  \\n2. Human oversight shall aim at preventing or minimising the risks to health, safety or \\nfundamental rights that may emerge when a high -risk AI system is used in \\naccordance with its intended purpose or under conditions of reasonably foreseeable \\nmisuse, in particular when such risks persist notwithstanding the application of other \\nrequirements set out in this Chapter . \\n3. Human oversight shall be ensured through either one or all of the following \\nmeasures:  \\n(a) identified a nd built, when technically feasible, into the high -risk AI system by \\nthe provider before it is placed on the market or put into service;  \\n(b) identified by the provider  before placing the high -risk AI system on the market \\nor putting it into service and tha t are appropriate to be implemented by the \\nuser.  \\n4. The measures referred to in paragraph 3 shall enable the individuals  to whom human \\noversight is assigned to do the following, as appropriate to the circumstances:  \\n(a) fully understand the capacities and l imitations of the high -risk AI system and \\nbe able to duly monitor its operation, so that signs of anomalies, dysfunctions \\nand unexpected performance can be detected and addressed as soon as \\npossible;  \\n(b) remain aware of the possible tendency of automatical ly relying or over -relying \\non the output produced by a high -risk AI system (‘automation bias’), in \\nparticular for high -risk AI systems used to provide information or \\nrecommendations for decisions to be taken by natural persons;  \\n(c) be able to correctly int erpret the high -risk AI system’s output, taking into \\naccount in particular the characteristics of the system and the interpretation \\ntools and methods available;  \\n(d) be able to decide, in any particular situation, not to use the high -risk AI system \\nor other wise disregard, override or reverse the output of the high -risk AI \\nsystem;  \\n(e) be able to intervene on the operation of the high -risk AI system or interrupt the \\nsystem through a “stop” button or a similar procedure.  \\n5. For high -risk AI systems referred to in point 1(a) of Annex III, the measures referred \\nto in paragraph 3 shall be such as to ensure that, in addition, no action or decision is \\ntaken by the user on the basis of the identification resulting from the system unless \\nthis has been verified and conf irmed by at least two natural persons.  \\nArticle 15  \\nAccuracy, robustness and cybersecurity  \\n1. High -risk AI systems shall be designed and developed in such a way that they \\nachieve, in the light of their intended purpose, an appropriate level of accuracy, EN 52  EN robustness and cybersecurity, and perform consistently in those respects throughout \\ntheir lifecycle.  \\n2. The levels of accuracy and the relevant accuracy metrics of high -risk AI systems \\nshall be declared in the accompanying instructions of use.  \\n3. High -risk AI systems shall be resilient as regards errors, faults or inconsistencies that \\nmay occur within the system or the environment in which the system operates, in \\nparticular due to their interaction with natural persons or other systems.  \\nThe robustness of high -risk AI systems may be achieved through technical \\nredundancy solutions, which may include backup or fail -safe plans.  \\nHigh -risk AI systems that continue to learn after being placed on the market or put \\ninto service shall be developed in such a way to ensure that possibly biased outputs \\ndue to outputs used as an input for future operations (‘feedback loops’) are duly \\naddressed with appropriate mitigation measures.  \\n4. High -risk AI systems shall be resilient as regards attempts by unauthorised third \\nparties to a lter their use or performance by exploiting the system vulnerabilities.  \\nThe technical solutions aimed at ensuring the cybersecurity of high -risk AI systems \\nshall be appropriate to the relevant circumstances and the risks.  \\nThe technical solutions to address AI specific vulnerabilities shall include, where \\nappropriate, measures to prevent and control for attacks trying to manipulate the \\ntraining dataset  (‘data poisoning’), inputs designed to cause the model to make a \\nmistake  (‘adversarial examples’), or model flaws.  \\nCHAPTER 3 \\nOBLIGATIONS OF PROVI DERS AND USERS OF HI GH-RISK AI SYSTEMS AND \\nOTHER PARTIES  \\nArticle 16  \\nObligations of providers of high -risk AI systems  \\nProviders of high -risk AI systems shall:  \\n(a) ensure that their hig h-risk AI systems are compliant with the requirements set out in \\nChapter 2 of this Title;  \\n(b) have a quality management system in place which complies with Article 17;  \\n(c) draw -up the technical documentation of the high -risk AI system;  \\n(d) when under their  control, keep the logs automatically generated by their high -risk AI \\nsystems;  \\n(e) ensure that the high -risk AI system undergoes the relevant conformity assessment \\nprocedure, prior to its placing on the market or putting into service;  \\n(f) comply with the r egistration obligations referred to in Article 51;  \\n(g) take the necessary corrective actions , if the high -risk AI system is not in conformity \\nwith the requirements set out in Chapter 2 of this Title ; EN 53  EN (h) inform the national competent authorities of the Mem ber States in which they made \\nthe AI system available or put it into service and, where applicable, the notified body \\nof the non -compliance and of any corrective actions taken;  \\n(i) to affix the CE marking to their high -risk AI systems to indicate the confo rmity with \\nthis Regulation in accordance with Article 49;  \\n(j) upon request of a national competent authority, demonstrate the conformity of the \\nhigh-risk AI system with the requirements set out in Chapter 2 of this Title . \\nArticle 17  \\nQuality management syst em  \\n1. Providers of high -risk AI systems shall put a quality management system in place \\nthat ensures compliance with this Regulation. That system shall be documented in a \\nsystematic and orderly manner in the form of written policies, procedures and \\ninstruc tions, and shall include at least the following aspects:  \\n(a) a strategy for regulatory compliance, including compliance with conformity \\nassessment procedures and procedures for the management of modifications to \\nthe high -risk AI system;  \\n(b) techniques, pro cedures and systematic actions to be used for the design, design \\ncontrol and design verification of the high -risk AI system;  \\n(c) techniques, procedures and systematic actions to be used for the development, \\nquality control and quality assurance of the high -risk AI system;  \\n(d) examination, test and validation procedures to be carried out before, during and \\nafter the development of the high -risk AI system, and the frequency with which \\nthey have to be carried out;  \\n(e) technical specifications, including standa rds, to be applied and, where the \\nrelevant harmonised standards are not applied in full, the means to be used to \\nensure that the high -risk AI system complies with the requirements set out in \\nChapter 2 of this Title;  \\n(f) systems and procedures for data management, including data collection, data \\nanalysis, data labelling, data storage, data filtration, data mining, data \\naggregation, data retention and any other operation regarding the data that is \\nperformed before and for the purposes of the placing on th e market or putting \\ninto service of high -risk AI systems;  \\n(g) the risk management system referred to in Article 9;  \\n(h) the setting -up, implementation and maintenance of a post -market monitoring \\nsystem, in accordance with Article 61;  \\n(i) procedures related to the reporting of serious incidents and of malfunctioning \\nin accordance with Article 62;  \\n(j) the handling of communication with national competent authorities, competent \\nauthorities, including sectoral ones, providing or supporting the access to data, \\nnotified bodies, other operators, customers or other interested parties;  \\n(k) systems and procedures for record keeping of all relevant documentation and \\ninformation;  \\n(l) resource management, including security of supply related measures;  EN 54  EN (m) an accountabil ity framework setting out the responsibilities of the management \\nand other staff with regard to all aspects listed in this paragraph.  \\n2. The implementation of aspects referred to in paragraph 1 shall be proportionate to the \\nsize of the provider’s organisat ion.  \\n3. For providers that are credit institutions regulated by Directive 2013/36/ EU, the \\nobligation to put a quality management system in place shall be deemed to be \\nfulfilled by complying with the rules on internal governance arrangements, processes \\nand mechanisms pursuant to Article 74 of that Directive. In that context, any \\nharmonised standards referred to in Article 40 of this Regulation shall be taken into \\naccount.  \\nArticle 18  \\nObligation to draw up technical documentation  \\n1. Providers of high -risk AI  systems shall draw up the technical documen tation referred \\nto in Article 11 in accordance with Annex IV.  \\n2. Providers that are credit institutions regulated by Directive 2013/36/EU shall \\nmaintain the technical documentation as part of the documentation c oncerning \\ninternal governance, arrangements, processes and mechanisms pursuant to Article 74 \\nof that Directive.  \\nArticle 19  \\nConformity assessment  \\n1. Providers of high -risk AI systems shall ensure that their systems undergo the relevant \\nconformity assessmen t procedure in accordance with Article 43, prior to their placing \\non the market or putting into service. Where the compliance of the AI systems with \\nthe requirements set out in Chapter 2 of this Title has been demonstrated following \\nthat conformity assessm ent, the providers shall draw up an EU declaration of \\nconformity in accordance with Article 48 and affix the CE marking of conformity in \\naccordance with Article 49.  \\n2. For high -risk AI systems referred to in point 5(b) of Annex III that are placed on the \\nmarket or put into service by providers that are credit institutions regulated by \\nDirective 2013/36/EU, the conformity assessment  shall be carried out as part of the \\nprocedure referred to in Articles 97 to101 of that Directive.  \\nArticle 20  \\nAutomatically gen erated logs  \\n1. Providers of high -risk AI systems shall keep the logs automatically generated by \\ntheir high -risk AI systems, to the extent such logs are under their control by virtue of \\na contractual arrangement with the user or otherwise by law. The logs s hall be kept \\nfor a period that is appropriate in the light of the intended purpose of high -risk AI \\nsystem and applicable legal obligations under Union or national law.  \\n2. Providers that are credit institutions regulated by Directive 2013/36/EU shall \\nmainta in the logs automatically generated by their high -risk AI systems as part of the \\ndocumentation under Articles 74 of that Directive.  EN 55  EN Article 21  \\nCorrective actions  \\nProviders of high -risk AI systems which consider o r have reason to consider that a high -risk \\nAI system which they have placed on the market or put into service is not in conformity with \\nthis Regulation shall immediately take the necessary corrective actions to bring that system \\ninto conformity, to withdraw it or to recall it, as appropriate. They s hall inform the \\ndistributors of the high -risk AI system in question and, where applicable, the authorised \\nrepresentative and importers accordingly.  \\nArticle 22  \\nDuty of information  \\nWhere the high-risk AI system presents a risk within the meaning of Article 6 5(1) and that \\nrisk is known to the provider of the system, that provider shall immediately inform the \\nnational competent authorities of the Member States in which it made the system available \\nand, where applicable, the notified body that issued a certifica te for the  high-risk AI system, \\nin particular of the non -compliance and of any corrective actions taken.   \\nArticle 23  \\nCooperation with competent authorities  \\nProviders of high -risk AI systems shall, upon request by a national competent authority, \\nprovide tha t authority with all the information and documentation necessary to demonstrate \\nthe conformity of the high -risk AI system with the requirements set out in Chapter 2 of this \\nTitle , in an official Union language determined by the Member State concerned. Upon  a \\nreasoned request from a national competent authority, providers shall also give that authority \\naccess to the logs automatically generated by the high -risk AI system, to the extent such logs \\nare under their control by virtue of a contractual arrangement with the user or otherwise by \\nlaw. \\nArticle 24  \\nObligations of product manufacturers  \\nWhere a high -risk AI system related to products to which the legal acts listed in Annex II, \\nsection A, apply, is placed on the market or put into service together with the product \\nmanufactured in accordance with those legal acts and under the name of the product \\nmanufacturer, the manufacturer of the product shall take the responsibility of the compliance \\nof the AI system with this Regulation and, as far as the AI system is c oncerned, have the same \\nobligations imposed by the present Regulation on the provider.  \\nArticle 25  \\nAuthorised representatives  \\n1. Prior to making their systems available on the Union market, where an importer \\ncannot be identified, providers established outside the Union shall, by written \\nmandate, appoint an authorised representative which is established in the Union.  \\n2. The authorised representative shall perform the tasks specified in the mandate \\nreceived from the provider. The mandate shall empower the  authorised representative \\nto carry out the following tasks:  EN 56  EN (a) keep a copy of the EU declaration of conformity and the technical \\ndocumentation at the disposal of the national competent authorities and \\nnational authorities referred to in Article 63(7);  \\n(b) provide a national competent authority, upon a reasoned request, with all the \\ninformation and documentation necessary to demonstrate the conformity of a \\nhigh-risk AI system with the requirements set out in Chapter 2 of this Title, \\nincluding access to the  logs automatically generated by the high -risk AI system  \\nto the extent such logs are under the control of the provider by virtue of a \\ncontractual arrangement with the user or otherwise by law ; \\n(c) cooperate with competent national authorities, upon a reaso ned request, on any \\naction the latter takes in relation to the high -risk AI system.  \\nArticle 26  \\nObligations of importers  \\n1. Before placing a high -risk AI system on the market, importers of such system shall \\nensure that:  \\n(a) the appropriate conformity assess ment procedure has been carried out by the \\nprovider of that AI system  \\n(b) the provider has drawn up the technical documentation in accordance with \\nAnnex IV;  \\n(c) the system bears the required conformity marking and is accompanied by the \\nrequired documenta tion and instructions of use.  \\n2. Where an importer considers or has reason to consider that a high -risk AI system is \\nnot in conformity with this Regulation, it shall not place that system on the market \\nuntil that AI system has been brought into conformity . Where the high -risk AI \\nsystem presents a risk within the meaning of Article 65(1), the importer shall inform \\nthe provider of the AI system and the market surveillance authorities to that effect.  \\n3. Importers shall indicate their name, registered trade na me or registered trade mark, \\nand the address at which they can be contacted on the high -risk AI system or, where \\nthat is not possible, on its packaging or its accompanying documentation, as \\napplicable.  \\n4. Importers shall ensure that, while a high -risk AI s ystem is under their responsibility, \\nwhere applicable, storage or transport conditions do not jeopardise its compliance \\nwith the requirements set out in Chapter 2 of this Title . \\n5. Importers shall provide national competent authorities, upon a reasoned req uest, with \\nall necessary information and documentation to demonstrate the conformity of a \\nhigh-risk AI system with the requirements set out in Chapter 2 of this Title in a \\nlanguage which can be easily understood by that national competent authority, \\ninclud ing access to the logs automatically generated by the high -risk AI system to the \\nextent such logs are under the control of the provider by virtue of a contractual \\narrangement with the user or otherwise by law. They shall also cooperate with those \\nauthoriti es on any action national competent authority takes in relation to that \\nsystem.  EN 57  EN Article 27  \\nObligations of distributors  \\n1. Before making a high -risk AI system available on the market, distributors shall \\nverify that the high -risk AI system bears the required  CE conformity marking, that it \\nis accompanied by the required documentation and instruction of use, and that the \\nprovider and the importer of the system, as applicable, have complied with the \\nobligations set out in this Regulation.  \\n2. Where a distributor considers or has reason to consider that a high -risk AI system is \\nnot in conformity with the requirements set out in Chapter 2 of this Title, it shall not \\nmake the high -risk AI system available on the market until that system has been \\nbrought into conformi ty with those requirements. Furthermore, where the system \\npresents a risk within the meaning of Article 65(1), the distributor shall inform the \\nprovider or the importer of the system, as applicable, to that effect.  \\n3. Distributors shall ensure that, while a high -risk AI system is under their \\nresponsibility, where applicable, storage or transport conditions do not jeopardise the \\ncompliance of the system with the requirements set out in Chapter 2 of this Title . \\n4. A distributor that considers or has reason to  consider that a high -risk AI system \\nwhich it has made available on the market is not in conformity with the requirements \\nset out in Chapter 2 of this Title shall take the corrective actions necessary to bring \\nthat system into conformity with those require ments, to withdraw it or recall it or \\nshall ensure that the provider, the importer or any relevant operator, as appropriate, \\ntakes those corrective actions. Where the high -risk AI system presents a risk within \\nthe meaning of Article 65(1), the distributor shall immediately inform the national \\ncompetent authorities of the Member States in which it has made the product \\navailable to that effect, giving details, in particular, of the non -compliance and of any \\ncorrective actions taken.  \\n5. Upon a reasoned request  from a national competent authority, distributors of high -\\nrisk AI systems shall provide that authority with all the information and \\ndocumentation necessary to demonstrate the conformity of a high -risk system with \\nthe requirements set out in Chapter 2 of t his Title . Distributors shall also cooperate \\nwith that national competent authority on any action taken by that authority.  \\nArticle 28  \\nObligations of distributors, importers, users or any other third -party  \\n1. Any distributor, importer, user or other third -party shall be considered a provider for \\nthe purposes of this Regulation and shall be subject to the obligations of the provider \\nunder Article 16, in any of the following circumstances:  \\n(a) they place on the market or put into service a high -risk AI syste m under their \\nname or trademark;  \\n(b) they modify the intended purpose of a high -risk AI system already placed on \\nthe market or put into service;  \\n(c) they make a substantial modification to the high -risk AI system.  \\n2. Where the circumstances referred to in paragraph 1, point (b) or (c), occur, the \\nprovider that initially placed the high -risk AI system on the market or put it into \\nservice shall no longer be considered a provider for the purposes of this Regulation.  EN 58  EN Article 29  \\nObligations of users of high -risk AI systems  \\n1. Users of high -risk AI systems shall use such systems in accordance with the \\ninstructions of use accompanying the systems, pursuant to paragraphs 2 and 5.  \\n2. The obligations in paragraph 1 are without prejudice to other user obligations under \\nUnion or national law and to the user’s discretion  in organising its own resources and \\nactivities for the purpose of implementing the human oversight measures indicated \\nby the provider.  \\n3. Without prejudice to paragraph 1, to the extent the user exer cises control over the \\ninput data, that user shall ensure that input data is relevant in view of the intended \\npurpose of the high -risk AI system.  \\n4. Users shall monitor the operation of the high -risk AI system on the basis of the \\ninstructions of use. When  they have reasons to consider that the use in accordance \\nwith the instructions of use may result in the AI system presenting a risk within the \\nmeaning of Article 65(1) they shall inform the provider or distributor and suspend \\nthe use of the system. They s hall also inform the provider or distributor when they \\nhave identified any serious incident or any malfunctioning within the meaning of \\nArticle 62 and interrupt the use of the AI system. In case the user is not able to reach \\nthe provider, Article 62 shall apply mutatis mutandis.  \\nFor users that are credit institutions regulated by Directive 2013/36/EU, the \\nmonitoring obligation set out in the first subparagraph shall be deemed to be fulfilled \\nby complying with the rules on internal governance arrangements, processes and \\nmechanisms pursuant to Article 74 of that Directive.  \\n5. Users of high -risk AI systems shall keep the logs automatically generated by that \\nhigh-risk AI system, to the extent such logs are under their control. The logs shall be \\nkept for a perio d that is appropriate in the light of the intended purpose of the high -\\nrisk AI system and applicable legal obligations under Union or national law.  \\nUsers that are credit institutions regulated by Directive 2013/36/EU shall maintain \\nthe logs as part of the documentation concerning internal governance arrangements, \\nprocesses and mechanisms  pursuant to Article 74 of that Directive.  \\n6. Users of high -risk AI systems shall use the information provided under Article 13 to \\ncomply with their obligation to carry out a data protection impact assessment under \\nArticle 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, \\nwhere applicable.  \\nCHAPTER 4 \\nNOTIFIYING AUTHORITI ES AND NOTIFIED BODI ES \\nArticle 30  \\nNotifying authorities  \\n1. Each Member State shall de signate or establish a notifying authority responsible for \\nsetting up and carrying out the necessary procedures for the assessment, designation \\nand notification of conformity assessment bodies and for their monitoring .  \\n2. Member States may designate a nat ional accreditation body referred to in Regulation \\n(EC) No 765/2008 as a notifying authority.  EN 59  EN 3. Notifying authorities shall be established, organised and operated in such a way that \\nno conflict of interest arises with conformity assessment bodies and the objectivity \\nand impartiality of their activities are safeguarded.  \\n4. Notifying authorities shall be organised in such a way that decisions relating to the \\nnotification of conformity assessment bodies are taken by competent persons \\ndifferent from those who carried out the assessment of those bodies.  \\n5. Notifying authorities shall not offer or provide any activities that conformity \\nassessment bodies perform or any consultancy services on a commercial or \\ncompetitive basis.  \\n6. Notifying authorities shall safegu ard the confidentiality of the information they \\nobtain.  \\n7. Notifying authorities shall have a sufficient number of competent personnel at their \\ndisposal for the proper performance of their tasks.  \\n8. Notifying authorities shall make sure that conformity ass essments are carried out in a \\nproportionate manner, avoiding unnecessary burdens for providers and that notified \\nbodies perform their activities taking due account of the size of an undertaking, the \\nsector in which it operates, its structure and the degree  of complexity of the AI \\nsystem in question.  \\nArticle 31  \\nApplication of a conformity assessment body for notification  \\n1. Conformity assessment bodies shall submit an application for notification to the \\nnotifying authority of the Member State in which they are established.  \\n2. The application for notification shall be accompanied by a description of the \\nconformity assessment activities, the conformity assessment module or modules and \\nthe artificial intelligence technologies for which the conformity assessment  body \\nclaims to be competent, as well as by an accreditation certificate, where one exists, \\nissued by a national accreditation body attesting that the conformity assessment body \\nfulfils the requirements laid down in Article 33. Any valid document related t o \\nexisting designations of the applicant notified body under any other Union \\nharmonisation legislation shall be added.  \\n3. Where the conformity assessment body concerned cannot provide an accreditation \\ncertificate, it shall provide the notifying authority with the documentary evidence \\nnecessary for the verification, recognition and regular monitoring of its compliance \\nwith the requirements laid down in Article 33. For notified bodies which are \\ndesignated under any other Union harmonisation legislation, all documents and \\ncertificates linked to those designations may be used to support their designation \\nprocedure under this Regulation, as appropriate.  \\nArticle 32  \\nNotification procedure  \\n1. Notifying authorities may notify only conformity assessment bodies which have \\nsatisfied the requirements laid down in Article 33.  \\n2. Notifying authorities shall notify the Commission and the other Member States using \\nthe electronic notification tool devel oped and managed by the Commission.  EN 60  EN 3. The notification shall include full details of the conformity assessment activities, the \\nconformity assessment module or modules and the artificial intelligence technologies \\nconcerned.  \\n4. The conformity assessment b ody concerned may perform the activities of a notified \\nbody only where no objections are raised by the Commission or the other Member \\nStates within one month of a notification.  \\n5. Notifying authorities shall notify the Commission and the other Member States of \\nany subsequent relevant changes to the notification.  \\nArticle 33  \\nNotified bodies  \\n1. Notified bodies shall verify the conformity of high -risk AI system in accordance with \\nthe conformity assessment procedures referred to in Article 43.  \\n2. Notified bodies shall satisfy the organisational, quality management, resources and \\nprocess requirements that are necessary to fulfil their tasks.  \\n3. The organisational structure, allocation of responsibilities, reporting lines and \\noperation of notified bodies shal l be such as to ensure that there is confidence in the \\nperformance by and in the results of the conformity assessment activities that the \\nnotified bodies conduct.  \\n4. Notified bodies shall be independent of the provider of a high-risk AI system in \\nrelation to which it performs conformity assessment activities. Notified bodies shall \\nalso be independent of any other operator having an economic interest in the high -\\nrisk AI system that is assessed, as well as of any competitors of the provider.  \\n5. Notified bodie s shall be organised and operated so as to safeguard the independence, \\nobjectivity and impartiality of their activities. Notified bodies shall document and \\nimplement a structure and procedures to safeguard impartiality and to promote and \\napply the principl es of impartiality throughout their organisation, personnel and \\nassessment activities.  \\n6. Notified bodies shall have documented procedures in place ensuring that their \\npersonnel, committees, subsidiaries, subcontractors and any associated body or \\npersonnel  of external bodies respect the confidentiality of the information which \\ncomes into their possession during the performance of conformity assessment \\nactivities, except when disclosure is required by law. The staff of notified bodies \\nshall be bound to obser ve professional secrecy with regard to all information \\nobtained in carrying out their tasks under this Regulation, except in relation to the \\nnotifying authorities of the Member State in which their activities are carried out.  \\n7. Notified bodies shall have  procedures for the performance of activities which take \\ndue account of the size of an undertaking, the sector in which it operates, its \\nstructure, the degree of complexity of the AI system in question.  \\n8. Notified bodies shall take out appropriate liabili ty insurance for their conformity \\nassessment activities, unless liability is assumed by the Member State concerned in \\naccordance with national law or that Member State is directly responsible for the \\nconformity assessment.  \\n9. Notified bodies shall be capab le of carrying out all the tasks falling to them under \\nthis Regulation with the highest degree of professional integrity and the requisite EN 61  EN competence in the specific field, whether those tasks are carried out by notified \\nbodies themselves or on their behal f and under their responsibility.  \\n10. Notified bodies shall have sufficient internal competences to be able to effectively \\nevaluate the tasks conducted by external parties on their behalf. To that end, at all \\ntimes and for each conformity assessment proced ure and each type of high-risk AI \\nsystem in relation to which they have been designated, the notified body shall have \\npermanent availability of sufficient administrative, technical and scientific personnel \\nwho possess experience and knowledge relating to t he relevant artificial intelligence \\ntechnologies, data and data computing and to the requirements set out in Chapter 2 of \\nthis Title . \\n11. Notified bodies shall participate in coordination activities as referred to in Article 38. \\nThey shall also take part d irectly or be represented in European standardisation \\norganisations, or ensure that they are aware and up to date in respect of relevant \\nstandards.  \\n12. Notified bodies shall make available and submit upon request all relevant \\ndocumentation, including the p roviders’ documentation, to the notifying authority \\nreferred to in Article 30 to allow it to conduct its assessment, designation, \\nnotification, monitoring and surveillance activities and to facilitate the assessment \\noutlined in this Chapter.  \\nArticle 34  \\nSubsidiaries of and subcontracting by notified bodies  \\n1. Where a notified body subcontracts specific tasks connected with the conformity \\nassessment or has recourse to a subsidiary, it shall ensure that the subcontractor or \\nthe subsidiary meets the requirement s laid down in Article 33 and shall inform the \\nnotifying authority accordingly.  \\n2. Notified bodies shall take full responsibility for the tasks performed by \\nsubcontractors or subsidiaries wherever these are established.  \\n3. Activities may be subcontracted or carried out by a subsidiary only with the \\nagreement of the provider.  \\n4. Notified bodies shall keep at the disposal of the notifying authority the relevant \\ndocuments concerning the assessment of the qualifications of the subcontractor or the \\nsubsidiary a nd the work carried out by them under this Regulation.  \\nArticle 35  \\nIdentification numbers and lists of notified bodies designated under this Regulation  \\n1. The Commission shall assign an identification number to notified bodies. It shall \\nassign a single numb er, even where a body is notified under several Union acts.  \\n2. The Commission shall make publicly available the list of the bodies notified under \\nthis Regulation, including the identification numbers that have been assigned to them \\nand the activities for w hich they have been notified. The Commission shall ensure \\nthat the list is kept up to date.  EN 62  EN Article 36  \\nChanges to notifications  \\n1. Where a notifying authority has suspicions or has been informed that a notified body \\nno longer meets the requirements laid do wn in Article 33, or that it is failing to fulfil \\nits obligations, that authority shall without delay investigate the matter with the \\nutmost diligence. In that context, it shall inform the notified body concerned about \\nthe objections raised and give it the  possibility to make its views known. If the \\nnotifying authority comes to the conclusion that the notified body investigation no \\nlonger meets the requirements laid down in Article 33 or that it is failing to fulfil its \\nobligations, it shall restrict, suspe nd or withdraw the notification as appropriate, \\ndepending on the seriousness of the failure. It shall also immediately inform the \\nCommission and the other Member States accordingly.  \\n2. In the event of restriction, suspension or withdrawal of notification, or where the \\nnotified body has ceased its activity, the notifying authority shall take appropriate \\nsteps to ensure that the files of that notified body are either taken over by another \\nnotified body or kept available for the responsible notifying authoriti es at their \\nrequest.  \\nArticle 37  \\nChallenge to the competence of notified bodies  \\n1. The Commission shall, where necessary, investigate all cases where there are reasons \\nto doubt whether a notified body complies with the requirements  laid down in Article \\n33. \\n2. The Notifying authority shall provide the Commission, on request, with all relevant \\ninformation relating to the notification of the notified body concerned.  \\n3. The Commission shall ensure that all confidential information obtained in the course \\nof its i nvestigations pursuant to this Article is treated confidentially.  \\n4. Where the Commission ascertains that a notified body does not meet or no longer \\nmeets the requirements  laid down in Article 33, it shall adopt a reasoned decision \\nrequesting the notifying  Member State to take the necessary corrective measures, \\nincluding withdrawal of notification if necessary. That implementing act shall be \\nadopted in accordance with the examination procedure referred to in Article 74(2).  \\nArticle 38  \\nCoordination of notifie d bodies  \\n1. The Commission shall ensure that, with regard to the areas covered by this \\nRegulation, appropriate coordination and cooperation between notified bodies active \\nin the conformity assessment procedures of AI systems pursuant to this Regulation \\nare put in place and properly operated in the form of a sectoral group of notified \\nbodies . \\n2. Member States shall ensure that the bodies notified by them participate in the work \\nof that group, directly or by means of designated representatives.  EN 63  EN Article 39  \\nConformity assessment bodies of third countries  \\nConformity assessment bodies established under the law of a third country with which the \\nUnion has concluded an agreement may be authorised to carry out the activities of notified \\nBodies under this Regulation.  \\nCHAPTER 5 \\nSTANDARDS,  CONFORMITY  ASSESSMENT,  CERTIFICATES,  REGISTRATION  \\nArticle 40  \\nHarmonised standards  \\nHigh -risk AI systems which are in conformity with harmonised standards or parts thereof the \\nreferences of which have been published in the Official Journa l of the European Union shall \\nbe presumed to be in conformity with the requirements set out in Chapter 2 of this Title , to the \\nextent those standards cover those requirements.  \\nArticle 41  \\nCommon specifications  \\n1. Where harmonised standards referred to in Ar ticle 40 do not exist or where the \\nCommission considers that the relevant harmonised standards are insufficient or that \\nthere is a need to address specific safety or fundamental right concerns, the \\nCommission may, by means of implementing acts, adopt commo n specifications in \\nrespect of the requirements set out in Chapter 2 of this Title . Those implementing \\nacts shall be adopted in accordance with the examination procedure referred to in \\nArticle 74(2).  \\n2. The Commission, when preparing the common specificati ons referred to in \\nparagraph 1, shall gather the views of relevant bodies or expert groups established \\nunder relevant sectorial Union law.  \\n3. High -risk AI systems which are in conformity with the common specifications \\nreferred to in paragraph 1 shall be pr esumed to be in conformity with the \\nrequirements set out in Chapter 2 of this Title,  to the extent those common \\nspecifications cover those requirements.  \\n4. Where providers do not comply with the common specifications referred to in \\nparagraph 1, they shall duly justify that they have adopted technical solutions that are \\nat least equivalent thereto.  \\nArticle 42  \\nPresumption of conformity with certain requirements  \\n1. Taking into account their intended purpose, high -risk AI systems that have been \\ntrained and test ed on data concerning the specific geographical, behavioural and \\nfunctional setting within which they are intended to be used shall be presumed to be \\nin compliance with the requirement set out in Article 10(4).  EN 64  EN 2. High -risk AI systems that have been certi fied or for which a statement of conformity \\nhas been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 \\nof the European Parliament and of the Council63 and the references of which have \\nbeen published in the Official Journal of the Euro pean Union shall be presumed to be \\nin compliance with the cybersecurity requirements set out in Article 15 of this \\nRegulation in so far as the cybersecurity certificate or statement of conformity or \\nparts thereof cover those requirements.  \\nArticle 43  \\nConfor mity assessment  \\n1. For high -risk AI systems listed in point 1 of Annex III, where, in demonstrating the \\ncompliance of a high -risk AI system with the requirements set out in Chapter 2 of \\nthis Title, the provider has applied harmonised standards referred to in Article 40, or, \\nwhere applicable, common specifications referred to in Article 41, the provider shall \\nfollow one of the following procedures:  \\n(a) the conformity assessment procedure based on internal control referred to in \\nAnnex VI;  \\n(b) the conformity assessment procedure based on assessment of the quality \\nmanagement system and assessment of the technical documentation, with the \\ninvolvement of a notified body, referred to in Annex VII.  \\nWhere, in demonstrating the compliance of a high -risk AI system with  the \\nrequirements set out in Chapter 2 of this Title , the provider has not applied or has \\napplied only in part harmonised standards referred to in Article 40, or where such \\nharmonised standards do not exist and common specifications referred to in Article \\n41 are not available, the provider shall follow the conformity assessment procedure \\nset out in Annex VII.  \\nFor the purpose of the conformity assessment procedure referred to in Annex VII, the \\nprovider may choose any of the notified bodies. However, when the  system is \\nintended to be put into service by law enforcement, immigration or asylum \\nauthorities as well as EU institutions, bodies or agencies, the market surveillance \\nauthority referred to in Article 63(5) or (6), as applicable, shall act as a notified b ody. \\n2. For high -risk AI systems referred to in points 2 to 8 of Annex III, providers shall \\nfollow the conformity assessment procedure based on internal control as referred to \\nin Annex VI, which does not provide for the involvement of a notified body.  For \\nhigh-risk AI systems referred to in point 5(b) of Annex III, placed on the market or \\nput into service by credit institutions regulated by Directive 2013/36/EU, the \\nconformity assessment  shall be carried out as part of the procedure referred to in \\nArticles 97 to101 of that Directive.  \\n3. For high -risk AI systems, to which legal acts listed in Annex II, section A, apply, the \\nprovider shall follow the relevant conformity assessment as required under those \\nlegal acts. The requirements set out in Chapter 2 of thi s Title  shall apply to those \\n                                                 \\n63 Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 April 2019 on ENISA \\n(the European Uni on Agency for Cybersecurity) and on information and communications technology \\ncybersecurity certification and repealing Regulation (EU) No 526/2013 (Cybersecurity Act) (OJ L 151, \\n7.6.2019, p. 1).  EN 65  EN high-risk AI systems and shall be part of that assessment. Points 4.3., 4.4., 4.5. and \\nthe fifth paragraph of point 4.6 of Annex VII shall also apply.  \\nFor the purpose of that assessment, notified bodies which have been notifie d under \\nthose legal acts shall be entitled to control the conformity of the high -risk AI systems \\nwith the requirements set out in Chapter 2 of this Title , provided that the compliance \\nof those notified bodies with requirements laid down in Article 33(4), ( 9) and (10) \\nhas been assessed in the context of the notification procedure under those legal acts.  \\nWhere the legal acts listed in Annex II, section A, enable the manufacturer of the \\nproduct to opt out from a third -party conformity assessment, provided that  that \\nmanufacturer has applied all harmonised standards covering all the relevant \\nrequirements, that manufacturer  may make use of that option only if he has also \\napplied harmonised standards or, where applicable, common specifications referred \\nto in Articl e 41, covering the requirements set out in Chapter 2 of this Title.  \\n4. High -risk AI systems shall undergo a new conformity assessment procedure \\nwhenever they are substantially modified, regardless of whether the modified system \\nis intended to be further distributed or continues to be used by the current user.  \\nFor high -risk AI systems that continue to learn after being placed on the market or \\nput into service, changes to the high -risk AI system and its performance that have \\nbeen pre -determined by the provi der at the moment of the initial conformity \\nassessment and are part of the information contained in the technical documentation \\nreferred to in point 2(f) of Annex IV, shall not constitute a substantial modification.  \\n5. The Commission is empowered to adopt delegated acts in accordance with Article 73 \\nfor the purpose of updating Annexes VI and Annex VII in order to introduce \\nelements of the conformity assessment procedures that become necessary in light of \\ntechnical progress.  \\n6. The Commission is empowered to  adopt delegated acts to amend paragraphs 1 and 2 \\nin order to subject high -risk AI systems referred to in points 2 to 8 of Annex III to \\nthe conformity assessment procedure referred to in Annex VII or parts thereof. The \\nCommission shall adopt such delegated  acts taking into account the effectiveness of \\nthe conformity assessment procedure based on internal control referred to in Annex \\nVI in preventing or minimizing the risks to health and safety and protection of \\nfundamental rights posed by such systems as we ll as the availability of adequate \\ncapacities and resources among notified bodies.  \\nArticle 44  \\nCertificates  \\n1. Certificates issued by notified bodies in accordance with Annex VII shall be drawn -\\nup in an official Union language determined by the Member State  in which the \\nnotified body is established or in an official Union language otherwise acceptable to \\nthe notified body.  \\n2. Certificates shall be valid for the period they indicate, which shall not exceed five \\nyears. On application by the provider, the vali dity of a certificate may be extended \\nfor further periods, each not exceeding five years, based on a re -assessment in \\naccordance with the applicable conformity assessment procedures.  \\n3. Where a notified body finds that an AI system no longer meets the requirements set \\nout in Chapter 2 of this Title , it shall, taking account of the principle of EN 66  EN proportionality, suspend or withdraw the certificate issued or impose any restrictions \\non it, unless compliance with those requirements is ensured by appropriate cor rective \\naction taken by the provider of the system within an appropriate deadline set by the \\nnotified body. The notified body shall give reasons for its decision.  \\nArticle 45  \\nAppeal against decisions of notified bodies  \\nMember States shall ensure that an app eal procedure against decisions of the notified bodies \\nis available to parties having a legitimate interest in that decision . \\nArticle 46  \\nInformation obligations of notified bodies  \\n1. Notified bodies shall inform the notifying authority of the following:  \\n(a) any Union technical documentation assessment certificates, any supplements to \\nthose certificates, quality management system approvals issued in accordance \\nwith the requirements of Annex VII;  \\n(b) any refusal, restriction, suspension or withdrawal of a Un ion technical \\ndocumentation assessment certificate or a quality management system approval \\nissued in accordance with the requirements of Annex VII;  \\n(c) any circumstances affecting the scope of or conditions for notification;  \\n(d) any request for informatio n which they have received from market surveillance \\nauthorities regarding conformity assessment activities;  \\n(e) on request, conformity assessment activities performed within the scope of \\ntheir notification and any other activity performed, including cross -border \\nactivities and subcontracting.  \\n2. Each notified body shall inform the other notified bodies of:  \\n(a) quality management system approvals which it has refused, suspended or \\nwithdrawn, and, upon request, of quality system approvals which it has issued;  \\n(b) EU technical documentation assessment certificates or any supplements thereto \\nwhich it has refused, withdrawn, suspended or otherwise restricted, and, upon \\nrequest, of the certificates and/or supplements thereto which it has issued.  \\n3. Each notified body shall provide the other notified bodies carrying out similar \\nconformity assessment activities covering the same artificial intelligence \\ntechnologies with relevant information on issues relating to negative and, on request, \\npositive confo rmity assessment results.  \\nArticle 47  \\nDerogation from conformity assessment procedure  \\n1. By way of derogation from Article 43, any market surveillance authority may \\nauthorise the placing on the market or putting into service of specific high -risk AI \\nsystems  within the territory of the Member State concerned, for exceptional reasons \\nof public security or the protection of life and health of persons, environmental \\nprotection and the protection of key industrial and infrastructural assets. That \\nauthorisation sh all be for a limited period of time, while the necessary conformity EN 67  EN assessment procedures are being carried out, and shall terminate once those \\nprocedures have been completed.  The completion of those procedures shall be \\nundertaken without undue delay.  \\n2. The authorisation referred to in paragraph 1 shall be issued only if the market \\nsurveillance authority concludes that the high -risk AI system complies with the \\nrequirements of Chapter 2 of this Title. The market surveillance authority shall \\ninform the Commi ssion and the other Member States of any authorisation issued \\npursuant to paragraph 1.  \\n3. Where, within 15 calendar days of receipt of the information referred to in paragraph \\n2, no objection has been raised by either a Member State or the Commission in \\nrespect of an authorisation issued by a market surveillance authority of a Member \\nState in accordance with paragraph 1, that authorisation shall be deemed justified.  \\n4. Where, within 15 calendar days of receipt of the notification referred to in paragraph \\n2, objections are raised by a Member State against an authorisation issued by a \\nmarket surveillance authority of another Member State, or where the Commission \\nconsiders the authorisation to be contrary to Union law or the conclusion of the \\nMember States rega rding the compliance of the system as referred to in paragraph 2 \\nto be unfounded, the Commission shall without delay enter into consultation with the \\nrelevant Member State; the operator(s) concerned shall be consulted and have the \\npossibility to present th eir views. In view thereof, the Commission shall decide \\nwhether the authorisation is justified or not. The Commission shall address its \\ndecision to the Member State concerned and the relevant operator or operators.  \\n5. If the authorisation is considered unj ustified, this shall be withdrawn by the market \\nsurveillance authority of the Member State concerned.  \\n6. By way of derogation from paragraphs 1 to 5, for high -risk AI systems intended to be \\nused as safety components of devices, or which are themselves devi ces, covered by \\nRegulation (EU) 2017/745 and Regulation (EU) 2017/746, Article 59 of Regulation \\n(EU) 2017/745 and Article 54 of Regulation (EU) 2017/746 shall apply also with \\nregard to the derogation from the conformity assessment of the compliance with th e \\nrequirements set out in Chapter 2 of this Title.  \\nArticle 48  \\nEU declaration of conformity  \\n1. The provider shall draw up a written EU declaration of conformity for each AI \\nsystem and keep it at the disposal of the national competent authorities for 10 years \\nafter the AI system has been placed on the market or put into service. The EU \\ndeclaration of co nformity shall identify the AI system for which it has been drawn \\nup. A copy of the EU declaration of conformity shall be given to the relevant \\nnational competent authorities upon request.  \\n2. The EU declaration of conformity shall state that the high -risk AI system in question \\nmeets the requirements set out in Chapter 2 of this Title . The EU declaration of \\nconformity shall contain the information set out in Annex V and shall be translated \\ninto an official Union language or languages required by the Member S tate(s) in \\nwhich the high -risk AI system is made available.  \\n3. Where high -risk AI systems are subject to other Union harmonisation legislation \\nwhich also requires an EU declaration of conformity, a single EU declaration of \\nconformity shall be drawn up in respect of all Union legislations applicable to the EN 68  EN high-risk AI system. The declaration shall contain all the information required for \\nidentification of the Union harmonisation legislation to which the declaration relates.  \\n4. By drawing up the EU declara tion of conformity, the provider shall assume \\nresponsibility for compliance with the requirements set out in Chapter 2 of this Title . \\nThe provider shall keep the EU declaration of conformity up -to-date as appropriate.  \\n5. The Commission shall be empowered t o adopt delegated acts in accordance with \\nArticle 73 for the purpose of updating the content of the EU declaration of \\nconformity set out in Annex V in order to introduce elements that become necessary \\nin light of technical progress.  \\nArticle 49  \\nCE marking o f conformity  \\n1. The CE marking shall be affixed visibly, legibly and indelibly for high -risk AI \\nsystems. Where that is not possible or not warranted on account of the nature of the \\nhigh-risk AI system, it shall be affixed to the packaging or to the accompa nying \\ndocumentation, as appropriate.  \\n2. The CE marking referred to in paragraph 1 of this Article shall be subject to the \\ngeneral principles set out in Article 30 of Regulation (EC) No 765/2008.  \\n3. Where applicable, the CE marking shall be followed by the  identification number of \\nthe notified body responsible for the conformity assessment procedures set out in \\nArticle 43. The identification number shall also be indicated in any promotional \\nmaterial which mentions that the high -risk AI system fulfils the re quirements for CE \\nmarking.  \\nArticle 50  \\nDocument retention  \\nThe provider shall, for a period ending 10 years after the AI system has been placed on the \\nmarket or put into service, keep at the disposal of the national competent authorities:  \\n(a) the technical documentation referred to in Article 11;  \\n(b) the documentation concerning the quality management system referred to Article 17;  \\n(c) the documentation concerning the changes approved by notified bodies where \\napplicable;  \\n(d) the decisions and other documen ts issued by the notified bodies where applicable;  \\n(e) the EU declaration of conformity referred to in Article 48.  \\nArticle 51  \\nRegistration  \\nBefore placing on the market or putting into service a high -risk AI system referred to in \\nArticle 6(2), the provider  or, where applicable, the authorised representative shall register that \\nsystem in the EU database referred to in Article 60.  EN 69  EN TITLE  IV \\nTRANSPARENCY  OBLIGATIONS  FOR  CERTAIN  AI SYSTEMS  \\nArticle 52  \\nTransparency obligations for certain AI systems  \\n1. Providers shall ensure that AI systems intended to interact with natural persons are \\ndesigned and developed in such a way that natural persons are informed that they are \\ninteracting with an AI system, unless this is obvious from the circumstances and the \\ncontext of use. This obligation shall not apply to AI systems authorised by law to \\ndetect, prevent, investigate and prosecute criminal offences, unless those systems are \\navailable for the public to report a criminal offence.  \\n2. Users of an emotion recognition system or a biometric categorisation system shall \\ninform of the operation of the system the natural persons exposed thereto. This \\nobligation shall not apply to AI systems used for biometric categorisation, which are \\npermitted by law to detect, prevent and investi gate criminal offences.  \\n3. Users of an AI system that generates or manipulates image, audio or video content \\nthat appreciably resembles  existing persons, objects, places or other entities or events  \\nand would falsely appear to a person to be authentic or tr uthful (‘deep fake’), shall \\ndisclose  that the content has been artificially generated or manipulated.  \\nHowever, the first subparagraph shall not apply where the use is authorised by law to \\ndetect, prevent, investigate and prosecute criminal offences  or it is necessary for the \\nexercise of the right to freedom of expression and the right to freedom of the arts and \\nsciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to \\nappropriate safeguards for the rights and freedoms of third part ies. \\n4. Paragraphs 1, 2 and 3 shall not affect the requirements and obligations set out in Title \\nIII of this Regulation.  \\nTITLE V  \\nMEASURES  IN SUPPORT  OF INNOVATION  \\nArticle 53  \\nAI regulatory sandboxes  \\n1. AI regulatory sandboxes established by one or more Member States competent \\nauthorities or the European Data Protection Supervisor shall provide a controlled \\nenvironment that facilitates the development, testing and validation of innovative AI \\nsystems for a  limited time before their placement on the market or putting into \\nservice  pursuant to a specific plan. This shall take place under the direct supervision \\nand guidance by the competent authorities  with a view to ensuring compliance with \\nthe requirements of  this Regulation and, where relevant, other Union and Member \\nStates legislation supervised within the sandbox.  \\n2. Member States shall ensure that to the extent the innovative AI systems involve the \\nprocessing of personal data or otherwise fall under the s upervisory remit of other \\nnational authorities or competent authorities providing or supporting access to data, EN 70  EN the national data protection authorities and those other national authorities are \\nassociated to the operation of the AI regulatory sandbox.  \\n3. The AI regulatory sandboxes shall not affect the supervisory and corrective powers \\nof the competent authorities. Any significant risks to health and safety and \\nfundamental rights identified during the development and testing of such systems \\nshall result in immediate mitigation and, failing that, in the suspension of the \\ndevelopment and testing process until such mitigation takes place.  \\n4. Participants in the AI regulatory sandbox shall remain liable under applicable Union \\nand Member States liability legislat ion for any harm inflicted on third parties as a \\nresult from the experimentation taking place in the sandbox.  \\n5. Member States’ competent authorities that have established AI regulatory sandboxes \\nshall coordinate their activities and cooperate within the f ramework of the European \\nArtificial Intelligence Board. They shall submit annual reports to the Board and the \\nCommission on the results from the implementation of those scheme, including good \\npractices, lessons learnt and recommendations on their setup and , where relevant, on \\nthe application of this Regulation and other Union legislation supervised within the \\nsandbox.  \\n6. The modalities and the conditions of the operation of the AI regulatory sandboxes, \\nincluding the eligibility criteria and the procedure f or the application, selection, \\nparticipation and exiting from the sandbox, and the rights and obligations of the \\nparticipants shall be set out in implementing acts. Those implementing acts shall be \\nadopted in accordance with the examination procedure refer red to in Article 74(2).  \\nArticle 54  \\nFurther processing of personal data for developing certain AI systems in the public interest in \\nthe AI regulatory sandbox  \\n1. In the AI regulatory sandbox personal data lawfully collected for other purposes shall \\nbe proce ssed for the purposes of developing and testing certain innovative AI \\nsystems in the sandbox under the following conditions:  \\n(a) the innovative AI systems shall be developed for safeguarding substantial \\npublic interest in one or more of the following areas : \\n(i) the prevention, investigation, detection or prosecution of criminal \\noffences or the execution of criminal penalties, including the \\nsafeguarding against and the prevention of threats to public security, \\nunder the control and responsibility of the comp etent authorities. The \\nprocessing shall be based on Member State or Union law;  \\n(ii) public safety and public health, including disease prevention, control and \\ntreatment;  \\n(iii) a high level of protection and improvement of the quality of the \\nenvironment;  \\n(b) the data processed are necessary for complying with one or more of the \\nrequirements referred to in Title III, Chapter 2 where those requirements \\ncannot be effectively fulfilled by processing anonymised, synthetic or other \\nnon-personal data;  EN 71  EN (c) there a re effective monitoring mechanisms to identify if any high risks to the \\nfundamental rights of the data subjects may arise during the sandbox \\nexperimentation as well as response mechanism to promptly mitigate those \\nrisks and, where necessary, stop the proce ssing;  \\n(d) any personal data to be processed in the context of the sandbox are in a \\nfunctionally separate, isolated and protected data processing environment \\nunder the control of the participants and only authorised persons have access to \\nthat data;  \\n(e) any personal data processed are not be transmitted, transferred or otherwise \\naccessed by other parties;  \\n(f) any processing of personal data in the context of the sandbox do not lead to \\nmeasures or decisions affecting the data subjects;  \\n(g) any personal da ta processed in the context of the sandbox are deleted once the \\nparticipation in the sandbox has terminated or the personal data has reached the \\nend of its retention period;  \\n(h) the logs of the processing of personal data in the context of the sandbox are  \\nkept for the duration of the participation in the sandbox and 1 year after its \\ntermination, solely for the purpose of and only as long as necessary for \\nfulfilling accountability and documentation obligations under this Article or \\nother application Union o r Member States legislation;  \\n(i) complete and detailed description of the process and rationale behind the \\ntraining, testing and validation of the AI system is kept together with the \\ntesting results as part of the technical documentation in Annex IV;  \\n(j) a short summary of the AI project developed in the sandbox, its objectives and \\nexpected results published on the website of the competent authorities.  \\n2. Paragraph 1 is without prejudice to Union or Member States legislation excluding \\nprocessing for other p urposes than those explicitly mentioned in that legislation.  \\nArticle 55  \\nMeasures for small -scale providers and users  \\n1. Member States shall undertake the following actions:  \\n(a) provide small -scale providers and start -ups with priority access to the AI \\nregulatory sandboxes to the extent that they fulfil the eligibility conditions;  \\n(b) organise specific awareness raising activities about the application of this \\nRegulation tailored to the needs of the small -scale providers and users ; \\n(c) where appropriate, establish a dedicated channel for communication with \\nsmall -scale providers and user and other innovators to provide guidance and \\nrespond to queries about the implementation of this Regulation.  \\n2. The specific interests and needs of the s mall-scale providers shall be taken into \\naccount when setting the fees for conformity assessment under Article 43, reducing \\nthose fees proportionately to their size and market size.  EN 72  EN TITLE  VI \\nGOVERNANCE  \\nCHAPTER 1 \\nEUROPEAN ARTIFICIAL INTELLIGENCE BOARD  \\nArtic le 56  \\nEstablishment of the European Artificial Intelligence Board  \\n1. A ‘European Artificial Intelligence Board’ (the ‘Board’) is established.  \\n2. The Board shall provide advice and assistance to the Commission in order to:  \\n(a) contribute to the effective co operation of the national supervisory authorities \\nand the Commission with regard to matters covered by this Regulation;  \\n(b) coordinate and contribute to guidance and analysis by the Commission and the \\nnational supervisory authorities and other competent au thorities on emerging \\nissues across the internal market with regard to matters covered by this \\nRegulation;  \\n(c) assist the national supervisory authorities and the Commission in ensuring the \\nconsistent application of this Regulation.  \\nArticle 57  \\nStructure of the Board  \\n1. The Board shall be composed of the national supervisory authorities, who shall be \\nrepresented by the head or equivalent high -level official of that authority, and the \\nEuropean Data Protection Supervisor. Other national authoritie s may be invited to \\nthe meetings, where the issues discussed are of relevance for them.  \\n2. The Board shall adopt its rules of procedure by a simple majority of its members, \\nfollowing the consent of the Commission. The rules of procedure shall also contain \\nthe operational aspects related to the execution of the Board’s tasks as listed in \\nArticle 58. The Board may establish sub -groups as appropriate  for the purpose of \\nexamining specific questions.  \\n3. The Board shall be chaired by the Commission. The Commissio n shall convene the \\nmeetings and prepare the agenda in accordance with the tasks of the Board pursuant \\nto this Regulation and with its rules of procedure. The Commission shall provide \\nadministrative and analytical support for the activities of the Board pu rsuant to this \\nRegulation.  \\n4. The Board may invite external experts and observers to attend its meetings and may \\nhold exchanges with interested third parties to inform its activities to an appropriate \\nextent. To that end the Commission may facilitate excha nges between the Board and \\nother Union bodies, offices, agencies and advisory groups.  EN 73  EN Article 58  \\nTasks of the Board  \\nWhen providing advice and assistance to the Commission in the context of Article 56(2), the \\nBoard shall  in particular:  \\n(a) collect and share  expertise and best practices among Member States;  \\n(b) contribute to uniform administrative practices in the Member States, including for \\nthe functioning of regulatory sandboxes referred to in Article 53;  \\n(c) issue opinions, recommendations or written cont ributions on matters related to the \\nimplementation of this Regulation, in particular  \\n(i) on technical specifications or existing standards regarding the requirements set \\nout in Title III, Chapter 2 ,  \\n(ii) on the use of harmonised standards or common specif ications referred to in \\nArticles 40 and 41,  \\n(iii) on the preparation of guidance documents, including the guidelines concerning \\nthe setting of administrative fines referred to in Article 71.  \\nCHAPTER  2 \\nNATIONAL COMPETENT A UTHORITIES  \\nArticle 59  \\nDesignation o f national competent authorities  \\n1. National competent authorities shall be established or designated by each Member \\nState for the purpose of ensuring the application and implementation of this \\nRegulation. National competent authorities shall be organised  so as to safeguard the \\nobjectivity and impartiality of their activities and tasks.  \\n2. Each Member State shall designate a national supervisory authority among the \\nnational competent authorities.  The national supervisory authority shall act as \\nnotifying au thority and market surveillance authority unless a Member State has \\norganisational and administrative reasons to designate more than one authority.  \\n3. Member States shall inform the Commission of their designation or designations and, \\nwhere applicable, the  reasons for designating more than one authority.  \\n4. Member States shall ensure that national competent authorities are provided with \\nadequate financial and human resources to fulfil their tasks under this Regulation. In \\nparticular, national competent aut horities shall have a sufficient number of personnel \\npermanently available whose competences and expertise shall include an in -depth \\nunderstanding of artificial intelligence technologies, data and data computing, \\nfundamental rights, health and safety risks  and knowledge of existing standards and \\nlegal requirements.  \\n5. Member States shall report to the Commission on an annual basis on the status of the \\nfinancial and human resources of the national competent authorities with an \\nassessment of their adequacy. The Commission shall transmit that information to the \\nBoard for discussion and possible recommendations.  \\n6. The Commission shall facilitate the exchange of experience between national \\ncompetent authorities.  EN 74  EN 7. National competent authorities may provide gu idance and advice on the \\nimplementation of this Regulation, including to small -scale providers. Whenever \\nnational competent authorities intend to provide guidance and advice with regard to \\nan AI system in areas covered by other Union legislation, the compe tent national \\nauthorities under that Union legislation shall be consulted, as appropriate. Member \\nStates may also establish one central contact point for communication with operators.  \\n8. When Union institutions, agencies and bodies fall within the scope of  this \\nRegulation, the European Data Protection Supervisor shall act as the competent \\nauthority for their supervision.  \\nTITLE VII  \\nEU DATABASE  FOR  STAND -ALONE  HIGH -RISK  AI SYSTEMS  \\nArticle 60  \\nEU database for stand -alone high -risk AI systems  \\n1. The Commission s hall, in collaboration with the Member States, set up and maintain \\na EU database containing information referred to in paragraph 2 concerning high -risk \\nAI systems referred to in Article 6(2) which are registered in accordance with Article \\n51. \\n2. The data listed in Annex VIII shall be entered into the EU database by the providers.  \\nThe Commission shall provide them with technical and administrative support.  \\n3. Information contained in the EU database shall be accessible to the public.  \\n4. The EU database sha ll contain personal data only insofar as necessary for collecting \\nand processing information in accordance with this Regulation. That information \\nshall include the names and contact details of natural persons who are responsible for \\nregistering the system and have the legal authority to represent the provider.  \\n5. The Commission shall be the controller of the EU database. It shall also ensure to \\nproviders adequate technical and administrative support.  \\nTITLE  VIII \\nPOST -MARKET  MONITORING,  INFORMATION  SHARING,  MARKET  \\nSURVEILLANCE  \\nCHAPTER 1 \\nPOST-MARKET MONITORING  \\nArticle 61  \\nPost-market monitoring by providers and post -market monitoring plan for high -risk AI \\nsystems  \\n1. Providers shall establish and document a post -market monitoring system in a manner \\nthat is propor tionate to the nature of the artificial intelligence technologies and the \\nrisks of the high -risk AI system.  EN 75  EN 2. The post -market monitoring system shall actively and systematically collect, \\ndocument and analyse relevant data provided by users or collected th rough other \\nsources on the performance of high -risk AI systems throughout their lifetime, and \\nallow the provider to evaluate the continuous compliance of AI systems with the \\nrequirements set out in Title III, Chapter 2 . \\n3. The post -market monitoring system  shall be based on a post -market monitoring plan. \\nThe post -market monitoring plan shall be part of the technical documentation \\nreferred to in Annex IV. The Commission shall adopt an implementing act laying \\ndown detailed provisions establishing a template f or the post -market monitoring plan \\nand the list of elements to be included in the plan.  \\n4. For high -risk AI systems covered by the legal acts referred to in Annex II, where a \\npost-market monitoring system and plan is already established under that legislat ion, \\nthe elements described in paragraphs 1, 2 and 3 shall be integrated into that system \\nand plan as appropriate.  \\nThe first subparagraph shall also apply to high -risk AI systems referred to in point \\n5(b) of Annex III placed on the market or put into servi ce by credit institutions \\nregulated by Directive 2013/36/EU.  \\nCHAPTER 2 \\nSHARING OF INFORMATIO N ON INCIDENTS AND M ALFUNCTIONING  \\nArticle 62  \\nReporting of serious incidents and of malfunctioning  \\n1. Providers of high -risk AI systems placed on the Union market shall report any \\nserious incident or any malfunctioning of those systems which constitutes a breach of \\nobligations under Union law intended to protect fundamental rights to the market \\nsurveillanc e authorities of the Member States where that incident or breach occurred.  \\nSuch notification shall be made immediately after the provider has established a \\ncausal link between the AI system and the incident or malfunctioning or the \\nreasonable likelihood o f such a link, and, in any event, not later than 15 days after the \\nproviders  becomes aware of the serious incident or of the malfunctioning.  \\n2. Upon receiving a notification related to a breach of obligations under Union law \\nintended to protect fundamental  rights, the market surveillance authority shall inform \\nthe national public authorities or bodies referred to in Article 64(3). The Commission \\nshall develop dedicated guidance to facilitate compliance with the obligations set out \\nin paragraph 1. That guida nce shall be issued 12 months after the entry into force of \\nthis Regulation, at the latest.  \\n3. For high -risk AI systems referred to in point 5(b) of Annex III which are placed on \\nthe market or put into service by providers that are credit institutions regu lated by \\nDirective 2013/36/EU and for high -risk AI systems which are safety components of \\ndevices, or are themselves devices, covered by Regulation (EU) 2017/745 and \\nRegulation (EU) 2017/746, the notification of serious incidents or malfunctioning \\nshall be  limited to those that that constitute a breach of obligations under Union law \\nintended to protect fundamental rights.  EN 76  EN CHAPTER 3 \\nENFORCEMENT  \\nArticle 63  \\nMarket surveillance and control of AI systems in the Union market  \\n1. Regulation (EU) 2019/1020 shall ap ply to AI systems covered by this Regulation. \\nHowever, for the purpose of the effective enforcement of this Regulation:  \\n(a) any reference to an economic operator under Regulation (EU) 2019/1020 shall \\nbe understood as including all operators identified in T itle III, Chapter 3 of this \\nRegulation;  \\n(b) any reference to a product under Regulation (EU) 2019/1020 shall be \\nunderstood as including all AI systems falling within the scope of this \\nRegulation.  \\n2. The national supervisory authority shall report to the Commission on a regular basis \\nthe outcomes of relevant market surveillance activities. The national supervisory \\nauthority shall report, without delay, to the Commission and relevant national \\ncompetition authorities any information identified in the course of market \\nsurveillance activities that may be of potential interest for the application of Union \\nlaw on competition rules.  \\n3. For high -risk AI systems, related to products to which legal acts listed in Annex II, \\nsection A apply, the market surveillance aut hority for the purposes of this Regulation \\nshall be the authority responsible for market surveillance activities designated under \\nthose legal acts.  \\n4. For AI systems placed on the market, put into service or used by financial institutions \\nregulated by Unio n legislation on financial services, the market surveillance \\nauthority for the purposes of this Regulation shall be the relevant authority \\nresponsible for the financial supervision of those institutions under that legislation.  \\n5. For AI systems listed in p oint 1(a) in so far as the systems are used for law \\nenforcement purposes, points 6 and 7  of Annex III, Member States shall designate as \\nmarket surveillance authorities for the purposes of this Regulation either the \\ncompetent data protection supervisory aut horities under Directive (EU) 2016/680, or \\nRegulation 2016/679 or the national competent authorities supervising the activities \\nof the law enforcement, immigration or asylum authorities putting into service or \\nusing those systems.  \\n6. Where Union institutio ns, agencies and bodies fall within the scope of this \\nRegulation, the European Data Protection Supervisor shall act as their market \\nsurveillance authority.  \\n7. Member States shall facilitate the coordination between market surveillance \\nauthorities designate d under this Regulation and other relevant national authorities or \\nbodies which supervise the application of Union harmonisation legislation listed in \\nAnnex II  or other Union legislation that might be relevant for the high -risk AI \\nsystems referred to in An nex III . EN 77  EN Article 64  \\nAccess to data and documentation  \\n1. Access to data and documentation i n the context of their activities, t he market \\nsurveillance authorities shall be granted full access to the training, validation and \\ntesting datasets used by the provi der, including through application programming \\ninterfaces (‘API’) or other appropriate technical means and tools enabling remote \\naccess.  \\n2. Where necessary to assess the conformity of the high -risk AI system with the \\nrequirements set out in Title III, Chap ter 2 and upon a reasoned request, the market \\nsurveillance authorities shall be granted access to the source code of the AI system . \\n3. National public authorities or bodies which supervise or enforce the respect of \\nobligations under Union law protecting fu ndamental rights in relation to the use of \\nhigh-risk AI systems referred to in Annex III shall have the power to request and \\naccess any documentation created or maintained under this Regulation when access \\nto that documentation is necessary for the fulfilm ent of the competences under their \\nmandate within the limits of their jurisdiction. The relevant public authority or body \\nshall inform the market surveillance authority of the Member State concerned of any \\nsuch request.  \\n4. By 3 months after the entering in to force of this Regulation, each Member State shall \\nidentify the public authorities or bodies referred to in paragraph 3 and make a list \\npublicly available on the website of the national supervisory authority. Member \\nStates shall notify the list to the Co mmission and all other Member States and keep \\nthe list up to date.  \\n5. Where the documentation referred to in paragraph 3 is insufficient to ascertain \\nwhether a breach of obligations under Union law intended to protect fundamental \\nrights has occurred, the public authority or body referred to paragraph 3 may make a \\nreasoned request to the market surveillance authority to organise testing of the high -\\nrisk AI system through technical means. The market surveillance authority shall \\norganise the testing with the close involvement of the requesting public authority or \\nbody within reasonable time following the request.  \\n6. Any information and documentation obtained by the national public authorities or \\nbodies referred to in paragraph 3 pursuant to the provisions of this Article shall be \\ntreated in compliance with the confidentiality obligations set out in Article 70.  \\nArticle 65  \\nProcedure for dealing with AI systems presenting a risk at national level  \\n1. AI systems presenting a risk shall be understood as a product pr esenting a risk \\ndefined in Article 3, point 19 of Regulation (EU) 2019/1020 insofar as risks to the \\nhealth or safety or to the protection of fundamental rights of persons are concerned.  \\n2. Where the market surveillance authority of a Member State has suffi cient reasons to \\nconsider that an AI system presents a risk as referred to in paragraph 1, they shall \\ncarry out an evaluation of the AI system concerned in respect of its compliance with \\nall the requirements and obligations laid down in this Regulation. Wh en risks to the \\nprotection of fundamental rights are present, the market surveillance authority shall \\nalso inform the relevant national public authorities or bodies referred to in Article \\n64(3).  The relevant operators shall cooperate as necessary with the market EN 78  EN surveillance authorities and the other national public authorities or bodies referred to \\nin Article 64(3).  \\nWhere, in the course of that evaluation, the market surveillance authority finds that \\nthe AI system does not comply with the requirements and obligations laid down in \\nthis Regulation, it shall without delay require the relevant operator to take all \\nappropriate corrective actions to bring the AI system into compliance, to withdraw \\nthe AI system from the market, or to recall it within a reasonable  period, \\ncommensurate with the nature of the risk, as it may prescribe.  \\nThe market surveillance authority shall inform the relevant notified body \\naccordingly. Article 18 of Regulation (EU) 2019/1020 shall apply to the measures \\nreferred to in the second subparagraph.  \\n3. Where the market surveillance authority considers that non -compliance is not \\nrestricted to its national territory, it shall inform the Commission and the other \\nMember States of the results of the evaluation and of the actions which it has \\nrequired the operator to take.  \\n4. The operator shall ensure that all appropriate corrective action is taken in respect of \\nall the AI systems concerned that it has made available on the market throughout the \\nUnion.  \\n5. Where the operator of an AI system does  not take adequate corrective action within \\nthe period referred to in paragraph 2, the market surveillance authority shall take all \\nappropriate provisional measures to prohibit or restrict the AI system\\'s being made \\navailable on its national market, to wit hdraw the product from that market or to recall \\nit. That authority shall inform the Commission and the other Member States, without \\ndelay, of those measures.  \\n6. The information referred to in paragraph 5 shall include all available details, in \\nparticular t he data necessary for the identification of the non -compliant AI system, \\nthe origin of the AI system, the nature of the non -compliance alleged and the risk \\ninvolved, the nature and duration of the national measures taken and the arguments \\nput forward by th e relevant operator. In particular, the market surveillance authorities \\nshall indicate whether the non -compliance is due to one or more of the following:  \\n(a) a failure of the AI system to meet requirements set out in Title III, Chapter 2;  \\n(b) shortcomings  in the harmonised standards or common specifications referred to \\nin Articles 40 and 41 conferring a presumption of conformity.  \\n7. The market surveillance authorities of the Member States other than the market \\nsurveillance authority of the Member State ini tiating the procedure shall without \\ndelay inform the Commission and the other Member States of any measures adopted \\nand of any additional information at their disposal relating to the non -compliance of \\nthe AI system concerned, and, in the event of disagree ment with the notified national \\nmeasure, of their objections.  \\n8. Where, within three months of receipt of the information referred to in paragraph 5, \\nno objection has been raised by either a Member State or the Commission in respect \\nof a provisional measure taken by a Member State, that measure shall be deemed \\njustified . This is without prejudice to the procedural rights of the concerned operator \\nin accordance with Article 18 of Regulation (EU) 2019/1020.  EN 79  EN 9. The market surveillance authorities of all Member States shall ensure that \\nappropriate restrictive measures are t aken in respect of the product concerned, such \\nas withdrawal of the product from their market, without delay.  \\nArticle 66  \\nUnion safeguard procedure  \\n1. Where, within three months of receipt of the notification referred to in Article 65(5), \\nobjections are rai sed by a Member State against a measure taken by another Member \\nState, or where the Commission considers the measure to be contrary to Union law, \\nthe Commission shall without delay enter into consultation with the relevant Member \\nState and operator or oper ators and shall evaluate the national measure. On the basis \\nof the results of that evaluation, the Commission shall decide whether the national \\nmeasure is justified or not within 9 months from the notification referred to in Article \\n65(5) and notify such d ecision to the Member State concerned.  \\n2. If the national measure is considered justified, all Member States shall take the \\nmeasures necessary to ensure that the non -compliant AI system is withdrawn from \\ntheir market, and shall inform the Commission accor dingly. If the national measure \\nis considered unjustified, the Member State concerned shall withdraw the measure.  \\n3. Where the national measure is considered justified and the non -compliance of the AI \\nsystem is attributed to shortcomings in the harmonised standards or common \\nspecifications referred to in Articles 40 and 41 of this Regulation, the Commission \\nshall apply the procedure provided for in Article 11 of Regulation (EU) No \\n1025/2012.  \\nArticle 67  \\nCompliant AI systems which present a risk  \\n1. Where, hav ing performed an evaluation under Article 65, the market surveillance \\nauthority of a Member State finds that although an AI system is in compliance with \\nthis Regulation, it presents a risk to the health or safety of persons, to the compliance \\nwith obligati ons under Union or national law intended to protect fundamental rights \\nor to other aspects of public interest protection, it shall require the relevant operator \\nto take all appropriate measures to ensure that the AI system concerned, when placed \\non the mar ket or put into service, no longer presents that risk, to withdraw the AI \\nsystem from the market or to recall it within a reasonable period, commensurate with \\nthe nature of the risk, as it may prescribe.  \\n2. The provider or other relevant operators shall en sure that corrective action is taken in \\nrespect of all the AI systems concerned that they have made available on the market \\nthroughout the Union within the timeline prescribed by the market surveillance \\nauthority of the Member State referred to in paragrap h 1. \\n3. The Member State shall immediately inform the Commission and the other Member \\nStates. That information shall include all available details, in particular the data \\nnecessary for the identification of the AI system concerned, the origin and the suppl y \\nchain of the AI system, the nature of the risk involved and the nature and duration of \\nthe national measures taken.  \\n4. The Commission shall without delay enter into consultation with the Member States \\nand the relevant operator and shall evaluate the nati onal measures taken. On the basis EN 80  EN of the results of that evaluation, the Commission shall decide whether the measure is \\njustified or not and, where necessary, propose appropriate measures.  \\n5. The Commission shall address its decision to the Member States.  \\nArticle 68  \\nFormal non -compliance  \\n1. Where the market surveillance authority of a Member State makes one of the \\nfollowing findings, it shall require the relevant provider to put an end to the non -\\ncompliance concerned:  \\n(a) the conformity marking has been aff ixed in violation of Article 49;  \\n(b) the conformity marking has not been affixed;  \\n(c) the EU declaration of conformity has not been drawn up;  \\n(d) the EU declaration of conformity has not been drawn up correctly;  \\n(e) the identification number of the notifie d body, which is involved in the \\nconformity assessment procedure, where applicable, has not been affixed;  \\n2. Where the non -compliance referred to in paragraph 1 persists, the Member State \\nconcerned shall take all appropriate measures to restrict or prohibi t the high -risk AI \\nsystem being made available on the market or ensure that it is recalled or withdrawn \\nfrom the market.  \\nTITLE  IX \\nCODES  OF CONDUCT  \\nArticle 69  \\nCodes of conduct  \\n1. The Commission and the Member States shall encourage and facilitate the drawing \\nup of codes of conduct intended to foster the voluntary application to AI systems \\nother than high -risk AI systems of the requirements set out in Title III, Chapter 2 on \\nthe bas is of technical specifications and solutions that are appropriate means of \\nensuring compliance with such requirements in light of the intended purpose of the \\nsystems.  \\n2. The Commission and the Board shall encourage and facilitate the drawing up of \\ncodes o f conduct intended to foster the voluntary application to AI systems of \\nrequirements related for example to environmental sustainability, accessibility for \\npersons with a disability, stakeholders participation in the design and development of \\nthe AI system s and diversity of development teams on the basis of clear objectives \\nand key performance indicators to measure the achievement of those objectives.  \\n3. Codes of conduct may be drawn up by individual providers of AI systems or by \\norganisations representing them or by both, including with the involvement of users \\nand any interested stakeholders and their representative organisations. Codes of \\nconduct may cover one or more AI systems taking into account the similarity of the \\nintended purpose of the relevant sy stems.  EN 81  EN 4. The Commission and the Board shall take into account the specific interests and \\nneeds of the small -scale providers and start -ups when encouraging and facilitating \\nthe drawing up of codes of conduct.  \\nTITLE  X \\nCONFIDENTIALITY  AND  PENALTIES   \\nArticle 70 \\nConfidentiality  \\n1. National competent authorities and notified bodies involved in the application of this \\nRegulation shall respect the confidentiality of information and data obtained in \\ncarrying out their tasks and activities in such a manner as to pro tect, in particular:  \\n(a) intellectual property rights, and confidential business information or trade \\nsecrets of a natural or legal person, including source code, except the cases \\nreferred to in Article 5 of Directive 2016/943 on the protection of undisclosed \\nknow -how and business  information (trade secrets) against their unlawful \\nacquisition, use and disclosure apply.  \\n(b) the effective implementation of this Regulation, in particular for the purpose of \\ninspections, investigations or audits; (c) public and national security interes ts;  \\n(c) integrity of criminal or administrative  proceedings.  \\n2. Without prejudice to paragraph 1, information exchanged on a confidential basis \\nbetween the national competent authorities and between national competent \\nauthorities and the Commission shall not be disclosed without the prior consultation \\nof the originating national competent authority and the user when high -risk AI \\nsystems referred to in points 1, 6 and 7 of Annex III are used by law enforcement, \\nimmigration or asylum authorities, when such d isclosure would jeopardise public and \\nnational security interests.  \\nWhen the law enforcement, immigration or asylum authorities are providers of high -\\nrisk AI systems referred to in points 1, 6 and 7 of Annex III, the technical \\ndocumentation referred to in A nnex IV shall remain within the premises of those \\nauthorities. Those authorities shall ensure that the market surveillance authorities \\nreferred to in Article 63(5) and (6), as applicable, can, upon request, immediately \\naccess the documentation or obtain a copy thereof. Only staff of the market \\nsurveillance authority holding the appropriate level of security clearance shall be \\nallowed to access that documentation or any copy thereof.  \\n3. Paragraphs 1 and 2 shall not affect the rights and obligations of the Co mmission, \\nMember States and notified bodies with regard to the exchange of information and \\nthe dissemination of warnings, nor the obligations of the parties concerned to provide \\ninformation under criminal law of the Member States.  \\n4. The Commission and Mem ber States may exchange, where necessary, confidential \\ninformation with regulatory authorities of third countries with which they have \\nconcluded bilateral or multilateral confidentiality arrangements guaranteeing an \\nadequate level of confidentiality.  EN 82  EN Artic le 71  \\nPenalties  \\n1. In compliance with the terms and conditions laid down in this Regulation, Member \\nStates shall lay down the rules on penalties, including administrative fines, applicable \\nto infringements of this Regulation and shall take all measures nec essary to ensure \\nthat they are properly and effectively implemented. The penalties provided for shall \\nbe effective, proportionate, and dissuasive. They shall take into particular account the \\ninterests of small -scale providers and start -up and their economi c viability.  \\n2. The Member States shall notify the Commission of those rules and of those measures \\nand shall notify it, without delay, of any subsequent amendment affecting them.  \\n3. The following infringements shall be subject to administrative fines of u p to 30 000 \\n000 EUR or, if the offender is company, up to 6 % of its total worldwide annual \\nturnover for the  preceding financial year, whichever is higher:  \\n(a) non-compliance with the prohibition of the artificial intelligence practices \\nreferred to in Arti cle 5;  \\n(b) non-compliance of the AI system with the requirements laid down in Article \\n10. \\n4. The non -compliance of the AI system with any requirements or obligations under \\nthis Regulation, other than those laid down in Articles 5 and 10, shall be subject t o \\nadministrative fines of up to 20 000 000 EUR or, if the offender is a company, up to \\n4 % of its total worldwide annual turnover for the preceding financial year, \\nwhichever is higher.  \\n5. The supply of incorrect, incomplete or misleading information to not ified bodies and \\nnational competent authorities in reply to a request shall be subject to administrative \\nfines of up to 10 000 000 EUR or, if the offender is a company, up to 2 % of its total \\nworldwide annual turnover for the preceding financial year, whic hever is higher.  \\n6. When deciding on the amount of the administrative fine in each individual case, all \\nrelevant circumstances of the specific situation shall be taken into account and due \\nregard shall be given to the following:  \\n(a) the nature, gravity and  duration of the infringement and of its consequences;  \\n(b) whether administrative fines have been already applied by other market \\nsurveillance authorities to the same operator for the same infringement.  \\n(c) the size and market share of the operator committing the infringement;  \\n7. Each Member State shall lay down rules on whether and to what extent \\nadministrative fines may be imposed on public authorities and bodies established in \\nthat Member State.  \\n8. Depending on the legal system of the Member State s, the rules on administrative \\nfines may be applied in such a manner that the fines are imposed by competent \\nnational courts of other bodies as applicable in those Member States.  The application \\nof such rules in those Member States shall have  an equivalent effect.  EN 83  EN Article 72  \\nAdministrative fines on Union institutions, agencies and  bodies  \\n1. The European Data Protection Supervisor may impose administrative fines on Union \\ninstitutions, agencies and bodies falling within the scope of this Regulat ion. When \\ndeciding whether to impose an administrative fine and deciding on the amount of the \\nadministrative fine in each individual case, all relevant circumstances of the specific \\nsituation shall be taken into account and due regard shall be given to the  following:  \\n(a) the nature, gravity and duration of the infringement and of its consequences;  \\n(b) the cooperation with the European Data Protection Supervisor in order to \\nremedy the infringement and mitigate the possible adverse effects of the \\ninfringement , including compliance with any of the measures previously \\nordered by the European Data Protection Supervisor against the Union \\ninstitution or agency or body concerned with regard to the same subject matter;  \\n(c) any similar previous infringements by the Un ion institution, agency or body;  \\n2. The following infringements shall be subject to administrative fines of up to 500 000 \\nEUR:  \\n(a) non-compliance with the prohibition of the artificial intelligence practices \\nreferred to in Article 5;  \\n(b) non-compliance of the AI system with the requirements laid down in Article \\n10. \\n3. The non -compliance of the AI system with any requirements or obligations under \\nthis Regulation, other than those laid down in Articles 5 and 10, shall be subject to \\nadministrative fines of up to 250 000 EUR.  \\n4. Before taking decisions pursuant to this Article, the European Data Protection \\nSupervisor shall give the Union institution, agency or body  which is the subject of \\nthe proceedings conducted by the European Data Protection Supervisor the \\nopportunity of being heard on the matter regarding the possible infringement. The \\nEuropean Data Protection Supervisor shall base his or her decisions only on elements \\nand circumstances on which the parties concerned have been able to comment. \\nComplainants, if any, shall be associated closely with the proceedings.  \\n5. The rights of defense of the parties concerned shall be fully respected in the \\nproceedings. They shall be entitled to have access to the European Data Protection \\nSupervisor’s file, subject to the  legitimate interest of individuals or undertakings in \\nthe protection of their personal data or business secrets.  \\n6. Funds collected by imposition of fines in this Article shall be the income of the \\ngeneral budget of the Union.  \\nTITLE  XI \\nDELEGATION  OF POWER  AND  COMMITTEE  PROCEDURE   \\nArticle 73  \\nExercise of the delegation  \\n1. The power to adopt delegated acts is conferred on the Commission subject to the \\nconditions laid down in this Article.  EN 84  EN 2. The delegation of power referred to in Article 4, Article 7(1), Arti cle 11(3), Article \\n43(5) and (6) and Article 48(5) shall be conferred on the Commission for an \\nindeterminate period of time from [ entering into force of the Regulation ]. \\n3. The delegation of power referred to in Article 4, Article 7(1), Article 11(3), Arti cle \\n43(5) and (6) and Article 48(5) may be revoked at any time by the European \\nParliament or by the Council. A decision of revocation shall put an end to the \\ndelegation of power specified in that decision. It shall take effect the day following \\nthat of its  publication in the Official Journal of the European Union  or at a later date \\nspecified therein. It shall not affect the validity of any delegated acts already in force.  \\n4. As soon as it adopts a delegated act, the Commission shall notify it simultaneously  to \\nthe European Parliament and to the Council.  \\n5. Any delegated act adopted pursuant to Article 4, Article 7(1), Article 11(3), Article \\n43(5) and (6) and Article 48(5) shall enter into force only if no objection has been \\nexpressed by either the European P arliament or the Council within a period of three \\nmonths of notification of that act to the European Parliament and the Council or if, \\nbefore the expiry of that period, the European Parliament and the Council have both \\ninformed the Commission that they wil l not object. That period shall be extended by \\nthree months at the initiative of the European Parliament or of the Council.  \\nArticle 74  \\nCommittee procedure  \\n1. The Commission shall be assisted by a committee. That committee shall be a \\ncommittee within the me aning of Regulation (EU) No 182/2011.  \\n2. Where reference is made to this paragraph, Article 5 of Regulation (EU) No \\n182/2011 shall apply.  \\nTITLE  XII \\nFINAL  PROVISIONS   \\nArticle 75  \\nAmendment to Regulation (EC) No 300/2008  \\nIn Article 4(3) of Regulation (EC) No 300/2008, the following subparagraph is added:  \\n“When adopting detailed measures related to technical specifications and procedures for \\napproval and use of security equipment concerning Artificial Intelligence systems in the \\nmeaning of Regulation (EU) YYY/X X [on Artificial Intelligence]  of the European Parliament \\nand of the Council* , the requirements set out in Chapter 2, Title III of that Regulation shall be \\ntaken into account.”  \\n__________  \\n* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”  \\nArtic le 76  \\nAmendment to Regulation (EU) No 167/2013  \\nIn Article 17(5) of Regulation (EU) No 167/2013, the following subparagraph is added:  EN 85  EN “When adopting delegated acts pursuant to the first subparagraph concerning artificial \\nintelligence systems which are safet y components in the meaning of Regulation (EU) \\nYYY/XX [on Artificial Intelligence]  of the European Parliament and of the Council*, the \\nrequirements set out in Title III, Chapter 2 of that Regulation  shall be taken into account.  \\n__________  \\n* Regulation (EU)  YYY/XX [on Artificial Intelligence] (OJ …).”  \\nArticle 77  \\nAmendment to Regulation (EU) No 168/2013  \\nIn Article 22(5) of Regulation (EU) No 168/2013, the following subparagraph is added:  \\n“When adopting delegated acts pursuant to the first subparagraph concern ing Artificial \\nIntelligence systems which are safety components in the meaning of Regulation (EU) \\nYYY/XX on [Artificial Intelligence]  of the European Parliament and of the Council*, the \\nrequirements set out in Title III, Chapter 2 of that Regulation  shall be taken into account.  \\n__________  \\n* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”  \\nArticle 78  \\nAmendment to Directive 2014/90/EU  \\nIn Article 8 of Directive 2014/90/EU, the following paragraph is added:  \\n“4. For Artificial Intelligence systems  which are safety components in the meaning of \\nRegulation (EU) YYY/XX [on Artificial Intelligence]  of the European Parliament and of the \\nCouncil*, when carrying out its activities pursuant to paragraph 1 and when adopting \\ntechnical specifications and testing standards in accordance with paragraphs 2 and 3, the \\nCommission shall take into account the requirements set out in Title III, Chapter 2 of that \\nRegulation.  \\n__________  \\n* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.  \\nArticle 79  \\nAmendment to Directive (EU) 2016/797  \\nIn Article 5 of Directive (EU) 2016/797, the following paragraph is added:  \\n“12. When adopting delegated acts pursuant to paragraph 1 and implementing acts pursuant to \\nparagraph 11 concerning Artificial Intelligence systems  which are  safety components in the \\nmeaning of Regulation (EU) YYY/XX [on Artificial Intelligence]  of the European Parliament \\nand of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation  shall be \\ntaken into account.  \\n__________  \\n* Regulatio n (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.  EN 86  EN Article 80  \\nAmendment to Regulation (EU) 2018/858  \\nIn Article 5 of Regulation (EU) 2018/858 the following paragraph is added:  \\n“4. When adopting delegated acts pursuant to paragraph 3 concerning Artificial Intelligence \\nsystems which are safety components in the meaning of Regulation (EU) YYY/XX [on \\nArtificial Intelligence]  of the European Parliament and of the Council *, the requirements set \\nout in Title III, Chapter 2 of that Regulation  shall be taken into account.  \\n__________  \\n* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.  \\nArticle 81  \\nAmendment to Regulation (EU) 2018/1139  \\nRegulation (EU) 2018/1139 is amended as follows:  \\n(1) In Article 17, the following paragraph is added:  \\n“3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph \\n1 concerning Artificial Intelligence systems which are safety components in the meaning of \\nRegulation (EU) YYY/XX [ on Artificial Intelligence ] of the European Parl iament and of the \\nCouncil*, the requirements set out in Title III, Chapter 2 of that Regulation  shall be taken into \\naccount.  \\n__________  \\n* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”  \\n(2) In Article 19, the following paragraph is added:  \\n“4. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \\nIntelligence systems which are safety components in the meaning of Regulation (EU) \\nYYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of th at \\nRegulation  shall be taken into account.”  \\n(3) In Article 43, the following paragraph is added:  \\n“4. When adopting implementing acts pursuant to paragraph 1 concerning Artificial \\nIntelligence systems which are safety components in the meaning of Regulation  (EU) \\nYYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that \\nRegulation  shall be taken into account.”  \\n(4) In Article 47, the following paragraph is added:  \\n“3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \\nIntelligence systems which are safety components in the meaning of Regulation (EU) \\nYYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that \\nRegulation  shall be taken into account.”  \\n(5) In Article  57, the following paragraph is added:  \\n“When adopting those implementing acts concerning Artificial Intelligence systems which are \\nsafety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence], \\nthe requirements set out in Title II I, Chapter 2 of that Regulation  shall be taken into account.”  \\n(6) In Article 58, the following paragraph is added:  EN 87  EN “3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \\nIntelligence systems which are safety components in the  meaning of Regulation (EU) \\nYYY/XX [on Artificial Intelligence] , the requirements set out in Title III, Chapter 2 of that \\nRegulation  shall be taken into account.”.  \\nArticle 82  \\nAmendment to Regulation (EU) 2019/2144  \\nIn Article 11 of Regulation (EU) 2019/214 4, the following paragraph is added:  \\n“3. When adopting the implementing acts pursuant to paragraph 2,  concerning artificial \\nintelligence systems which are safety components in the meaning of Regulation (EU) \\nYYY/XX [on Artificial Intelligence] of the Europe an Parliament and of the Council*, the \\nrequirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.  \\n__________  \\n* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.  \\nArticle 83  \\nAI systems already placed on the marke t or put into service  \\n1. This Regulation shall not apply to the AI systems which are components of the large -\\nscale IT systems established by the legal acts listed in Annex IX that have been \\nplaced on the market or put into service before [12 months after t he date of \\napplication of this Regulation referred to in Article 85(2) ], unless the replacement or \\namendment of those legal acts leads to a significant change in the design or intended \\npurpose of the AI system or AI systems concerned.  \\nThe requirements laid  down in this Regulation shall be taken into account, where \\napplicable, in the evaluation of each large -scale IT systems established by the legal \\nacts listed in Annex IX to be undertaken as provided for in those respective acts.  \\n2. This Regulation shall ap ply to the high -risk AI systems, other than the ones referred \\nto in paragraph 1, that have been placed on the market or put into service before \\n[date of application of this Regulation referred to in Article 85(2) ], only if, from that \\ndate, those systems are subject to significant changes in their design or intended \\npurpose.  \\nArticle 84  \\nEvaluation and review  \\n1. The Commission shall assess the need for amendment of the list in Annex III once a \\nyear following the entry into force of this Regulation.  \\n2. By [three years after the date of application of this Regulation referred to in Article \\n85(2) ] and every four years thereafter, the Commission shall submit a report on the \\nevaluation and review of this Regulation to the European Parliament and to the \\nCouncil. The reports shall be made public.   \\n3. The reports referred to in paragraph 2 shall devote specific attention to the following:  \\n(a) the status of the financial and human resources of the national competent \\nauthorities in order to effectively perform the ta sks assigned to them under this \\nRegulation;  EN 88  EN (b) the state of penalties, and notably administrative fines as referred to in Article \\n71(1), applied by Member States to infringements of the provisions of this \\nRegulation.  \\n4. Within [ three years after the date of application of this Regulation referred to in \\nArticle 85(2) ] and every four years thereafter, the Commission shall evaluate the \\nimpact and effectiveness of codes of conduct to foster the application of the \\nrequirements set out in Title III, Chapter 2 and possibly other additional requirements \\nfor AI systems other than high -risk AI systems.  \\n5. For the purpose of paragraphs 1 to 4 the Board, the Member States and national \\ncompetent authorities shall provide the Commission with information on its request.  \\n6. In carrying out the evaluations and reviews referred to in paragraphs 1 to 4 the \\nCommission shall take into account the positions and findings of the Board, of the \\nEuropean Parliament, of the Council, and of other relevant bodies or sources.  \\n7. The Commi ssion shall, if necessary, submit appropriate proposals to amend this \\nRegulation, in particular taking into account developments in technology and in the \\nlight of the state of progress in the information society.  \\nArticle 85  \\nEntry into force and application  \\n1. This Regulation shall enter into force on the twentieth day following that of its \\npublication in the Official Journal of the European Union . \\n2. This Regulation shall apply from [24 months following the entering into force of the \\nRegulation].  \\n3. By way of derogation from  paragraph 2:  \\n(a) Title III, Chapter 4  and Title VI  shall apply from [three months following the \\nentry into force of this Regulation];  \\n(b) Article 71 shall apply from [twelve months following the entry into force of \\nthis Regulation].  \\nThis Regulation shall be binding in its entirety and directly applicable in all Member States.  \\nDone at Brussels,  \\nFor the European Parliament  For the Council  \\nThe President  The President  EN 89  EN LEGISLATIVE FINANCIAL STATEMENT  \\n1. FRAMEWORK  OF THE  PROPOSAL/INITIATIVE   \\n 1.1.  Title of the proposal/initiative  \\n 1.2.  Policy area(s) concerned  \\n 1.3.  The proposal/initiative relates to:  \\n 1.4.  Objective(s)  \\n1.4.1.   General objective(s)  \\n 1.4.2.  Specific objective(s)  \\n1.4.3.  Expected result(s) and impact  \\n1.4.4.  Indicators of performance  \\n 1.5.  Grounds for the proposal/initiative  \\n 1.5.1.   Requirement(s) to be met in the short or long term including a detailed \\ntimeline for roll -out of the implementation of the initiative  \\n1.5.2.  Added v alue of Union involvement (it may result from different factors, e.g. \\ncoordination gains, legal certainty, greater effectiveness or complementarities). For \\nthe purposes of this point \\'added value of Union involvement\\' is the value resulting \\nfrom Union inte rvention which is additional to the value that would have been \\notherwise created by Member States alone  \\n1.5.3.   Lessons learned from similar experiences in the past  \\n1.5.4. Compatibility with the Multiannual Financial Framework and possible \\nsynergies with other appropriate instruments  \\n1.5.5   Assessment of the different available financing options, including scope for \\nredeployment  \\n1.6.      Duration and financial impact of the proposal/initiative  \\n 1.7.   Management mode(s) planned  \\n2. MANAGEMENT  MEASURES   \\n 2.1.  Monitoring and reporting rules  \\n 2.2.  Management and control system  \\n2.2.1. Justification of the management mode(s), the funding implementation \\nmechanism(s), the payment modalities and the control strategy proposed  \\n2.2.2. Information concerning the risks identified and the internal control system(s) \\nset up to mitigate them  \\n2.2.3. Estimation and justification of the cost -effectiveness of the controls (ratio of \\n\"control costs ÷ value of the related funds managed\"), and assessment of the \\nexpected levels  of risk of error (at payment & at closure)  EN 90  EN  2.3. Measures to prevent fraud and irregularities  \\n3. ESTIMATED  FINANCIAL  IMPACT  OF THE  PROPOSAL/INITIATIVE   \\n 3.1. Heading(s) of the multiannual financial framework and expenditure budget \\nline(s) affected  \\n 3.2. Estimated financial impact of the proposal on appropriations   \\n 3.2.1.  Summary of estimated impact on operational appropriations  \\n 3.2.2.  Estimated output funded with operational appropriations  \\n 3.2.3. Summary of estimated impact on administrative appropri ations  \\n 3.2.4.  Compatibility with the current multiannual financial framework  \\n 3.2.5.  Third -party contributions  \\n 3.3. Estimated impact on revenueEN 91  EN LEGISLATIVE FINANCIAL STATEMENT  \\n1. FRAMEWORK  OF THE  PROPOSAL/INITIATIVE   \\n1.1. Title of the proposal/initiative  \\nRegulation of the European Parliament and of the Council Laying Down Harmonised \\nRules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain \\nUnion Legislative Acts  \\n1.2. Policy area(s) concerned  \\nCommunications Networks, Content and Technology;  \\nInternal Market, Industry, Entrepreneurship and SMEs;  \\nThe budgetary impact concerns the new tasks entrusted with the Commission, \\nincluding the support to the EU AI Board;  \\nActivity: Shaping Europe\\'s digital future.  \\n1.3. The proposal/initiative relates to:  \\nX  a new action   \\n\\uf0a8 a new action following a pilot project/preparatory action64  \\n\\uf0a8 the extension of an existing action   \\n\\uf0a8 an action redirected towards a new action   \\n1.4. Objective(s)  \\n1.4.1.  General objective( s)  \\nThe general objective of the intervention is to ensure the proper functioning of the \\nsingle market by creating the conditions for the development and use of trustworthy \\nartificial intelligence in the Union.  \\n1.4.2.  Specific objective(s)  \\nSpecific objective No 1  \\nTo set requirements specific to AI systems and obligations on all value chain \\nparticipants in order to ensure that AI systems placed on the market and used are safe \\nand respect existing law on fundamental rights and Union values;  \\nSpecific ob jective No 2  \\nTo ensure legal certainty to facilitate investment and innovation in AI by making it \\nclear what essential requirements, obligations, as well as conformity and compliance \\nprocedures must be followed to place or use an AI system in the Union mar ket; \\nSpecific objective No 3  \\nTo enhance governance and effective enforcement of existing law on fundamental \\nrights and safety requirements applicable to AI systems by providing new powers, \\nresources and clear rules for relevant authorities on conformity as sessment and ex \\n                                                 \\n64 As referred to in Article 54(2)(a) or (b) of the Financial  Regulation   EN 92  EN post monitoring procedures and the division of governance and supervision tasks \\nbetween national and EU levels;  \\nSpecific objective No 4  \\nTo facilitate the development of a single market for lawful, safe and trustworthy AI \\napplications and prevent market fragmentation by taking EU action to set minimum \\nrequirement for AI systems to be placed and used in the Union market in compliance \\nwith existing law on fundamental rights and safety.  EN 93  EN 1.4.3.  Expected result(s) and impact  \\nSpecify the effects  which the proposal/initiative should have on the beneficiaries/groups targeted.  \\nAI suppliers should benefit from a minimal but clear set of requirements, creating \\nlegal certainty and ensuring access to the entire single market.  \\nAI users should benefit fro m legal certainty that the high -risk AI systems they buy \\ncomply with European laws and values.  \\nConsumers should benefit by reducing the risk of violations of their safety or \\nfundamental rights.  \\n1.4.4.  Indicators of performance  \\nSpecify the indicators for mo nitoring implementation of the proposal/initiative.  \\nIndicator 1  \\nNumber of serious incidents or AI performances which constitute a serious incident \\nor a breach of fundamental rights obligations (semi -annual) by fields of applications \\nand calculated a) in ab solute terms, b) as share of applications deployed and c) as \\nshare of citizens concerned.  \\nIndicator 2  \\na) Total AI investment in the EU (annual)  \\nb) Total AI investment by Member State (annual)  \\nc) Share of companies using AI (annual)  \\nd) Share of SMEs using AI (annual)  \\na) and b) will be calculated based on official sources and benchmarked against \\nprivate estimates  \\nc) and d) will be collected by regular company surveys  \\n1.5. Grounds for the proposal/initiative  \\n1.5.1.  Requirement(s) to be met in the short or lo ng term including a detailed timeline for \\nroll-out of the implementation of the initiative  \\nThe Regulation should be fully applicable one year and a half after its adoption. \\nHowever, elements of the governance structure should be in place before then. In \\nparticular, Member States shall have appointed existing authorities and/or established \\nnew authorities performing the tasks set out in the legislation earlier, and the EU AI \\nBoard should be set -up and effective. By the time of applicability, the European \\ndatabase of AI systems should be fully operative. In parallel to the adoption process, \\nit is therefore necessary to develop the database, so that its development has come to \\nan end when the regulation enters into force.  \\n1.5.2.  Added value of Union involvement  (it may result from different factors, e.g. \\ncoordination gains, legal certainty, greater effectiveness or complementarities). For \\nthe purposes of this point \\'added value of Union involvement\\' is the value resulting \\nfrom Union intervention which is additio nal to the value that would have been \\notherwise created by Member States alone.  \\nAn emerging patchy framework of potentially divergent national rules will hamper \\nthe seamless provision of AI systems across the EU and is ineffective in ensuring the EN 94  EN safety a nd protection of fundamental rights and Union values across the different \\nMember States. A common EU legislative action on AI could boost the internal \\nmarket and has great potential to provide European industry with a competitive edge \\nat the global scene a nd economies of scale that cannot be achieved by individual \\nMember States alone.  \\n1.5.3.  Lessons learned from similar experiences in the past  \\nThe E -commerce Directive 2000/31/EC provides the core framework for the \\nfunctioning of the single market and the su pervision of digital services and sets a \\nbasic structure for a general cooperation mechanism among Member States, covering \\nin principle all requirements applicable to digital services. The evaluation of the \\nDirective pointed to shortcomings in several aspe cts of this cooperation mechanism, \\nincluding important procedural aspects such as the lack of clear timeframes for \\nresponse from Member States coupled with a general lack of responsiveness to \\nrequests from their counterparts. This has led over the years to  a lack of trust \\nbetween Member States in addressing concerns about providers offering digital \\nservices cross -border. The evaluation of the Directive showed the need to define a \\ndifferentiated set of rules and requirements at European level. For this reaso n, the \\nimplementation of the specific obligations laid down in this Regulation would \\nrequire a specific cooperation mechanism at EU level, with a governance structure \\nensuring coordination of specific responsible bodies at EU level.  \\n1.5.4.  Compatibility wi th the Multiannual Financial Framework and possible synergies \\nwith other appropriate instruments  \\nThe Regulation Laying Down Harmonised Rules on Artificial Intelligence and \\nAmending Certain Union Legislative Acts defines a new common framework of \\nrequiremen ts applicable to AI systems, which goes well beyond the framework \\nprovided by existing legislation. For this reason, a new national and European \\nregulatory and coordination function needs to be established with this proposal.  \\nAs regards possible synergies with other appropriate instruments, the role of \\nnotifying authorities at national level can be performed by national authorities \\nfulfilling similar functions sunder other EU regulations.  \\nMoreover, by increasing trust in AI and thus encouraging investment i n development \\nand adoption of AI, it complements Digital Europe, for which promoting the \\ndiffusion of AI is one of five priorities.  \\n1.5.5.  Assessment of the different available financing options, including scope for \\nredeployment  \\nThe staff will be redeploye d. The other costs will be supported from the DEP. \\nenvelope, given that the objective of this regulation – ensuring trustworthy AI – \\ncontributes directly to one key objective of Digital Europe – accelerating AI \\ndevelopment and deployment in Europe.  EN 95  EN 1.6. Duration and financial impact of the proposal/initiative  \\n\\uf0a8 limited duration  \\n– \\uf0a8 in effect from [DD/MM]YYYY to [DD/MM]YYYY  \\n– \\uf0a8 Financial impact from YYYY to YYYY for commitment appropriations and \\nfrom YYYY to YYYY for payment appropriations.  \\nX unlimited durat ion \\n– Implementation with a start -up period from one/two (tbc) year,  \\n– followed by full -scale operation.  \\n1.7. Management mode(s) planned65  \\nX Direct management  by the Commission  \\n– \\uf0a8 by its departments, including by its staff in the Union delegations;  \\n– \\uf0a8 by the executive agencies  \\n\\uf0a8 Shared management  with the Member States  \\n\\uf0a8 Indirect management  by entrusting budget implementation tasks to:  \\n– \\uf0a8 third countries or the bodies they have designated;  \\n– \\uf0a8 international organisations and their agencies (to be specifi ed); \\n– \\uf0a8 the EIB and the European Investment Fund;  \\n– \\uf0a8 bodies referred to in Articles 70 and 71 of the Financial Regulation;  \\n– \\uf0a8 public law bodies;  \\n– \\uf0a8 bodies governed by private law with a public service mission to the extent that \\nthey provide adequate financial guarantees;  \\n– \\uf0a8 bodies governed by the private law of a Member State that are entrusted with \\nthe implementation of a public -private partnership and that provide adequate \\nfinancial guarantees;  \\n– \\uf0a8 persons entrusted with the implementation of specific actions in  the CFSP \\npursuant to Title V of the TEU, and identified in the relevant basic act.  \\n– If more than one management mode is indicated, please provide details in the ‘Comments’ section.  \\nComments  \\n \\n \\n                                                 \\n65 Details of management modes and references to the Financial Regulation may be found on the \\nBudgWeb site: http://www.cc.cec/budg/man/budgmanag/budgmanag_en.html  EN 96  EN 2. MANAGEMENT  MEASURES   \\n2.1. Monitoring and reporting rules  \\nSpecify frequency and conditions.  \\nThe Regulation will be reviewed and evaluated five years from the entry into force of \\nthe regulation. The Commission will report on the findings of the evaluation to the \\nEuropean Parliament, the Council and the European Economic and Social \\nCommittee.  \\n2.2. Management and control system(s)  \\n2.2.1.  Justification of the management mode(s), the funding implementation mechanism(s), \\nthe payment modalities and the control strategy proposed  \\nThe Regulation establishes a new policy with regard to harmonised rules for the \\nprovision of artificial intelligence systems in the internal market while ensuring the \\nrespect of safety and fundamental rights. These new rules require a consistency \\nmechanism for the cross -border application of the obligations under this Regulation \\nin the form of a new advisory group coordinating the activities of national \\nauthorities.  \\nIn order to face these new tasks, it is necessary to appropriately resource the \\nCommission’s services. The enforcement of the new Regulation is estimated to \\nrequire 10 FTE à regime (5 FTE for the support to the activities of the Board and 5 \\nFTE for the European Data Protection Supervisor acting as a notifying body for AI \\nsystems deployed by a body of the European Union).  \\n2.2.2.  Information concerning the risks identified  and the internal control system(s) set up \\nto mitigate them  \\nIn order to ensure that the members of the Board have the possibility to make \\ninformed analysis on the basis of factual evidence, it is foreseen that the Board \\nshould be supported by the administr ative structure of the Commission and that an \\nexpert group be created to provide additional expertise where required.  \\n2.2.3.  Estimate and justification of the cost -effectiveness of the controls (ratio of \"control \\ncosts ÷ value of the related funds managed\" ), and assessment of the expected levels \\nof risk of error (at payment & at closure)  \\nFor the meeting expenditure, given the low value per transaction (e.g. refunding \\ntravel costs for a delegate for a meeting), standard control procedures seem \\nsufficient. Re garding the development of the database, contract attribution has a \\nstrong internal control system in place in DG CNECT through centralised \\nprocurement activities.  \\n2.3. Measures to prevent fraud and irregularities  \\nSpecify existing or envisaged prevention and protection measures, e.g. from the Anti -Fraud Strategy.  \\nThe existing fraud prevention measures applicable to the Commission will cover the \\nadditional appropriations necessary for this Regulation.   \\nEN 97  EN 3. ESTIMATED  FINANCIAL  IMPACT  OF THE  PROPOSAL/INITIATIVE   \\n3.1. Heading(s) of the multiannual financial framework and expenditure budget line(s) affected  \\n\\uf0b7 Existing budget lines  \\nIn order  of multiannual financial framework headings and budget lines.  \\nHeading of \\nmultiannual \\nfinancial \\nframewo rk Budget line  Type of  \\nexpenditure  Contribution  \\nNumber  \\n Diff./Non -\\ndiff.66 from \\nEFTA \\ncountries\\n67 \\n from \\ncandidate \\ncountries68 \\n from third \\ncountries  within the \\nmeaning of \\nArticle  21(2)(b) of \\nthe Financial \\nRegulation  \\n7 20 02 06 Administrative expenditure  Non-diff. NO NO NO NO \\n1 02 04 03 DEP Artificial Intelligence  Diff.  YES  NO NO NO \\n1 02 01 30 01 Support expenditure for \\nthe Digital Europe programme  Non-diff. YES  NO NO NO \\n3.2. Estimated financial impact of the proposal on appropriations  \\n3.2.1.  Summary of  estimated impact on expenditure on operational appropriations  \\n– \\uf0a8 The proposal/initiative does not require the use of operational appropriations  \\n– X The proposal/initiative requires the use of operational appropriations, as explained below:  \\nEUR million (to t hree decimal places)  \\n                                                 \\n66 Diff. = Differentiated appropriations / Non -diff. = Non -differentiated appropriations.  \\n67 EFTA: European Free Trade Association.  \\n68 Candidate countries and, where applicable, potential candidate countries from the Western Balkans.   \\nEN 98  EN Heading of multiannual financial  \\nframework  1  \\n \\nDG: CNECT     Year \\n2022  Year \\n2023  Year \\n2024  Year \\n2025  Year \\n2026  Year \\n202769 TOTAL  \\n\\uf09f Operational appropriations          \\nBudget line70 02 04 03  Commitments  (1a)  1.000       1.000  \\nPayments  (2a)  0.600  0.100  0.100  0.100  0.100   1.000  \\nBudget line  Commitments  (1b)         \\nPayments  (2b)         \\nAppropriations of an administrative nature financed from the envelope of specific \\nprogrammes71  \\n         \\nBudget line 02 01 30 01   (3)  0.240  0.240  0.240  0.240  0.240   1.200  \\nTOTAL appropriations  \\nfor DG CNECT  Commitments  =1a+1b +3   1.240   0.240  0.240  0.240   2.200  \\n Payments  =2a+2b  \\n+3  0.840  0.340  0.340  0.340  0.340   2.200  \\n \\n \\n \\n                                                 \\n69 Indicative and dependent on budg et availability.  \\n70 According to the official budget nomenclature.  \\n71 Technical and/or administrative assistance and expenditure in support of the implementation of EU programmes and/or actions ( former ‘BA’ lines), indirect research, \\ndirect research.   \\nEN 99  EN \\uf09f TOTAL operational appropriations  Commitments  (4)  1.000       1.000  \\nPayments  (5)  0.600  0.100  0.100  0.100  0.100   1.000  \\n\\uf09f TOTAL appropriations of an administrative nature financed from the \\nenvelope for specific programmes  (6)  0.240  0.240  0.240  0.240  0.240   1.200  \\nTOTAL appropriations  \\nunder HEADING 1  \\nof the multiannual  financial framework  Commitments  =4+ 6   1.240  0.240  0.240  .0.240  0.240   2.200  \\nPayments  =5+ 6   0.840  0.340  0.340  0.340  0.340   2.200  \\nIf more than one heading is affected by the proposal / initiative, repeat the section above:  \\n\\uf09f TOTAL operational appropriations (all \\noperational headings)  Commitments  (4)         \\nPayments  (5)         \\n\\uf09f TOTAL appropriations of an administrative nature \\nfinanced from the envelope for specific programmes (all \\noperational headings)  (6)         \\nTOTAL appropriations  \\nunder HEADINGS 1 to 6  \\nof the multiannual financial framework  \\n(Reference amount)  Commitments  =4+ 6          \\nPayments  =5+ 6          \\n \\n   \\nEN 100  EN Heading of multiannual financial  \\nframework  7 ‘Administrative expenditure’  \\nThis section should be filled in using the \\'budget data of an administrative nature\\' to be firstly introduced in the Annex  to the Legislative \\nFinancial Statement  (Annex V to the internal rules), which is uploaded to DECIDE for interservice consultation purposes.  \\nEUR million (to three decimal places)  \\n   Year \\n2023  Year \\n2024  Year \\n2025  Year \\n2026  Year 2027  After \\n202772 TOTAL  \\nDG: CNECT  \\n\\uf09f Human resources  0.760  0.760  0.760  0.760  0.760  0.760  3.800  \\n\\uf09f Other administrative expenditure  0.010  0.010  0.010  0.010  0.010  0.010  0.050  \\nTOTAL DG CNECT  Appropriations  0.760  0.760  0.760  0.760  0.760  0.760  3.850  \\nEuropean Data Protection Supervisor   \\n\\uf09f Human resources  0.760  0.760  0.760  0.760  0.760  0.760  3.800  \\n\\uf09f Other administrative expenditure         \\nTOTAL EDPS  Appropriations  0.760  0.760  0.760  0.760  0.760  0.760  3.800  \\nTOTAL appropriations  \\nunder HEADING  7 \\nof the multiannual financial framework   (Total commitments = Total payments)  1.530  1.530  1.530  1.530  1.530  1.530  7.650  \\nEUR million (to three decimal places)  \\n   Year \\n2022  Year \\n2023  Year \\n2024  Year \\n2025  Year 2026  Year 2027   TOTAL  \\nTOTAL appropriations  Commitments   2.770  1.770  1.770  1.770  1.770   9.850  \\n                                                 \\n72 All figures in this column are indicative and subject to the continuation of the programmes and availability of appropriations   \\nEN 101  EN under HEADINGS 1 to 7  \\nof the multiannual financial framework   Payments   2.370  1.870  1.870  1.870  1.870  9.850   \\nEN 102  EN 3.2.2.  Estimated output funded with operational appropriations  \\nCommitment appropriations in EUR million (to three decimal places)  \\nIndicate objectives \\nand outputs  \\n \\n\\uf0f2   Year \\n2022  Year \\n2023  Year \\n2024  Year \\n2025  Year \\n2026  Year \\n2027  After \\n202773 TOTAL  \\n OUTPUTS  \\n Type  \\n Average \\ncost \\nNo \\nCost \\nNo \\nCost \\nNo \\nCost \\nNo \\nCost \\nNo \\nCost \\nNo \\nCost \\nNo \\nCost Tota\\nl No Total \\ncost \\nSPECIFIC OBJECTIVE No 174…                 \\nDatabase      1 1.000  1  1  1  1  1 0.100  1 1.000  \\nMeetings - Output      10 0.200  10 0.200  10 0.200  10 0.200  10 0.200  10 0.200  50 1.000  \\nCommunication \\nactivities      2 0.040  2 0.040  2 0.040  2 0.040  2 0.040  2 0.040  10 0.040  \\nSubtotal for specific objective No 1                  \\nSPECIFIC OBJECTIVE No 2 ...                  \\n- Output                    \\nSubtotal for specific objective No 2                  \\nTOTALS    13 0.240  13 0.240  13 0.240  13 0.240  13 0.240  13 0.100  65 2.200  \\n                                                 \\n73 All figures in this column are indicative and subject to the continuation of the programmes and availability of appropriation s \\n74 As described in point 1.4.2. ‘Specific objective(s)…’   \\nEN 103  EN 3.2.3.  Summary of estimated impact on administrative appropriations   \\n– \\uf0a8 The proposal/initiative does not require the use of appropriations of an \\nadministrative nature  \\n– X The proposal/initiative requires the use of appropriations of an administrative \\nnature, as explained below:  \\nEUR million (to three decimal places)  \\n Year \\n2022  Year \\n2023  Year \\n2024  Year \\n2025  Year \\n2026  Year \\n2027  Yearly after  \\n202775 TOTAL  \\n \\nHEADING 7  \\nof the multiannual \\nfinancial framework          \\nHuman resources   1.520  1.520  1.520  1.520  1.520  1.520  7.600  \\nOther administrative \\nexpenditure   0.010  0.010  0.010  0.010  0.010  0.010  0.050  \\nSubtotal HEADING 7  \\nof the multiannual \\nfinancial framework   1.530  1.530  1.530  1.530  1.530  1.530  7.650  \\n \\nOutside HEADING 776 \\nof the multiannual \\nfinancial framework  \\n         \\nHuman resources          \\nOther expenditure  \\nof an administrative \\nnature   0.240  0.240  0.240  0.240  0.240  0.240  1.20 \\nSubtotal  \\noutside HEADING 7  \\nof the multiannual \\nfinancial framework   0.240  0.240  0.240  0.240  0.240  0.240  1.20 \\n \\nTOTAL   1.770  1.770  1.770  1.770  1.770  1.770  8.850  \\nThe appropriations required for human resources and other expenditure of an administrative nature will be met by \\nappropriations from the DG that are already assigned to management of the action and/or have been redeployed within the \\nDG, together if necessary with any additional alloc ation which may be granted to the managing DG under the annual \\nallocation procedure and in the light of budgetary constraints.  \\n                                                 \\n75 All figures in this column are indicative and subject to the continuation of the programmes and availability of \\nappropriations.  \\n76 Technical and/or administrative assistance and expenditure in support of  the implementation of \\nEU programmes and/or actions (former ‘BA’ lines), indirect research, direct research.   \\nEN 104  EN 3.2.3.1.  Estimated requirements of human resources  \\n– \\uf0a8 The proposal/initiative does not require the use of human resources.  \\n– X The proposal/initiative requires the use of human resources, as explained \\nbelow:  \\nEstimate to be expressed in full time equivalent units  \\n \\n.  Year \\n2023  Year \\n2024  Year \\n2025  2026  2027  After \\n202777  \\n\\uf09f Establishment plan posts (officials and temporary staff)  \\n20 01 02 01  (Headquarters and Commission’s Representation \\nOffices)  10 10 10 10 10 10  \\n20 01 02 03 (Delegations)         \\n01 01 01 01   (Indirect research)         \\n 01 01 01 11 (Direct research)         \\nOther budget lines (specify)         \\n\\uf09f External staff (in Full Time Equivalent unit: FTE)78 \\n \\n20 02 01  (AC, END, INT from the ‘global envelope’)         \\n20 02 03 (AC, AL, END, INT and JPD in the delegations)         \\nXX 01  xx yy zz  79 \\n - at Headquarters  \\n        \\n- in Delegations         \\n01 01 01 02  (AC, END, INT - Indirect research)         \\n 01 01 01 12 (AC, END, INT - Direct research)         \\nOther budget lines (specify)         \\nTOTAL   10 10 10 10 10 10  \\nXX is the policy area or budget title concerned.  \\nThe human resources required will be met by staff from the DG who are already assigned to management of the \\naction and/or have been redeployed within the DG, together if necessary with any additional allocatio n which \\nmay be granted to the managing DG under the annual allocation procedure and in the light of budgetary \\nconstraints.  \\nEDPS is expected to provide half of the resources required.  \\n \\nDescription of tasks to be carried out:  \\nOfficials and temporary staff  To prepare a total of 13 -16 meetings, draft reports, continue policy work, e.g. \\nregarding future amendments of the list of high -risk AI applications, and maintain \\nrelations with Member States’ authorities will require four AD FTE and 1 AST FTE.  \\nFor AI system s developed by the EU institutions, the European Data Protection \\nSupervisor is responsible. Based on past experience, it can be estimated that 5 AD FTE \\nare reuqired to fulfill the EDPS responsibilites under the draft legislation.  \\n                                                 \\n77  All figures in this column are indicative and subject to the continuation of the programmes and \\navailability of appropriations.  \\n78 AC = Contract Staff; AL = Local Staff; END = Seconded National Expert; INT = agency staff; JPD \\n= Junior Professionals in Delegations.  \\n79 Sub-ceiling for external staff covered by operational appropriations (former ‘BA’ lines).   \\nEN 105  EN External staff    \\nEN 106  EN 3.2.4.  Compatibility with the current multiannual financial framework  \\nThe proposal/initiative:  \\n– X can be fully financed through redeployment within the relevant heading of the \\nMultiannual Financial Framework (MFF).  \\nNo reporgramming is needed.   \\n– \\uf0a8 requires use of the unallocated margin under the relevant heading of the MFF \\nand/or use of the special instruments as defined in the MFF Regulation.  \\nExplain what is required, specifying the headings and budget lines concerned, the corresponding \\namounts, and the instruments proposed to be used.   \\n– \\uf0a8 requires a revision of the MFF.  \\nExplain what is required, specifying the headings and budget lines concerned and the corresponding \\namounts.  \\n3.2.5.  Third -party contributions  \\nThe proposal/in itiative:  \\n– X does not provide for co -financing by third parties  \\n– \\uf0a8 provides for the co -financing by third parties estimated below:  \\nAppropriations in EUR million (to three decimal places)  \\n Year \\nN80 Year \\nN+1 Year \\nN+2 Year \\nN+3 Enter as many years as necessary \\nto show the duration of the \\nimpact (see point 1.6)  Total  \\nSpecify the co -financing \\nbody           \\nTOTAL appropriations \\nco-financed          \\n \\n \\n                                                 \\n80 Year N is the year in which implementatio n of the proposal/initiative starts. Please replace \"N\" by the \\nexpected first year of implementation (for instance: 2021). The same for the following years.   \\nEN 107  EN 3.3. Estimated impact on revenue  \\n– \\uf0a8 The proposal/initiative has the following financial impact:  \\n– \\uf0a8 The proposal/initiative has the following financial impact:  \\n– \\uf0a8 on other revenue  \\n– \\uf0a8 on other revenue  \\n– Please indicate, if the revenue is assigned to expenditure lines \\uf0a8 \\nEUR million (to three decimal places)  \\nBudget revenue line:  Appropriation\\ns available for \\nthe current \\nfinancial year  Impact of the proposal/initiative81 \\nYear \\nN Year \\nN+1 Year \\nN+2 Year \\nN+3 Enter as many years as necessary to show \\nthe duration of the impact (see point 1.6)  \\nArticle ………….          \\nFor assigned revenue, specify the budget expenditure line(s) affected.  \\n  \\nOther remarks (e.g. method/formula used for calculating the impact on revenue or any other \\ninformation).  \\n \\n                                                 \\n81 As regards traditional own resources (customs duties, sugar levies), the amounts indicated must be net \\namounts, i.e. gross amounts after deduction of 20  % for collection costs.  '"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_text_and_save(text, output_file, chunk_size=500):\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:  \n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            if end >= len(text):\n",
    "                file.write(text[start:])\n",
    "            else:\n",
    "                # until the word is reached it end to not split words \n",
    "                while end < len(text) and text[end] != ' ':\n",
    "                    end -= 1\n",
    "                file.write(text[start:end] + '\\n')  \n",
    "                start = end + 1 \n",
    "            start = end\n",
    "        return text\n",
    "\n",
    "# Chunk text and save it to a file\n",
    "chunk_text_and_save(text, 'chunked_text.txt', chunk_size=500)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T09:42:28.867219800Z",
     "start_time": "2024-05-17T09:42:28.725577400Z"
    }
   },
   "id": "3297d06c4f544f9"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def chunk_table_rows_and_save_all(tables, output_file, chunk_size=3):\n",
    "    table_chunks = []\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for table in tables:\n",
    "            for i in range(0, len(table), chunk_size):\n",
    "                chunk = table[i:i + chunk_size]\n",
    "                table_chunks.append(chunk) \n",
    "                for row in chunk:\n",
    "                    cleaned_row = [cell if cell is not None else '' for cell in row]\n",
    "                    file.write(','.join(cleaned_row) + '\\n')\n",
    "                file.write('\\n')\n",
    "    return table_chunks  \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T10:13:55.966285900Z",
     "start_time": "2024-05-17T10:13:55.897293200Z"
    }
   },
   "id": "3db89b360770180"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text chunks: EN   EN \n",
      " \n",
      " \n",
      " EUROPEAN  \n",
      "COMMISSION   \n",
      "Brussels, 21.4.2021  \n",
      "COM(2021) 206 final  \n",
      "2021/0106 (COD)  \n",
      " \n",
      "Proposal for a  \n",
      "REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL  \n",
      "LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE \n",
      "(ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION \n",
      "LEGISLATIVE ACTS  \n",
      "{SEC(2021)  167 final}  - {SWD(2021)  84 final}  - {SWD(2021)  85 final}   EN 1  EN EXPLANATORY MEMORANDUM  \n",
      "1. CONTEXT  OF THE  PROPOSAL  \n",
      "1.1. Reasons for and objectives of the proposal  \n",
      "This explanatory memorandum accompanies the proposal for a Regulation laying down \n",
      "harmonised rules on artificial intelligence (Artificial Intelligence Act). Artificial Intelligence \n",
      "(AI) is a fast evolving family of technologies that can bring a wide array of economic and \n",
      "societal benefits across the entire s pectrum of industries and social activities. By improving \n",
      "prediction, optimising operations and resource allocation, and personalising service delivery, \n",
      "the use of artificial intelligence can support socially and environmentally beneficial outcomes \n",
      "and pro vide key competitive advantages to companies and the European economy. Such \n",
      "action is especially needed in high -impact sectors, including climate change, environment and \n",
      "health, the public sector, finance, mobility, home affairs and agriculture. However, t he same \n",
      "elements and techniques that power the socio -economic benefits of AI can also bring about \n",
      "new risks or negative consequences for individuals or the society. In light of the speed of \n",
      "technological change and possible challenges, the EU is committed to strive for a balanced \n",
      "approach.  It is in the Union interest to preserve the EU’s technological leadership and to \n",
      "ensure that Europeans can benefit from new technologies developed and functioning \n",
      "according to Union values, fundamental rights and principl es. \n",
      "This proposal delivers on the political commitment by President von der Leyen, who \n",
      "announced in her political guidelines for the 2019 -2024 Commission “A Union that strives for \n",
      "more”1, that the Commission would put forward legislation for a coordinated European \n",
      "approach on the human and ethical implications of AI. Following on that announcement, on \n",
      "19 February 2020 the Commission published the White Paper on AI - A European approach \n",
      "to excellence and trust2. The White Paper sets out policy options on how  to achieve the twin \n",
      "objective of promoting the uptake of AI and of addressing the risks associated with certain \n",
      "uses of such technology. This proposal aims to implement the second objective for the \n",
      "development of an ecosystem of trust by proposing a legal  framework for trustworthy AI. The \n",
      "proposal is based on EU values and fundamental rights and aims to give people and other \n",
      "users the confidence to embrace AI -based solutions, while encouraging businesses to develop \n",
      "them. AI should be a tool for people and be a force for good in society with the ultimate aim \n",
      "of increasing human well -being. Rules for AI available in the Union market or otherwise \n",
      "affecting people in the Union should therefore be human centric, so that people can trust that \n",
      "the technology is us ed in a way that is safe and compliant with the law, including the respect \n",
      "of fundamental rights. Following the publication of the White Paper, the Commission \n",
      "launched a broad stakeholder consultation,  which was met with a great interest by a large \n",
      "number of stakeholders who were largely supportive of regulatory intervention to address the \n",
      "challenges and concerns raised by the increasing use of AI.  \n",
      "The proposal also responds to explicit requests from the European Parliament (EP) and the \n",
      "European  Council, w hich have repeatedly expressed calls for legislative action to ensure a \n",
      "well-functioning internal market for artificial intelligence systems (‘AI systems’) where both \n",
      "benefits and risks of AI are adequately addressed at Union level.  It supports the objecti ve of \n",
      "the Union being a global leader in the development of secure, trustworthy and ethical artificial \n",
      "                                                 \n",
      "1 https://ec.europa.eu/commission/sites/beta -political/files/political -guidelines -next-commission_en.pdf  \n",
      "2 European Commission, White Paper on Artificial Intelligence - A European approach to excellence and \n",
      "trust, COM(2020) 65 final, 2020.  EN 2  EN intelligence as stated by the European Council3 and ensures the protection of ethical principles \n",
      "as specifically requested by the European Parliament4.  \n",
      "In 2017, the European Council called for a ‘sense of urgency to address emerging trends’ \n",
      "including ‘issues such as artificial intelligence …, while at the same time ensuring a high \n",
      "level of data protection, digital rights and ethical standards’5. In its 20 19 Conclusions on the \n",
      "Coordinated Plan on the development and use of artificial intelligence Made in Europe6, the \n",
      "Council further highlighted the importance of ensuring that European citizens’ rights are fully \n",
      "respected and called for a review of the exist ing relevant legislation to make it fit for purpose \n",
      "for the new opportunities and challenges raised by AI. The European Council has also called \n",
      "for a clear determination of the AI applications that should be considered high -risk7.  \n",
      "The most recent Conclusi ons from 21 October 2020  further called for addressing the opacity, \n",
      "complexity, bias, a certain degree of unpredictability and partially autonomous behaviour of \n",
      "certain AI systems, to ensure their compatibility with fundamental rights and to facilitate the  \n",
      "enforcement of legal rules8. \n",
      "The European Parliament has also undertaken a considerable amount of work in the area of \n",
      "AI. In October 2020, it adopted a number of resolutions related to AI, including on ethics9, \n",
      "liability10 and copyright11. In 2021, those were followed by resolutions on AI in criminal \n",
      "matters12 and in education, culture and the audio -visual sector13. The EP Resolution on a \n",
      "Framework of Ethical Aspects of Artificial Intelligence, Robotics and Related Technologies \n",
      "specifically recommends to the  Commission to propose legislative action to harness the \n",
      "opportunities and benefits of AI, but also to ensure protection of ethical principles. The \n",
      "resolution includes a text of the legislative proposal for a regulation on ethical principles for \n",
      "the develo pment, deployment and use of AI, robotics and related technologies. In accordance \n",
      "with the political commitment made by President von der Leyen in her Political Guidelines as \n",
      "regards resolutions adopted by the European Parliament under Article 225 TFEU, th is \n",
      "                                                 \n",
      "3 European Council, Special meeting of the European Council (1 and 2 October 2020) – Conclusions , \n",
      "EUCO 13/20, 2020, p. 6.  \n",
      "4 European Parliament resolution of 20 Oct ober 2020 with recommendations to the Commission on a \n",
      "framework of ethical aspects of artificial intelligence, robotics and related technologies, \n",
      "2020/2012(INL).  \n",
      "5 European Council, European Council meeting (19 October 2017) – Conclusion  EUCO 14/17, 2017, p. \n",
      "8. \n",
      "6 Council of the European Union, Artificial intelligence b) Conclusion s on the coordinated plan on \n",
      "artificial intelligence -Adoption  6177/19, 2019.  \n",
      "7 European Council, Special meeting of the European Council (1and 2 October 2020) – Conclusions  \n",
      "EUCO 13/20, 2020.  \n",
      "8 Council of the European Union, Presidency conclusions - The Charter of Fundamental Rights in the \n",
      "context of Artificial Intelligence and Digital Chan ge, 11481/20, 2020.  \n",
      "9 European Parliament resolution of 20 October 2020 on a framework of ethical aspects of artificial \n",
      "intelligence, robotics and related technologies, 2020/2012(INL) . \n",
      "10 European Parliament resolution of 20 October 2020 on a civil liability regime for artificial intelligence, \n",
      "2020/2014(INL).  \n",
      "11 European Parliament resolution of 20 October 2020 on intellectual property rights for the development \n",
      "of artificial intelligence technologies, 2020/2015(INI).  \n",
      "12 European  Parliament Draft Report, Artificial intelligence in criminal law and its use by the police and \n",
      "judicial authorities in criminal matters,  2020/2016(INI) .  \n",
      "13 European Parliament Draft Report, Artificial intelligence in education, culture and the audiovisual \n",
      "sector, 2020/2017(INI) . In that regard, the Commission has adopted the Digital Education Action Plan \n",
      "2021 -2027: Resetting education and training for the digital age, which foresees the development of \n",
      "ethical guidelines in AI and Data usage in education – Commission Communicatio n COM(2020) 624 \n",
      "final.  EN 3  EN proposal takes into account the aforementioned resolution of the European Parliament in full \n",
      "respect of proportionality, subsidiarity and better law making principles.  \n",
      "Against this political context, the Commission puts forward the proposed regulatory \n",
      "framework on Artificial Intelligence with the following specific objectives : \n",
      " ensure that AI systems placed on the Union market and used are safe and respect \n",
      "existing law on fundamental rights and Union values;  \n",
      " ensure legal certainty to facilitate investmen t and innovation in AI;  \n",
      " enhance governance and effective enforcement of existing law on fundamental \n",
      "rights and safety requirements applicable to AI systems;  \n",
      " facilitate the development of a single market for lawful, safe and trustworthy AI \n",
      "applications and prevent market fragmentation.  \n",
      "To achieve those objectives, this proposal presents a balanced and proportionate horizontal \n",
      "regulatory approach to AI that is limited to the minimum necessary requirements to address \n",
      "the risks and problems linked to AI, withou t unduly constraining or hindering technological \n",
      "development or otherwise disproportionately increasing the cost of placing AI solutions on \n",
      "the market.  The proposal sets a robust and flexible legal framework. On the one hand, it is \n",
      "comprehensive and future -proof in its fundamental regulatory choices, including the \n",
      "principle -based requirements that AI systems should comply with. On the other hand, it puts \n",
      "in place a proportionate regulatory system centred on a well -defined risk -based regulatory \n",
      "approach that  does not create unnecessary restrictions to trade, whereby legal intervention is \n",
      "tailored to those concrete situations where there is a justified cause for concern or where such \n",
      "concern can reasonably be anticipated in the near future. At the same time, t he legal \n",
      "framework includes flexible mechanisms that enable it to be dynamically adapted as the \n",
      "technology evolves and new concerning situations emerge.  \n",
      "The proposal sets harmonised rules for the development, placement on the market and use of \n",
      "AI systems i n the Union following a proportionate risk -based approach. It proposes a single \n",
      "future -proof definition of AI. Certain particularly harmful AI practices are prohibited as \n",
      "contravening Union values, while specific restrictions and safeguards are proposed in  relation \n",
      "to certain uses of remote biometric identification systems for the purpose of law enforcement. \n",
      "The proposal lays down a solid risk methodology to define “high -risk” AI systems that pose \n",
      "significant risks to the health and safety or fundamental ri ghts of persons. Those AI systems \n",
      "will have to comply with a set of horizontal mandatory requirements for trustworthy AI and \n",
      "follow conformity assessment procedures before those systems can be placed on the Union \n",
      "market. Predictable, proportionate and clea r obligations are also placed on providers and users \n",
      "of those systems to ensure safety and respect of existing legislation protecting fundamental \n",
      "rights throughout the whole AI systems’ lifecycle. For some specific AI systems, only \n",
      "minimum transparency obl igations are proposed, in particular when chatbots or ‘deep fakes’ \n",
      "are used.  \n",
      "The proposed rules will be enforced through a governance system at Member States level, \n",
      "building on already existing structures, and a cooperation mechanism at Union level with the \n",
      "establishment of a European Artificial Intelligence Board . Additional measu res are also \n",
      "proposed to support innovation, in particular through AI regulatory sandboxes and other \n",
      "measures to reduce the regulatory burden and to support Small and Medium -Sized Enterprises \n",
      "(‘SMEs’) and start -ups. EN 4  EN 1.2. Consistency with existing policy pr ovisions in the policy area  \n",
      "The horizontal nature of the proposal requires full consistency with existing Union legislation \n",
      "applicable to sectors where high -risk AI systems are already used or likely to be used in the \n",
      "near future.  \n",
      "Consistency is also ensu red with the EU Charter of Fundamental Rights and the existing \n",
      "secondary Union legislation on data protection, consumer protection, non -discrimination and \n",
      "gender equality. The proposal is without prejudice and complements the General Data \n",
      "Protection Regula tion (Regulation (EU) 2016/679) and the Law Enforcement Directive \n",
      "(Directive (EU) 2016/680) with a set of harmonised rules applicable to the design, \n",
      "development and use of certain high -risk AI systems and restrictions on certain uses of remote \n",
      "biometric id entification systems. Furthermore, the proposal complements existing Union law \n",
      "on non -discrimination with specific requirements that aim to minimise the risk of algorithmic \n",
      "discrimination, in particular in relation to the design and the quality of data set s used for the \n",
      "development of AI systems complemented with obligations for testing, risk management, \n",
      "documentation and human oversight throughout the AI systems’ lifecycle. The proposal is \n",
      "without prejudice to the application of Union competition law.  \n",
      "As regards high-risk AI systems which are safety components of products, this proposal will \n",
      "be integrated into the existing sectoral safety legislation to ensure consistency, avoid \n",
      "duplications and minimise additional burdens. In particular, as regards  high-risk AI systems \n",
      "related to products covered by the New Legislative Framework (NLF) legislation (e.g. \n",
      "machinery, medical devices, toys), the requirements for AI systems set out in this proposal \n",
      "will be checked as part of the existing conformity assessment pro cedures under the relevant \n",
      "NLF legislation. With regard to the interplay of requirements, while the safety risks specific \n",
      "to AI systems are meant to be covered by the requirements of this proposal, NLF legislation \n",
      "aims at ensuring the overall safety of the  final product and therefore may contain specific \n",
      "requirements regarding the safe integration of an AI system into the final product. The \n",
      "proposal for a Machinery Regulation, which is adopted on the same day as this proposal fully \n",
      "reflects this approach. As regards high-risk AI systems related to products  covered by relevant \n",
      "Old Approach legislation (e.g. aviation, cars), this proposal would not directly apply. \n",
      "However, the ex -ante essential requirements for high -risk AI systems set out in this proposal \n",
      "will have to be taken into account when adopting relevant implementing or delegated \n",
      "legislation under those acts.  \n",
      "As regards AI systems provided or used by regulated credit institutions , the authorities \n",
      "responsible for the supervision of the Union’s financial  services legislation should be \n",
      "designated as competent authorities for supervising the requirements in this proposal to ensure \n",
      "a coherent enforcement of the obligations under this proposal and the Union’s financial \n",
      "services legislation where AI systems ar e to some extent implicitly regulated in relation to the \n",
      "internal governance system of credit institutions . To further enhance consistency, the \n",
      "conformity assessment procedure and some of the providers’ procedural obligations under this \n",
      "proposal are integr ated into the procedures under Directive 2013/36/EU on access to the \n",
      "activity of credit institutions and the prudential supervision14.  \n",
      "                                                 \n",
      "14 Directive 2013/36/EU of the European Parliament and of the Council of 26 June 2013 on access to the \n",
      "activity of credit institutions and the prudential supervision of credit institutions and investment firms, \n",
      "amending Directive 2002 /87/EC and repealing Directives 2006/48/EC and 2006/49/EC Text with EEA \n",
      "relevance, OJ L 176, 27.6.2013, p. 338 –436. EN 5  EN This proposal is also consistent with the applicable Union legislation on services, including on \n",
      "intermediary services re gulated by the e -Commerce Directive 2000/31/EC15 and the \n",
      "Commission’s recent proposal for the Digital Services Act (DSA)16. \n",
      "In relation to AI systems that are components of large -scale IT systems in the Area of \n",
      "Freedom, Security and Justice managed by the European Union Agency for the Operational \n",
      "Management of Large -Scale IT Systems (eu -LISA), the proposal will not apply to tho se AI \n",
      "systems that have been placed on the market or put into service before one year has elapsed \n",
      "from the date of application of this Regulation,  unless the replacement or amendment of those \n",
      "legal acts leads to a significant change in the design or intend ed purpose of the AI system or \n",
      "AI systems concerned . \n",
      "1.3. Consistency with other Union policies  \n",
      "The proposal is part of a wider comprehensive package of measures that address problems \n",
      "posed by the development and use of AI, as examined in the White Paper o n AI. Consistency \n",
      "and complementarity is therefore ensured with other ongoing or planned initiatives of the \n",
      "Commission that also aim to address those problems, including the revision of sectoral \n",
      "product legislation (e.g. the Machinery Directive, the Genera l Product Safety Directive) and \n",
      "initiatives that address liability issues related to new technologies, including AI systems. \n",
      "Those initiatives will build on and complement this proposal in order to bring legal clarity and \n",
      "foster the development of an ecosy stem of trust in AI in Europe.  \n",
      "The proposal is also coherent with the Commission’s overall digital strategy in its \n",
      "contribution to promoting technology that works for people, one of the three main pillars of \n",
      "the policy orientation and objectives announced  in the Communication ‘Shaping Europe's \n",
      "digital future’17. It lays down a coherent, effective and proportionate framework to ensure AI \n",
      "is developed in ways that respect people’s rights and earn their trust, making Europe fit for the \n",
      "digital age and turning the next ten years into the Digital Decade18. \n",
      "Furthermore, the promotion of AI -driven innovation is closely linked to the Data \n",
      "Governance Act19, the Open Data Directive20 and other initiatives under the EU strategy \n",
      "for data21, which will establish trusted mech anisms and services for the re -use, sharing and \n",
      "pooling of data that are essential for the development of data -driven AI models of high \n",
      "quality.  \n",
      "The proposal also strengthens significantly the Union’s role to help shape global norms and \n",
      "standards and promo te trustworthy AI that is consistent with Union values and interests. It \n",
      "provides the Union with a powerful basis to engage further with its external partners, \n",
      "including third countries, and at international fora on issues relating to AI.  \n",
      "                                                 \n",
      "15 Directive 2000/31/EC of the European Parliament and of the Council of 8 June 2000 on certain legal \n",
      "aspects of information society services , in particular electronic commerce, in the Internal Market \n",
      "('Directive on electronic commerce'), OJ L 178, 17.7.2000, p. 1 –16. \n",
      "16 See Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL \n",
      "on a Single Market For Digital Services (Digital S ervices Act) and amending Directive 2000/31/EC \n",
      "COM/2020/825 final.  \n",
      "17 Communication from the Commission, Shaping Europe's Digital Future, COM/2020/67 final.  \n",
      "18 2030 Digital Compass: the European way for the Digital Decade . \n",
      "19 Proposal for a Regulation on European data governance (Data Governance Act) COM/2020/767 . \n",
      "20 Directive (EU) 2019/1024 of the European Parliament and of the Council of 20 June 2019 on open data \n",
      "and the re -use of public sector information, PE/28/2019/REV/1, OJ L 172, 26.6.2019, p. 56 –83. \n",
      "21 Commission Communication, A  European strategy for data COM/2020/66 final.  \n",
      " EN 6  EN 2. LEGAL  BASIS,  SUBSIDIARITY  AND  PROPORTIONALITY  \n",
      "2.1. Legal  basis  \n",
      "The legal basis for the proposal is in the first place Article 114 of the Treaty on the \n",
      "Functioning of the European Union (TFEU), which provides for the adoption of measures to \n",
      "ensure the establishment and f unctioning of the internal market.  \n",
      "This proposal constitutes a core part of the EU digital single market strategy. The primary \n",
      "objective of this proposal is to ensure the proper functioning of the internal market by setting \n",
      "harmonised rules in particular on the development, placing on the Union market and the use \n",
      "of products and services making use of AI technologies or provided as stand -alone AI \n",
      "systems. Some Member States are already considering national rules to ensure that AI is safe \n",
      "and is developed a nd used in compliance with fundamental rights obligations. This will likely \n",
      "lead to two main problems: i) a fragmentation of the internal market on essential elements \n",
      "regarding in particular the requirements for the AI products and services , their marketin g, \n",
      "their use, the liability  and the supervision  by public authorities , and ii) the substantial \n",
      "diminishment of legal certainty for both providers and users of AI systems on how existing \n",
      "and new rules will apply to those systems in the Union. Given the wide  circulation of products \n",
      "and services across borders, these two problems can be best solved through EU harmonizing \n",
      "legislation.  \n",
      "Indeed, the proposal defines common mandatory requirements applicable to the design and \n",
      "development of certain AI systems befor e they are placed on the market that will be further \n",
      "operationalised through harmonised technical standards. The proposal also addresses the \n",
      "situation after AI systems have been placed on the market by harmonising the way in which \n",
      "ex-post controls are cond ucted.  \n",
      "In addition, considering that this proposal contains certain specific rules  on the protection of \n",
      "individuals with regard to the processing of personal data,  notably restrictions of the use of AI \n",
      "systems for ‘real -time’ remote biometric identification in publicly accessible spaces for the \n",
      "purpose of law enforcement, it is appropriate to base this regulation, in as far as those specific \n",
      "rules are concerned, on Article 16 of the TFEU.  \n",
      "2.2. Subsidiarity  (for non-exclusive  competence)   \n",
      "The nature of AI, which often relies on large and varied datasets and which may be embedded \n",
      "in any product or service circulating freely within the internal market, entails that the \n",
      "objectives of this proposal cannot be effectively achieved by Member States alone. \n",
      "Furthermore, a n emerging patchwork of potentially divergent national rules will hamper the \n",
      "seamless circulation of products and services related to  AI systems across the EU and will be \n",
      "ineffective in ensuring the safety and protection of fundamental rights and Union values \n",
      "across the different Member States. National approaches in addressing the problems will only \n",
      "create additional legal uncertainty  and barriers, and will slo w market uptake of AI.   \n",
      "The objectives of this proposal can be better achieved at Union level to avoid a further \n",
      "fragmentation of the Single Market into potentially contradictory national frameworks \n",
      "preventing the free circulation of goods and services emb edding AI. A solid European \n",
      "regulatory framework for trustworthy AI will also ensure a level playing field and protect all \n",
      "people, while strengthening Europe’s competitiveness and industrial basis in AI. Only \n",
      "common action at Union level can also protect t he Union’s digital sovereignty and leverage \n",
      "its tools and regulatory powers to shape global rules and standards.   EN 7  EN 2.3. Proportionality  \n",
      "The proposal builds on existing legal frameworks and is proportionate and necessary to \n",
      "achieve its objectives, since it f ollows a risk -based approach and imposes regulatory burdens \n",
      "only when an AI system is likely to pose high risks to fundamental rights and safety. For \n",
      "other, non -high-risk AI systems, only very limited transparency obligations are imposed, for \n",
      "example in te rms of the provision of information to flag the use of an AI system when \n",
      "interacting with humans. For high -risk AI systems, the requirements of high quality data, \n",
      "documentation and traceability, transparency, human oversight, accuracy and robustness, are \n",
      "strictly necessary to mitigate the risks to fundamental rights and safety posed by AI and that \n",
      "are not covered by other existing legal frameworks. Harmonised standards and supporting \n",
      "guidance and compliance tools will assist providers and users in complying  with the \n",
      "requirements laid down by the proposal  and minimise their costs. The costs incurred by \n",
      "operators are proportionate to the objectives achieved and the economic and reputational \n",
      "benefits that operators can expect from this proposal.  \n",
      "2.4. Choice  of the instrument  \n",
      "The choice of a regulation as a legal instrument is justified by the need for a uniform \n",
      "application of the new rules, such as definition of AI, the prohibition of certain harmful AI -\n",
      "enabled practices and the classification of certain AI syst ems. The direct applicability of a \n",
      "Regulation, in accordance with Article 288 TFEU, will reduce legal fragmentation and \n",
      "facilitate the development of a single market for lawful, safe and trustworthy AI systems. It \n",
      "will do so, in particular, by introducing a harmonised set of core requirements with regard to \n",
      "AI systems classified as high -risk and obligations for providers and users of those systems, \n",
      "improving the protection of fundamental rights and providing legal certainty for operators and \n",
      "consumers alike . \n",
      "At the same time, the provisions of the regulation are not overly prescriptive and leave room \n",
      "for different levels of Member State action for elements that do not undermine the objectives \n",
      "of the initiative, in particular the internal organisation of the market surveillance system and \n",
      "the uptake of measures to foster innovation.  \n",
      "3. RESULTS  OF EX-POST  EVALUATIONS,  STAKEHOLDER  \n",
      "CONSULTATIONS  AND  IMPACT  ASSESSMENTS  \n",
      "3.1. Stakeholder  consultation  \n",
      "This proposal is the result of extensive consultation with all major stakeholders, in which the \n",
      "general principles and minimum standards for consultation of interested parties by the \n",
      "Commission were applied.  \n",
      "An online public consultation  was launched on 1 9 February 2020 along with the publication \n",
      "of the White Paper on Artificial Intelligence and ran until 14 June 2020. The objective of that \n",
      "consultation was to collect views and opinions on the White Paper. It targeted all interested \n",
      "stakeholders from the p ublic and private sectors, including governments, local authorities, \n",
      "commercial and non -commercial organisations, social partners, experts, academics and \n",
      "citizens. After analysing all the responses received, the Commission published a summary \n",
      "outcome and t he individual responses on its website22. \n",
      "In total, 1215 contributions were received, of which 352 were from companies or business \n",
      "organisations/associations, 406 from individuals (92%individuals from EU ), 152 on behalf of \n",
      "                                                 \n",
      "22 See all consultation results here.  EN 8  EN academic/research institutions, a nd 73 from public authorities. Civil society’s voices were \n",
      "represented by 160 respondents (among which 9 consumers’ organisations, 129 non -\n",
      "governmental organisations and 22 trade unions), 72 respondents contributed as ‘others’. Of \n",
      "the 352 business and indu stry representatives, 222 were companies and business \n",
      "representatives, 41.5% of which were micro, small and medium -sized enterprises. The rest \n",
      "were business associations. Overall, 84% of business and industry replies came from the EU -\n",
      "27. Depending on the q uestion, between 81 and 598 of the respondents used the free text \n",
      "option to insert comments. Over 450 position papers were submitted through the EU Survey \n",
      "website, either in addition to questionnaire answers (over 400) or as stand -alone contributions \n",
      "(over  50). \n",
      "Overall, there is a general agreement amongst stakeholders on a need for action. A large \n",
      "majority of stakeholders agree that legislative gaps exist or that new legislation is needed. \n",
      "However, several stakeholders warn the Commission to avoid duplicat ion, conflicting \n",
      "obligations and overregulation. There were many comments underlining the importance of a \n",
      "technology neutral and proportionate regulatory framework.  \n",
      "Stakeholders mostly requested a narrow, clear and precise definition for AI. Stakeholders a lso \n",
      "highlighted that besides the clarification of the term of AI, it is important to define ‘risk’, \n",
      "‘high -risk’, ‘low -risk’, ‘remote biometric identification’ and ‘harm’.  \n",
      "Most of the respondents are explicitly in favour of the risk -based approach. Using a risk-based \n",
      "framework was considered a better option than blanket regulation of all AI systems. The types \n",
      "of risks and threats should be based on a sector -by-sector and case -by-case approach. Risks \n",
      "also should be calculated taking into account the impact on  rights and safety.  \n",
      "Regulatory sandboxes could be very useful for the promotion of AI  and are welcomed by \n",
      "certain stakeholders, especially the Business Associations.  \n",
      "Among those who formulated their opinion on the enforcement models, more than 50%, \n",
      "especi ally from the business associations, were in favour of a combination of an ex -ante risk \n",
      "self-assessment and an ex -post enforcement for high -risk AI systems.  \n",
      "3.2. Collection  and use of expertise  \n",
      "The proposal builds on two years of analysis and close involve ment of stakeholders, including \n",
      "academics, businesses, social partners, non -governmental organisations, Member States and \n",
      "citizens. The preparatory work started in 2018 with the setting up of a High -Level Expert \n",
      "Group on AI (HLEG) which had an inclusive an d broad composition of 52 well -known \n",
      "experts tasked to advise the Commission on the implementation of the Commission’s Strategy \n",
      "on Artificial Intelligence. In April 2019, the Commission supported23 the key requirements set \n",
      "out in the HLEG ethics guidelines for Trustworthy AI24, which had been revised to take into \n",
      "account more than 500 submissions from stakeholders. The key requirements reflect a \n",
      "widespread and common approach, as evidenced by a plethora of ethical codes and principles \n",
      "developed by many privat e and public organisations in Europe and beyond, that AI \n",
      "development and use should be guided by certain essential value -oriented principles. The \n",
      "Assessment List for Trustworthy Artificial Intelligence (ALTAI)25 made those requirements \n",
      "operational in a pilo ting process with over 350 organisations.  \n",
      "                                                 \n",
      "23 European Commission, Building Trust in Human -Centric Artificial Intelligence , COM(2019) 168.  \n",
      "24 HLEG, Ethics Guidelines for Trustworthy AI , 2019.  \n",
      "25 HLEG, Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self -assessment , 2020.  EN 9  EN In addition, the AI Alliance26 was formed as a platform for approximately 4000 stakeholders \n",
      "to debate the technological and societal implications of AI, culminating in a yearly AI \n",
      "Assembly.  \n",
      "The White Paper  on AI f urther developed this inclusive approach, inciting comments from \n",
      "more than 1250 stakeholders, including over 450 additional position papers. As a result, the \n",
      "Commission published an Inception Impact Assessment, which in turn attracted more than \n",
      "130 comment s27. Additional stakeholder workshops and events were also organised the \n",
      "results of which support the analysis in the impact assessment and the policy choices made in \n",
      "this proposal28. An external study  was also procured to feed into the impact assessment.  \n",
      "3.3. Impact  assessment  \n",
      "In line with its “Better Regulation” policy, the Commission conducted an impact assessment \n",
      "for this proposal examined by the Commission's Regulatory Scrutiny Board. A meeting with \n",
      "the Regulatory Scrutiny Board was held on 16 December 2020, which was follow ed by a \n",
      "negative opinion. After substantial revision of the impact assessment to address the comments \n",
      "and a resubmission of the impact assessment, the Regulatory Scrutiny Board issued a positive \n",
      "opinion on 21 March 2021. The opinions of the Regulatory Scru tiny Board, the \n",
      "recommendations and an explanation of how they have been taken into account are presented \n",
      "in Annex 1 of the impact assessment.  \n",
      "The Commission examined different policy options to achieve the general objective of the \n",
      "proposal, which is to ensure the proper functioning of the single market  by creating the \n",
      "conditions for the development and use of trustworthy AI in the Union.  \n",
      "Four policy options of different degrees of regulatory intervention were assessed:  \n",
      " Option 1 : EU legislative instrument setting up a voluntary labelling scheme;  \n",
      " Option 2 : a sectoral, “ad -hoc” approach;  \n",
      " Option 3 : Horizontal EU legislative instrument following a proportionate risk -\n",
      "based approach;  \n",
      " Option 3+ : Horizontal EU legislative instrument following a proportionate risk -\n",
      "based approach + codes of conduct for non -high-risk AI systems;  \n",
      " Option 4 : Horizontal EU legislative instrument establishing mandatory \n",
      "requirements for all AI systems, irrespective of the risk they pose.  \n",
      "According to the Commission's established methodology, each policy option was evaluated \n",
      "against economic and societal impacts, with a particular focus on impacts on fundamental \n",
      "rights. The preferred option is option 3+, a regulatory framework for high -risk AI systems \n",
      "only, with the possibility for all providers of non -high-risk AI systems to follow a code of \n",
      "conduct. The requirements will concern data, documentation and traceability, provision of \n",
      "information and transparency, human oversight and robustness and accuracy and would be \n",
      "mandatory for high -risk AI systems. Companies that introduced codes of conduct for other AI \n",
      "systems would do so voluntarily.  \n",
      "                                                 \n",
      "26 The AI Alliance is  a multi -stakeholder forum launched in June 2018, AI Alliance \n",
      "https://ec.europ a.eu/digital -single -market/en/european -ai-alliance  \n",
      "27 European Commission, Inception Impact Assessment For  a Proposal for a legal act of the European \n",
      "Parliament and the Council laying down requirements for Artificial Intelligence .  \n",
      "28 For details of all the consultations that have been carried out see Annex 2 of the impact assessment.  EN 10  EN The preferred option was considered suitable to address in the most effective way the \n",
      "objectives of this proposal. By requiring a restricted yet effective set of actions from AI \n",
      "developers and users, the preferred option limits the risks of violation of fundamental rights \n",
      "and safety of people and foster effective supervision and enforcement, by targeting the \n",
      "requirements only to systems where there is a high risk that such violations could occur. As a \n",
      "result, that option keeps compliance costs to a minimum, thus avoiding an unnecessary \n",
      "slowing of uptake due to higher prices and compliance costs. In order to ad dress possible \n",
      "disadvantages for SMEs, this option includes several provisions to support their compliance \n",
      "and reduce their costs, including creation of regulatory sandboxes and obligation to consider \n",
      "SMEs interests when setting fees related to conformity assessment.  \n",
      "The preferred option will increase people’s trust in AI, companies will gain in legal certainty, \n",
      "and Member States will see no reason to take unilateral action that could fragment the single \n",
      "market. As a result of higher demand due to higher t rust, more available offers due to legal \n",
      "certainty, and the absence of obstacles to cross -border movement of AI systems, the single \n",
      "market for AI will likely flourish. The European Union will continue to develop a fast -\n",
      "growing AI ecosystem of innovative se rvices and products embedding AI technology or \n",
      "stand -alone AI systems, resulting in increased digital autonomy.  \n",
      "Businesses or public authorities that develop or use AI applications that constitute a high risk \n",
      "for the safety or fundamental rights of citizen s would have to comply with specific \n",
      "requirements and obligations. Compliance with these requirements would  imply costs \n",
      "amounting to approximately EUR € 6000 to EUR € 7000 for the supply of an average high -\n",
      "risk AI system of around EUR € 170000 by 2025. For  AI users, there would also be the \n",
      "annual cost for the time spent on ensuring human oversight where this is appropriate, \n",
      "depending on the use case. Those have been estimated at approximately EUR € 5000 to EUR \n",
      "€ 8000 per year. Verification costs could amoun t to another EUR € 3000 to EUR € 7500 for \n",
      "suppliers of high -risk AI. Businesses or public authorities that develop or use any AI \n",
      "applications not classified as high risk would only have minimal obligations of information. \n",
      "However, they could choose to join  others and together adopt a code of conduct to follow \n",
      "suitable requirements,  and to ensure that their AI systems are trustworthy. In such a case, \n",
      "costs would be at most as high as for high -risk AI systems, but most probably lower.  \n",
      "The impacts of the poli cy options on different categories of stakeholders (economic operators/ \n",
      "business; conformity assessment bodies, standardisation bodies and other public bodies; \n",
      "individuals/citizens; researchers) are explained in detail in Annex 3 of the Impact assessment \n",
      "supporting this proposal.  \n",
      "3.4. Regulatory  fitness  and simplification  \n",
      "This proposal lays down obligation that will apply to providers and users of high -risk AI \n",
      "systems. For providers who develop and place such systems on the Union market, it will \n",
      "create lega l certainty and ensure that no obstacle to the cross -border provision of AI -related \n",
      "services and products emerge. For companies using AI, it will promote trust among their \n",
      "customers. For national public administrations, it will promote public trust in the use of AI \n",
      "and strengthen enforcement mechanisms (by introducing a European coordination \n",
      "mechanism, providing for appropriate capacities, and facilitating audits of the AI systems \n",
      "with new requirements for documentation, traceability and transparency). More over, the \n",
      "framework will envisage specific measures supporting innovation, including regulatory \n",
      "sandboxes and specific measures supporting small -scale users and providers of high -risk AI \n",
      "systems to comply with the new rules . \n",
      "The proposal also specifically aims at strengthening Europe’s competitiveness and industrial \n",
      "basis in AI. Full consistency is ensured with existing sectoral Union legislation applicable to EN 11  EN AI systems (e.g. on products and services) that will bring further clarity and simplify the \n",
      "enforc ement of the new rules.  \n",
      "3.5. Fundamental  rights  \n",
      "The use of AI with its specific characteristics (e.g. opacity, complexity, dependency on data, \n",
      "autonomous behaviour) can adversely affect a number of fundamental rights enshrined in the \n",
      "EU Charter of Fundamen tal Rights (‘the Charter’) . This proposal seeks to ensure a high level \n",
      "of protection for those fundamental rights and aims to address various sources of risks \n",
      "through a clearly defined risk -based approach. With a set of requirements for trustworthy AI \n",
      "and proportionate obligations on all value chain participants, the proposal will enhance and \n",
      "promote the protection of the rights  protected by the Charter: the right  to human dignity \n",
      "(Article 1), respect for private life and protection of personal data (Articl es 7 and 8), non -\n",
      "discrimination (Article 21) and equality between women and men (Article 23). It aims to \n",
      "prevent a chilling effect on the rights to freedom of expression (Article 11) and freedom of \n",
      "assembly (Article 12), to ensure protection of the right t o an effective remedy and to a fair \n",
      "trial, the rights of defence and the presumption of innocence (Articles 47 and 48), as well as \n",
      "the general principle of good administration. Furthermore, as applicable in certain domains, \n",
      "the proposal will positively aff ect the rights of a number of special groups, such as the \n",
      "workers’ rights to fair and just working conditions (Article 31), a high level of consumer \n",
      "protection (Article 28), the rights of the child (Article 24) and the integration of persons with \n",
      "disabilit ies (Article 26). The right to a high level of environmental protection and the \n",
      "improvement of the quality of the environment (Article 37) is also relevant, including in \n",
      "relation to the health and safety of people. The obligations for ex ante testing, risk  \n",
      "management and human oversight will also facilitate the respect of other fundamental rights \n",
      "by minimising the risk of erroneous or biased AI -assisted decisions in critical areas such as \n",
      "education and training, employment, important services, law enforceme nt and the  judiciary. In \n",
      "case infringements of fundamental rights still happen, effective redress for affected persons \n",
      "will be made possible by ensuring transparency and traceability of the AI systems coupled \n",
      "with strong ex post controls.  \n",
      "This proposal imp oses some restrictions on the freedom to conduct business (Article 16) and \n",
      "the freedom of art and science (Article 13) to ensure compliance with overriding reasons of \n",
      "public interest such as health, safety, consumer protection and the protection of other \n",
      "fundamental rights  (‘responsible innovation’) when high -risk AI technology is developed and \n",
      "used. Those restrictions are proportionate and limited to the minimum necessary to prevent \n",
      "and mitigate serious safety risks and likely infringements of fundamental rights.  \n",
      "The increased transparency obligations will also not disproportionately affect the right to \n",
      "protection of intellectual property (Article 17(2)), since they will be limited only to the \n",
      "minimum necessary information for individuals to exercise their right to an effective remedy \n",
      "and to the necessary transparency towards supervision and enforcement authorities, in line \n",
      "with their mandates. Any disclosure of information will be carried out in compliance with \n",
      "relevant legislation in the field, including Directive 2016/943 on the protection of undisclosed \n",
      "know -how and business information (trade secrets) against their unlawful acquisition, use and \n",
      "disclosure . When public authorities and notified bodies need to be given access to confidential \n",
      "information or source code to examine compliance with substantial obligations, they are \n",
      "placed under binding confidentiality obligations.  \n",
      "4. BUDGETARY  IMPLICATIONS  \n",
      "Member States will have to designate supervisory authorities in charge of implementing the \n",
      "legislative requ irements. Their supervisory function could build on existing arrangements, for EN 12  EN example regarding conformity assessment bodies or market surveillance, but would require \n",
      "sufficient technological expertise and human and financial resources. Depending on the p re-\n",
      "existing structure in each Member State, this could amount to 1 to 25 Full Time Equivalents \n",
      "per Member State.  \n",
      "A detailed overview of the costs involved is provided in the ‘financial statement’ linked to \n",
      "this proposal.  \n",
      "5. OTHER  ELEMENTS  \n",
      "5.1. Implementati on plans  and monitoring,  evaluation  and reporting  arrangements  \n",
      "Providing for a robust monitoring and evaluation mechanism is crucial to ensure that the \n",
      "proposal will be effective in achieving its specific objectives. The Commission will be in \n",
      "charge of mon itoring the effects of the proposal. It will establish a system for registering \n",
      "stand -alone high -risk AI applications in a public EU -wide database. This registration will also \n",
      "enable competent authorities, users and other interested people to verify if the  high-risk AI \n",
      "system complies with the requirements  laid down in the proposal  and to exercise enhanced \n",
      "oversight over those AI systems posing high risks to fundamental rights. To feed this \n",
      "database, AI providers will be obliged to provide meaningful inform ation about their systems \n",
      "and the conformity assessment carried out on those systems .  \n",
      "Moreover, AI providers will be obliged to inform national competent authorities about serious \n",
      "incidents or malfunctioning that constitute a breach of fundamental rights obligations as soon \n",
      "as they become aware of them, as well as any recalls or withdrawals of AI systems from the \n",
      "market. National competent authorities will then investigate the incidents/or malfunctioning, \n",
      "collect all the necessary information and regularly  transmit it to the Commission with \n",
      "adequate metadata. The Commission will complement this information on the incidents by a \n",
      "comprehensive analysis of the overall market for AI.  \n",
      "The Commission will publish a report evaluating and reviewing the proposed AI  framework \n",
      "five years following the date on which it becomes applicable.  \n",
      "5.2. Detailed  explanation  of the specific  provisions  of the proposal  \n",
      "5.2.1.  SCOPE  AND  DEFINITIONS  (TITLE  I) \n",
      "Title I defines the subject matter of the regulation and the scope of application of the new \n",
      "rules that cover the placing on the market, putting into service and use of AI systems . It also \n",
      "sets out the definitions used throughout the instrument. The definition of AI system in the \n",
      "legal framework aims to be as technology neutral and future proof as possible, taking into \n",
      "account the fast technological and market developments related to AI. In order to provide the \n",
      "needed legal certainty, Title I is complemented by Ann ex I, which contains a detailed list of \n",
      "approaches and techniques for the development of AI to be adapted by the Commission in line \n",
      "with new technological developments. Key participants across the AI value chain are also \n",
      "clearly defined such as providers a nd users of AI systems that cover both public and private \n",
      "operators to ensure a level playing field.  \n",
      "5.2.2.  PROHIBITED ARTIFICIAL INTELLIGENCE PRACTICES (TITLE II)  \n",
      "Title II  establishes a list of prohibited AI. The regulation follows a risk -based approach,  \n",
      "differentiating between uses of AI that create (i) an unacceptable risk, (ii) a high risk, and (iii) \n",
      "low or minimal risk. The list of prohibited practices in Title II comprises all those AI systems \n",
      "whose use is considered unacceptable as contravening Unio n values, for instance by violating \n",
      "fundamental rights. The prohibitions covers practices that have a significant potential to \n",
      "manipulate persons  through subliminal techniques beyond their consciousness or exploit EN 13  EN vulnerabilities of specific vulnerable gro ups such as children or persons with disabilities in \n",
      "order to materially distort their behaviour in a manner that is likely to cause them or another \n",
      "person psychological or physical harm. Other manipulative or exploitative practices affecting \n",
      "adults that m ight be facilitated by AI systems could be covered by the existing data \n",
      "protection, consumer protection and digital service legislation that guarantee that natural \n",
      "persons are properly informed and have free choice not to be subject to profiling or other \n",
      "practices that might affect their behaviour. The proposal also prohibits AI -based social \n",
      "scoring for general purposes done by public authorities. Finally, the use of ‘real time’ remote \n",
      "biometric identification systems in publicly accessible spaces for the purpose of law \n",
      "enforcement is also prohibited unless certain limited exceptions apply.  \n",
      "5.2.3.  HIGH -RISK AI SYSTEMS (TITLE III)  \n",
      "Title III contains specific rules for AI systems that create a high risk to the health and safety \n",
      "or fundamental rights of natural persons. In line with a risk -based approach, those high -risk \n",
      "AI systems are permitted on the European market subject to compliance with certain \n",
      "mandatory requirements and an ex -ante conformity assessment. The classification of an AI \n",
      "system as hi gh-risk is based on the intended purpose of the AI system, in line with existing \n",
      "product safety legislation. Therefore, the classification as high -risk does not only depend on \n",
      "the function performed by the AI system, but also on the specific purpose and mo dalities for \n",
      "which that system is used.  \n",
      "Chapter 1 of Title III sets the classification rules and identifies two main categories of high -\n",
      "risk AI systems:  \n",
      " AI systems intended to be used as safety component of products that are subject to \n",
      "third party ex -ante conformity assessment;  \n",
      " other stand -alone AI systems with mainly fundamental rights implications that are \n",
      "explicitly listed in Annex III.  \n",
      "This list of high -risk AI systems in Annex III contains a limited number of AI systems whose \n",
      "risks have already materi alised or are likely to materialise in the near future. To ensure that \n",
      "the regulation can be adjusted to emerging uses and applications of AI, the Commission may \n",
      "expand the list of high -risk AI systems used  within certain pre -defined areas, by applying a \n",
      "set of criteria and risk assessment methodology.  \n",
      "Chapter 2 sets out the legal requirements for high -risk AI systems in relation to data and data \n",
      "governance, documentation and recording keeping, transparency and provision of information \n",
      "to users, human over sight, robustness, accuracy and security. The proposed minimum \n",
      "requirements are already state -of-the-art for many diligent operators and the result of two \n",
      "years of preparatory work, derived from the Ethics Guidelines of the HLEG29, piloted by \n",
      "more than 350 organisations30. They are also largely consistent with other international \n",
      "recommendations and principles, which ensures that the proposed AI framework is \n",
      "compatible with those adopted by the EU’s international trade partners . The precise technical \n",
      "solution s to achieve compliance with those requirements may be provided by standards or by \n",
      "other technical specifications or otherwise be developed in accordance with general \n",
      "engineering or scientific knowledge at the discretion of the provider of the AI system. T his \n",
      "flexibility is particularly important, because it allows providers of AI systems to choose the \n",
      "                                                 \n",
      "29 High -Level Expert Group on Artificial Intelligence, Ethics Guidelines for Trustworthy AI , 2019.  \n",
      "30 They were also endorsed by the Commission in its 2019 Communication on human -centric approach to \n",
      "AI. EN 14  EN way to meet their requirements, taking into account the state -of-the-art and technological and \n",
      "scientific progress in this field.  \n",
      "Chapter 3 places a clear se t of horizontal obligations on providers of high -risk AI systems. \n",
      "Proportionate obligations are also placed on users and other participants across the AI value \n",
      "chain (e.g., importers, distributors, authorized representatives).  \n",
      "Chapter 4 sets the framework for notified bodies to be involved as independent third parties in \n",
      "conformity assessment procedures, while Chapter 5 explains in detail the conformity \n",
      "assessment procedures to be followed for each type of high -risk AI system.  The conformity \n",
      "assessment app roach aims to minimise the burden for economic operators as well as for \n",
      "notified bodies, whose capacity needs to be progressively ramped up over time. AI systems \n",
      "intended to be used as safety components of products that are regulated under the New \n",
      "Legislat ive Framework legislation (e.g. machinery, toys, medical devices, etc.) will be subject \n",
      "to the same ex -ante and ex -post compliance and enforcement mechanisms of the products of \n",
      "which they are a component. The key difference is that the ex -ante and ex -post mechanisms \n",
      "will ensure compliance not only with the requirements established by sectorial legislation, but \n",
      "also with the requirements established by this regulation.  \n",
      "As regards stand -alone high -risk AI systems that are referred to in Annex III, a new \n",
      "comp liance and enforcement system will be established. This follows the model of the New \n",
      "Legislative Framework legislation implemented through internal control checks by the \n",
      "providers with the exception of remote biometric identification systems that would be subject \n",
      "to third party conformity assessment. A comprehensive ex -ante conformity assessment \n",
      "through internal checks, combined with a strong ex -post enforcement, could be an effective \n",
      "and reasonable solution for those systems, given the early phase of the r egulatory intervention \n",
      "and the fact the AI sector is very innovative and expertise for auditing is only now being \n",
      "accumulated. An assessment through internal checks for ‘stand -alone’ high -risk AI systems \n",
      "would require a full, effective and properly documen ted ex ante compliance with all \n",
      "requirements of the regulation and compliance with robust quality and risk management \n",
      "systems and post -market monitoring. After the provider has performed the relevant \n",
      "conformity assessment, it should register those stand -alone high -risk AI systems in an EU \n",
      "database that will be managed by the Commission to increase public transparency and \n",
      "oversight and strengthen ex post supervision by competent authorities. By contrast, for \n",
      "reasons of consistency with the existing product s afety legislation, the conformity assessments \n",
      "of AI systems that are safety components of products will follow a system with third party \n",
      "conformity assessment procedures already established under the relevant sectoral product \n",
      "safety legislation. New ex ant e re-assessments of the conformity will be needed in case of \n",
      "substantial modifications to the AI systems (and notably changes which go beyond what is \n",
      "pre-determined by the provider in its technical documentation and checked at the moment of \n",
      "the ex -ante con formity assessment).  \n",
      "5.2.4.  TRANSPARENCY OBLIGATIONS FOR CERTAIN AI SYSTEMS (TITLE IV)  \n",
      "Title IV  concerns certain AI systems to take account of the specific risks of manipulation they \n",
      "pose. Transparency obligations will apply for systems that (i) interact with humans, (ii) are \n",
      "used to detect emotions or determine association with (social) categories based on biometric \n",
      "data, or (iii) generate or manipulate content (‘deep fakes’). When persons interact with an AI \n",
      "system or their emotions or characteristics ar e recognised through automated means, people \n",
      "must be informed of that circumstance. If an AI system is used to generate or manipulate \n",
      "image, audio or video content that appreciably resembles  authentic content, there should be an \n",
      "obligation to disclose that  the content is generated through automated means, subject to EN 15  EN exceptions for legitimate purposes (law enforcement, freedom of expression). This allows \n",
      "persons to make informed choices or step back from a given situation.  \n",
      "5.2.5.  MEASURES IN SUPPORT OF INNOV ATION (TITLE V)  \n",
      "Title V contributes to the objective to create a legal framework that is innovation -friendly, \n",
      "future -proof and resilient to disruption. To that end, it encourages national competent \n",
      "authorities to set up regulatory sandboxes and sets a bas ic framework in terms of governance, \n",
      "supervision and liability. AI regulatory sandboxes establish a controlled environment to test \n",
      "innovative technologies for a limited time on the basis of a testing plan agreed with the \n",
      "competent authorities. Title V also  contains measures to reduce the regulatory burden on \n",
      "SMEs and start -ups.  \n",
      "5.2.6.  GOVERNANCE AND IMPLEMENTATION (TITLES VI, VII AND VII)  \n",
      "Title VI sets up the governance systems at Union and national level. At Union level, the \n",
      "proposal establishes a European Artificial Intelligence Board (the ‘Board’), composed of \n",
      "representatives from the Member States and the Commission. The Board will facilitate a \n",
      "smooth, effective and harmonised implementation of this regulation by contributing to the \n",
      "effective cooperation of the national supervisory authorities and the Commission and \n",
      "providing advice and expertise to the Commission. It will also collect and share best practices \n",
      "among the Member States.  \n",
      "At national level, Member States will have to designate one or more national competent \n",
      "authorities and, among them, the national supervisory authority, for the purpose of \n",
      "supervising the application and implementati on of the regulation. The European Data \n",
      "Protection Supervisor will act as the competent authority for the supervision of the Union \n",
      "institutions, agencies and bodies when they fall within the scope of this regulation.  \n",
      "Title VII  aims to facilitate the monito ring work of the Commission and national authorities \n",
      "through the establishment of an EU -wide database for stand -alone high -risk AI systems with \n",
      "mainly fundamental rights implications. The database will be operated by the Commission \n",
      "and provided with data b y the providers of the AI systems, who will be required to register \n",
      "their systems before placing them on the market or otherwise putting them into service.  \n",
      "Title VIII sets out the monitoring and reporting obligations for providers of AI systems with \n",
      "regard  to post -market monitoring and reporting and investigating on AI -related incidents and \n",
      "malfunctioning. Market surveillance authorities would also control the market and investigate \n",
      "compliance with the obligations and requirements for all high -risk AI syste ms already placed \n",
      "on the market. Market surveillance authorities would have all powers under Regulation (EU) \n",
      "2019/1020 on market surveillance. Ex -post enforcement should ensure that once the AI \n",
      "system has been put on the market, public authorities have the  powers and resources to \n",
      "intervene in case AI systems generate unexpected risks, which warrant rapid action. They will \n",
      "also monitor compliance of operators with their relevant obligations under the regulation. The \n",
      "proposal does not foresee the automatic cr eation of any additional bodies or authorities at \n",
      "Member State level. Member States may therefore appoint (and draw upon the expertise of) \n",
      "existing sectorial authorities, who would be entrusted also with the powers to monitor and \n",
      "enforce the provisions of the regulation.  \n",
      "All this is without prejudice to the existing system and allocation of powers of ex -post \n",
      "enforcement of obligations regarding fundamental rights in the Member States. When \n",
      "necessary for their mandate, existing supervision and enforcement a uthorities will also have \n",
      "the power to request and access any documentation maintained following this regulation and, \n",
      "where needed, request market surveillance authorities to organise testing of the high -risk AI \n",
      "system through technical means.  EN 16  EN 5.2.7.  CODES  OF CONDUCT (TITLE IX)  \n",
      "Title IX creates a framework for the creation of codes of conduct, which aim to encourage \n",
      "providers of non -high-risk AI systems to apply voluntarily the mandatory requirements for \n",
      "high-risk AI systems (as laid out in Title III). Pro viders of non -high-risk AI systems may \n",
      "create and implement the codes of conduct themselves. Those codes may also include \n",
      "voluntary commitments related, for example, to environmental sustainability, accessibility for \n",
      "persons with disability, stakeholders’ participation in the design and development of AI \n",
      "systems, and diversity of development teams.  \n",
      "5.2.8.  FINAL PROVISIONS (TITLES X, XI AND XII)  \n",
      "Title X  emphasizes the obligation of all parties to respect the confidentiality of information \n",
      "and data  and sets  out rules for the exchange of information obtained during the \n",
      "implementation of the regulation. Title X also includes measures to ensure the effective \n",
      "implementation of the regulation through effective, proportionate, and dissuasive penalties for \n",
      "infringe ments of the provisions . \n",
      "Title XI sets out rules for the exercise of delegation and implementing powers. The proposal \n",
      "empowers the Commission to adopt, where appropriate, implementing acts to ensure uniform \n",
      "application of the regulation or delegated acts t o update or complement the lists in Annexes I \n",
      "to VII.  \n",
      "Title XII  contains an obligation for the Commission to assess regularly the need for an update \n",
      "of Annex III and to prepare regular reports on the evaluation and review of the regulation. It \n",
      "also lays d own final provisions, including a differentiated transitional period for the initial \n",
      "date of the applicability of the regulation to facilitate the smooth implementation for all \n",
      "parties concerned.  EN 17  EN 2021/0106 (COD)  \n",
      "Proposal for a  \n",
      "REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL  \n",
      "LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE \n",
      "(ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION \n",
      "LEGISLATIVE ACTS  \n",
      "THE EUROPEAN PARLIAMENT AND THE COUNCI L OF THE EUROPEAN UNION,  \n",
      "Having regard to the Treaty on the Functioning of the European Union, and in particular \n",
      "Articles 16 and 114 thereof,  \n",
      "Having regard to the proposal from the European Commission,  \n",
      "After transmission of the draft legislative act to the  national parliaments,  \n",
      "Having regard to the opinion of the European Economic and Social Committee31, \n",
      "Having regard to the opinion of the Committee of the Regions32, \n",
      "Acting in accordance with the ordinary legislative procedure,  \n",
      "Whereas:  \n",
      "(1) The purpose of thi s Regulation is to improve the functioning of the internal market by \n",
      "laying down a uniform legal framework in particular for the development, marketing \n",
      "and use of artificial intelligence in conformity with Union values. This Regulation \n",
      "pursues a number of overriding reasons of public interest, such as a high level of \n",
      "protection of health, safety and fundamental rights, and it ensures the free movement \n",
      "of AI -based goods and services cross -border, thus preventing Member States from \n",
      "imposing restrictions on th e development, marketing and use of AI systems, unless \n",
      "explicitly authorised by this Regulation.  \n",
      "(2) Artificial intelligence systems (AI systems) can be easily deployed in multiple sectors \n",
      "of the economy and society, including cross border, and circulate throughout the \n",
      "Union. Certain Member States have already explored the adoption of national rules to \n",
      "ensure that artificial intelligence is safe and is developed and used in compliance with \n",
      "fundamental rights obligations. Differing national rules may lead t o fragmentation of \n",
      "the internal market and decrease legal certainty for operators that develop or use AI \n",
      "systems. A consistent and high level of protection throughout the Union should \n",
      "therefore be ensured, while divergences hampering the free circulation o f AI systems \n",
      "and related products and services within the internal market should be prevented, by \n",
      "laying down uniform obligations for operators and guaranteeing the uniform \n",
      "protection of overriding reasons of public interest and of rights of persons throug hout \n",
      "the internal market based on Article 114 of the Treaty on the Functioning of the \n",
      "European Union (TFEU). To the extent that this Regulation contains specific rules on \n",
      "the protection of individuals with regard to the processing of personal data  concerni ng \n",
      "                                                 \n",
      "31 OJ C […], […], p. […].  \n",
      "32 OJ C […], […], p. […].  EN 18  EN restrictions of the use of AI systems for ‘real -time’ remote biometric identification in \n",
      "publicly accessible spaces for the purpose of law enforcement, it is appropriate to base \n",
      "this Regulation, in as far as those specific rules are concerned, on Articl e 16 of the \n",
      "TFEU. In light of  those specific rules and the recourse to Article 16 TFEU, it is \n",
      "appropriate  to consult the European Data Protection Board.  \n",
      "(3) Artificial intelligence is a fast evolving family of technologies that can contribute to a \n",
      "wide arr ay of economic and societal benefits across the entire spectrum of industries \n",
      "and social activities. By improving prediction, optimising operations and resource \n",
      "allocation, and personalising digital solutions available for individuals and \n",
      "organisations, th e use of artificial intelligence can provide key competitive advantages \n",
      "to companies and support socially and environmentally beneficial outcomes, for \n",
      "example in healthcare, farming, education and training, infrastructure management, \n",
      "energy, transport and logistics, public services, security, justice, resource and energy \n",
      "efficiency, and climate change mitigation and adaptation.  \n",
      "(4) At the same time, depending on the circumstances regarding its specific application \n",
      "and use, artificial intelligence may genera te risks and cause harm to public interests \n",
      "and rights that are protected by Union law. Such harm might be material or \n",
      "immaterial.  \n",
      "(5) A Union legal framework laying down harmonised rules on artificial intelligence is \n",
      "therefore needed to foster the develop ment, use and uptake of artificial intelligence in \n",
      "the internal market that at the same time meets a high level of protection of public \n",
      "interests, such as health and safety and the protection of fundamental rights, as \n",
      "recognised and protected by Union law.  To achieve that objective, rules regulating the \n",
      "placing on the market and putting into service of certain AI systems should be laid \n",
      "down, thus ensuring the smooth functioning of the internal market and allowing those \n",
      "systems to benefit from the principle of free movement of goods and services. By \n",
      "laying down those rules, this Regulation supports the objective of the Union of being a \n",
      "global leader in the development of secure, trustworthy and ethical artificial \n",
      "intelligence, as stated by the European Counci l33, and it ensures the protection of \n",
      "ethical principles, as specifically requested by the European Parliament34. \n",
      "(6) The notion of AI system should be clearly defined to ensure legal certainty, while \n",
      "providing the flexibility to accommodate future technolog ical developments. The \n",
      "definition should be based on the key functional characteristics of the software, in \n",
      "particular the ability, for a given set of human -defined objectives, to generate outputs \n",
      "such as content, predictions, recommendations, or decisions  which influence the \n",
      "environment with which the system interacts, be it in a physical or digital dimension. \n",
      "AI systems can be designed to operate with varying levels of autonomy and be used on \n",
      "a stand -alone basis or as a component of a product, irrespectiv e of whether the system \n",
      "is physically integrated into the product (embedded) or serve the functionality of the \n",
      "product without being integrated therein (non -embedded). The definition of AI system \n",
      "should be complemented by a list of specific techniques and approaches used for its \n",
      "development, which should be kept up -to–date in the light of market and technological \n",
      "                                                 \n",
      "33 European Council, Special meeting of the European Council (1 and 2 October 2020) – Conclusions, \n",
      "EUCO 13/20, 2020, p. 6.  \n",
      "34 European Parliament resolution of 20 October 2020 with recommendations to the Commissio n on a \n",
      "framework of ethical aspects of artificial intelligence, robotics and related technologies, \n",
      "2020/2012(INL).  EN 19  EN developments through the adoption of delegated acts by the Commission to amend that \n",
      "list. \n",
      "(7) The notion of biometric data used in this Regulation  is in line with and should be \n",
      "interpreted consistently with the notion of biometric data as defined in Article 4(14) of \n",
      "Regulation (EU) 2016/679 of the European Parliament and of the Council35, Article \n",
      "3(18) of Regulation (EU) 2018/1725 of the European Par liament and of the Council36 \n",
      "and Article 3(13) of Directive (EU) 2016/680 of the European Parliament and of the \n",
      "Council37.   \n",
      "(8) The notion of remote biometric identification system as used in this Regulation should \n",
      "be defined functionally, as  an AI system  intended for the identification of natural \n",
      "persons at a distance through the comparison of a person’s biometric data with the \n",
      "biometric data contained in a reference database, and without prior knowledge whether \n",
      "the targeted person will be present and can  be identified, irrespectively of the \n",
      "particular technology, processes or types of biometric data used. Considering their \n",
      "different characteristics and manners in which they are used, as well as the different \n",
      "risks involved, a distinction should be made be tween ‘real -time’ and ‘post’ remote \n",
      "biometric identification systems. In the case of ‘real -time’ systems, the capturing of \n",
      "the biometric data, the comparison and the identification occur all instantaneously, \n",
      "near-instantaneously or in any event without a s ignificant delay. In this regard, there \n",
      "should be no scope for circumventing the rules of this Regulation on the ‘real -time’ \n",
      "use of the AI systems in question by providing for minor delays. ‘Real -time’ systems \n",
      "involve the use of ‘live’ or ‘near -‘live’ mate rial, such as video footage, generated by a \n",
      "camera or other device with similar functionality. In the case of ‘post’ systems, in \n",
      "contrast, the biometric data have already been captured and the comparison and \n",
      "identification occur only after a significant de lay. This involves material, such as \n",
      "pictures or video footage generated by  closed circuit television cameras or private \n",
      "devices, which has been generated before the use of the system in respect of the \n",
      "natural persons concerned.  \n",
      "(9) For the purposes of thi s Regulation the notion of publicly accessible space should be \n",
      "understood as referring to any physical place that is accessible to the public, \n",
      "irrespective of whether the place in question is privately or publicly owned. Therefore, \n",
      "the notion does not cove r places that are private in nature and normally not freely \n",
      "accessible for third parties, including law enforcement authorities, unless those parties \n",
      "have been specifically invited or authorised, such as homes, private clubs, offices, \n",
      "warehouses and factor ies.  Online spaces are not covered either, as they are not \n",
      "physical spaces. However, the mere fact that certain conditions for accessing a \n",
      "                                                 \n",
      "35 Regulation  (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the \n",
      "protection of natural persons with regard to  the processing of personal data and on the free movement of \n",
      "such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, \n",
      "p. 1).  \n",
      "36 Regulation (EU) 2018/1725 of the European Parliament and of the Council of 23 Octobe r 2018 on the \n",
      "protection of natural persons with regard to the processing of personal data by the Union institutions, \n",
      "bodies, offices and agencies and on the free movement of such data, and repealing Regulation (EC) No \n",
      "45/2001 and Decision No 1247/2002/EC (OJ L 295, 21.11.2018, p. 39)  \n",
      "37 Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the \n",
      "protection of natural persons with regard to the processing of personal data by competent authorities for \n",
      "the purposes of the prevention, investigation, detection or prosecution of criminal offences or the \n",
      "execution of criminal penalties, and on the free movement of such data, and repealing Council \n",
      "Framework Decision 2008/977/JHA (Law Enforcement Directive) ( OJ L 119, 4.5.2016, p . 89).  EN 20  EN particular space may apply, such as admission tickets or age restrictions, does not \n",
      "mean that the space is not publi cly accessible within the meaning of this Regulation. \n",
      "Consequently, in addition to public spaces such as streets, relevant parts of \n",
      "government buildings and most transport infrastructure, spaces such as cinemas, \n",
      "theatres, shops and shopping centres are nor mally also publicly accessible. Whether a \n",
      "given space is accessible to the public should however be determined on a case -by-\n",
      "case basis, having regard to the specificities of the individual situation at hand.   \n",
      "(10) In order to ensure a level playing field a nd an effective protection of rights and \n",
      "freedoms of individuals across the Union, the rules established by this Regulation \n",
      "should apply to providers of AI systems in a non -discriminatory manner, irrespective \n",
      "of whether they are established within the Unio n or in a third country, and to users of \n",
      "AI systems established within the Union.  \n",
      "(11) In light of their digital nature, certain AI systems should fall within the scope of this \n",
      "Regulation even when they are neither placed on the market, nor put into servic e, nor \n",
      "used in the Union. This is the case for example of an operator established in the Union \n",
      "that contracts certain services to an operator established outside the Union in relation \n",
      "to an activity to be performed by an AI system that would qualify as hig h-risk and \n",
      "whose effects impact natural persons located in the Union. In those circumstances, the \n",
      "AI system used by the operator outside the Union could process data lawfully \n",
      "collected in and transferred from the Union, and provide to the contracting opera tor in \n",
      "the Union the output of that AI system resulting from that processing, without that AI \n",
      "system being placed on the market, put into service or used in the Union. To prevent \n",
      "the circumvention of this Regulation and to ensure an effective protection of  natural \n",
      "persons located in the Union, this Regulation should also apply to providers and users \n",
      "of AI systems that are established in a third country, to the extent the output produced \n",
      "by those systems is used in the Union. Nonetheless, to take into accoun t existing \n",
      "arrangements and special needs for cooperation with foreign partners with whom \n",
      "information and evidence is exchanged, this Regulation should not apply to public \n",
      "authorities of a third country and international organisations when acting in the \n",
      "framework of international agreements concluded at national or European level for law \n",
      "enforcement and judicial cooperation with the Union or with its Member States. Such \n",
      "agreements have been concluded bilaterally between Member States and third \n",
      "countries or between the European Union, Europol and other EU agencies and third \n",
      "countries and international organisations.  \n",
      "(12) This Regulation should also apply to Union institutions, offices, bodies and agencies \n",
      "when acting as a provider or user of an AI system. AI  systems exclusively developed \n",
      "or used for military purposes should be excluded from the scope of this Regulation \n",
      "where that use falls under the exclusive remit of the Common Foreign and Security \n",
      "Policy regulated under Title V of the Treaty on the European  Union (TEU).  This \n",
      "Regulation should be without prejudice to the provisions regarding the liability of \n",
      "intermediary service providers set out in Directive 2000/31/EC of the European \n",
      "Parliament and of the Council [as amended by the Digital Services Act].  \n",
      "(13) In order to ensure a consistent and high level of protection of public interests as \n",
      "regards health, safety and fundamental rights, common normative standards for all \n",
      "high-risk AI systems should be established. Those standards should be consistent with  \n",
      "the Charter of fundamental rights of the European Union (the Charter) and should be \n",
      "non-discriminatory and in line with the Union’s international trade commitments.  EN 21  EN (14) In order  to introduce a proportionate and effective set of binding rules for AI syste ms, \n",
      "a clearly defined risk -based approach should be followed. That approach should tailor \n",
      "the type and content of such rules to the intensity and scope of the risks that AI \n",
      "systems can generate. It is therefore necessary to prohibit certain artificial inte lligence \n",
      "practices, to lay down requirements for high -risk AI systems and obligations for the \n",
      "relevant operators, and to lay down transparency obligations for certain AI systems.  \n",
      "(15) Aside from the many beneficial uses of artificial intelligence, that tec hnology can also \n",
      "be misused and provide novel and powerful tools for manipulative, exploitative and \n",
      "social control practices. Such practices are particularly harmful and should be \n",
      "prohibited because they contradict Union values of respect for human dignity , \n",
      "freedom, equality, democracy and the rule of law and Union fundamental rights, \n",
      "including the right to non -discrimination, data protection and privacy  and the rights of \n",
      "the child . \n",
      "(16) The placing on the market, putting into service or use of certain AI s ystems intended \n",
      "to distort human  behaviour, whereby physical or psychological harms are likely to \n",
      "occur, should be forbidden. Such AI systems deploy subliminal components \n",
      "individuals cannot perceive or exploit vulnerabilities of children and people due to \n",
      "their age, physical or mental incapacities. They do so with the intention to materially \n",
      "distort the behaviour of a person and in a manner that causes or is likely to cause harm \n",
      "to that or another person. The intention may not be presumed if the distortion of \n",
      "human behaviour results from factors external to the AI system which are outside of \n",
      "the control of the provider or the user. Research for legitimate purposes in relation to \n",
      "such AI systems should not be stifled by the prohibition, if such research does not \n",
      "amount to use of the AI system in human -machine relations that exposes natural \n",
      "persons to harm and such research is carried out in accordance with recognised ethical \n",
      "standards for scientific research.  \n",
      "(17) AI systems  providing social scoring of natural  persons for general purpose by public \n",
      "authorities or on their behalf may lead to discriminatory outcomes and the exclusion of \n",
      "certain groups. They may violate the right to dignity and non -discrimination and the \n",
      "values of equality and justice. Such AI syst ems evaluate or classify the trustworthiness \n",
      "of natural persons based on their social behaviour in multiple contexts or known or \n",
      "predicted personal or personality characteristics. The social score obtained from such \n",
      "AI systems may lead to the detrimental o r unfavourable treatment of natural persons or \n",
      "whole groups thereof in social contexts, which are unrelated to the context in which \n",
      "the data was originally generated or collected or to a detrimental treatment that is \n",
      "disproportionate or unjustified to the gravity of their social behaviour.  Such AI \n",
      "systems should be therefore prohibited.  \n",
      "(18) The use of AI systems for ‘real -time’ remote biometric identification of natural \n",
      "persons in publicly accessible spaces for the purpose of law enforcement is considered \n",
      "particularly intrusive in the rights and freedoms of the concerned persons, to the extent \n",
      "that it may affect the private life of a large part of the population, evoke a feeling of \n",
      "constant surveillance and indirectly dissuade the exercise of the freedom of  assembly \n",
      "and other fundamental rights. In addition, the immediacy of the impact and the limited \n",
      "opportunities for further checks or corrections  in relation to the use of such systems \n",
      "operating in ‘real -time’ carry heightened risks for the rights and freed oms of the \n",
      "persons that are concerned by law enforcement activities.  \n",
      "(19) The use of those systems for the purpose of law enforcement should therefore be \n",
      "prohibited, except in three exhaustively listed and narrowly defined situations, where EN 22  EN the use is stri ctly necessary to achieve a substantial public interest, the importance of \n",
      "which outweighs the risks. Those situations involve the search for potential victims of \n",
      "crime, including missing children; certain threats to the life or physical safety of \n",
      "natural persons or of a terrorist attack; and the detection, localisation, identification or \n",
      "prosecution of perpetrators or suspects of the criminal offences referred to in Council \n",
      "Framework Decision 2002/584/JHA38 if those criminal offences are punishable in the \n",
      "Member State concerned by a custodial sentence or a detention order for a maximum \n",
      "period of at least three years and as they are defined in the law of that Member State . \n",
      "Such threshold for the custodial sentence or detention order in accordance with \n",
      "nationa l law contributes to ensure that the offence should be serious enough to \n",
      "potentially justify the use of ‘real -time’ remote biometric identification systems. \n",
      "Moreover, of the 32 criminal offences listed in the Council Framework Decision \n",
      "2002/584/JHA, some a re in practice likely to be more relevant than others, in that the \n",
      "recourse to ‘real-time’ remote biometric identification  will foreseeably be necessary \n",
      "and proportionate to highly varying degrees for the practical pursuit of the detection, \n",
      "localisation, i dentification or prosecution of a perpetrator or suspect of the different \n",
      "criminal offences listed and having regard to the likely differences in  the seriousness, \n",
      "probability and scale of the harm or possible negative consequences.  \n",
      "(20) In order to ensure that those systems are used in a responsible and proportionate \n",
      "manner, it is also important to establish that, in each of those three exhaustively listed \n",
      "and narrowly defined situations, certain elements should be taken into account, in \n",
      "particular as regar ds the nature of the situation giving rise to the request and the \n",
      "consequences of the use for the rights and freedoms of all persons concerned  and the \n",
      "safeguards and conditions provided for with the use. In addition, the use of ‘real -time’ \n",
      "remote biometric identification systems in publicly accessible spaces for the purpose \n",
      "of law enforcement should be subject to appropriate limits in time and space, hav ing \n",
      "regard in particular to the evidence or indications regarding the threats, the victims or \n",
      "perpetrator.  The reference database of persons should be appropriate for each use case \n",
      "in each of the three situations mentioned above.  \n",
      "(21) Each use of a ‘real -time’ remote biometric identification system in publicly accessible \n",
      "spaces for the purpose of law enforcement should be subject to an express and specific \n",
      "authorisation by a judicial authority or by an independent administrative authority of a \n",
      "Member State.  Such authorisation should in principle be obtained prior to the use, \n",
      "except in duly justified situations of urgency, that is, situations where the need to use \n",
      "the systems in question is such as to make it effectively and objectively impossible to \n",
      "obtain a n authorisation before commencing the use. In such situations of urgency, the \n",
      "use should be restricted to the absolute minimum necessary and be subject to \n",
      "appropriate safeguards and conditions, as determined in national law and specified in \n",
      "the context of each individual urgent use case by the law enforcement authority itself. \n",
      "In addition, the law enforcement authority should in such situations seek to obtain an \n",
      "authorisation as soon as possible, whilst providing the reasons for not having been able \n",
      "to requ est it earlier.  \n",
      "(22) Furthermore, it is appropriate to provide, within the exhaustive framework set by this \n",
      "Regulation that such use in the territory of a Member State in accordance with this \n",
      "Regulation should only be possible where and in as far as the M ember State in \n",
      "question has decided to expressly provide for the possibility to authorise such use in its \n",
      "                                                 \n",
      "38 Council Framework Decision 2002/584/JHA of 13 June 2002 on the European arrest warrant and the \n",
      "surrender procedures between Member States ( OJ L 190, 18.7.2002, p. 1).  EN 23  EN detailed rules of national law. Consequently, Member States remain free under this \n",
      "Regulation not to provide for such a possibility at all or to only provide for such a \n",
      "possibility in respect of some of the objectives capable of justifying authorised use \n",
      "identified in this Regulation.  \n",
      "(23) The use of AI systems for ‘real -time’ remote biometric identification of natural \n",
      "persons in publicly accessible spa ces for the purpose of law enforcement necessarily \n",
      "involves the processing of biometric data. The rules of this Regulation that prohibit, \n",
      "subject to certain exceptions, such use, which are based on Article 16 TFEU, should \n",
      "apply as lex specialis  in respect of the rules on the processing of biometric data \n",
      "contained in Article 10 of Directive (EU) 2016/680, thus regulating such use and the \n",
      "processing of biometric data involved in an exhaustive manner. Therefore, such use \n",
      "and processing should only be possible in as far as it is compatible with the framework \n",
      "set by this Regulation, without there being scope, outside that framework, for the \n",
      "competent authorities, where they act for purpose of law enforcement, to use such \n",
      "systems and process such data in connectio n thereto on the grounds listed in Article 10 \n",
      "of Directive (EU) 2016/680. In this context, this Regulation is not intended to provide \n",
      "the legal basis for the processing of personal data under Article 8 of Directive \n",
      "2016/680. However , the use of ‘real -time’  remote biometric identification systems in \n",
      "publicly accessible spaces for purposes other than law enforcement, including by \n",
      "competent authorities, should not be covered by the specific framework regarding such \n",
      "use for the purpose of law enforcement set by  this Regulation. Such use for purposes \n",
      "other than law enforcement should therefore not be subject to the requirement of an \n",
      "authorisation under this Regulation  and the applicable detailed rules of national law \n",
      "that may give effect to it.  \n",
      "(24) Any processin g of biometric data and other personal data involved in the use of AI \n",
      "systems for biometric identification, other than in connection to the use of ‘real -time’ \n",
      "remote biometric identification systems in publicly accessible spaces for the purpose \n",
      "of law enfo rcement as regulated by this Regulation, including where those systems are \n",
      "used by competent authorities in publicly accessible spaces for other purposes than \n",
      "law enforcement, should continue to comply with all requirements resulting from \n",
      "Article 9(1) of R egulation (EU) 2016/679, Article 10(1) of Regulation (EU) \n",
      "2018/1725 and Article 10 of Directive (EU) 2016/680, as applicable.   \n",
      "(25) In accordance with Article 6a of Protocol No 21 on the position of the United \n",
      "Kingdom and Ireland in respect of the area of freedom, security and justice, as \n",
      "annexed to the TEU and to the TFEU, Ireland is not bound by the rules laid down in \n",
      "Article 5(1), point (d), (2) and (3) of this Regulation adopted on the basis of Article 16 \n",
      "of the TFEU which relate to the processing of pe rsonal data by the Member States \n",
      "when carrying out activities falling within the scope of Chapter 4 or Chapter 5 of Title \n",
      "V of Part Three of the TFEU, where Ireland is not bound by the rules governing the \n",
      "forms of judicial cooperation in criminal matters o r police cooperation which require \n",
      "compliance with the provisions laid down on the basis of Article 16 of the TFEU.   \n",
      "(26) In accordance with Articles 2 and 2a of Protocol No 22 on the position of Denmark, \n",
      "annexed to the TEU and TFEU, Denmark is not bound b y rules laid down in Article \n",
      "5(1), point (d), (2) and (3) of this Regulation adopted on the basis of Article 16 of the \n",
      "TFEU, or subject to their application, which relate to the processing of personal data \n",
      "by the Member States when carrying out activities falling within the scope of Chapter \n",
      "4 or Chapter 5 of Title V of Part Three of the TFEU.   EN 24  EN (27) High -risk AI systems should only be placed on the Union market or put into service if \n",
      "they comply with certain mandatory requirements. Those requirements should ensure \n",
      "that high -risk AI systems available in the Union or whose output is otherwise used in \n",
      "the Union do not pose unacceptable risks to important Union public interests  as \n",
      "recognised and protected by Union law . AI systems identified as high -risk should be  \n",
      "limited to those that have a significant harmful impact on the health, safety and \n",
      "fundamental rights of persons in the Union and such limitation minimises any \n",
      "potential restriction to international trade, if any.  \n",
      "(28) AI systems could produce adverse outc omes to health and safety of persons, in \n",
      "particular when such systems operate as components of products. Consistently with \n",
      "the objectives of Union harmonisation legislation to facilitate the free movement of \n",
      "products in the internal market and to ensure th at only safe and otherwise compliant \n",
      "products find their way into the market, it is important that the safety risks that may be \n",
      "generated by a product as a whole due to its digital components, including AI systems, \n",
      "are duly prevented and mitigated. For ins tance, increasingly autonomous robots, \n",
      "whether in the context of manufacturing or personal assistance and care should be able \n",
      "to safely operate and performs their functions in complex environments. Similarly, in \n",
      "the health sector where the stakes for life and health are particularly high, increasingly \n",
      "sophisticated diagnostics systems and systems supporting human decisions should be \n",
      "reliable and accurate. The extent of the adverse impact caused by the AI system on the \n",
      "fundamental rights protected by the Cha rter is of particular relevance when classifying \n",
      "an AI system as high -risk. Those rights include the right to human dignity, respect for \n",
      "private and family life, protection of personal data, freedom of expression and \n",
      "information, freedom of assembly and of  association, and non -discrimination, \n",
      "consumer protection, workers’ rights,  rights of persons with disabilities, right to an \n",
      "effective remedy and to a fair trial, right of defence and the presumption of innocence, \n",
      "right to good administration.  In addition to those rights, it is important to highlight that \n",
      "children have specific rights as enshrined in Article 24 of the EU Charter and in the \n",
      "United Nations Convention on the Rights of the Child (further elaborated in the \n",
      "UNCRC General Comment No. 25 as regards  the digital environment), both of which \n",
      "require consideration of the children’s vulnerabilities and provision of such protection \n",
      "and care as necessary for their well -being.  The fundamental right to a high level of \n",
      "environmental protection enshrined in the  Charter and implemented in Union policies \n",
      "should also be considered when assessing the severity of the harm that an AI system \n",
      "can cause, including in relation to the health and safety of persons.  \n",
      "(29) As regards high -risk AI systems that are safety compon ents of products or systems, or \n",
      "which are themselves products or systems falling within the scope of Regulation (EC) \n",
      "No 300/2008 of the European Parliament and of the Council39, Regulation (EU) No \n",
      "167/2013  of the European Parliament and of the Council40, Reg ulation (EU) No \n",
      "168/2013  of the European Parliament and of the Council41, Directive 2014/90/EU  of \n",
      "                                                 \n",
      "39 Regulation (EC) No 300/2008 of the European Parliament and of the Council of 11 March 2008 on \n",
      "common rules in the field of civil aviation security and repealing Regulation (EC) No 2320/2002 (OJ L \n",
      "97, 9.4.2008, p. 72).  \n",
      "40 Regulation (EU) No 167/2013 of the European Parliament and of the Council of 5 February 2013 on the \n",
      "approval and market surveillance of agricultural and forestry vehicles (OJ L 60, 2.3.2013, p. 1).  \n",
      "41 Regulation (EU) No 168/2013 of the European Parliament and of the Council of 15 January 2013 on the \n",
      "approval and market surveillance of two - or three -wheel vehicles and quadricycles (OJ L 60, 2.3.2013, \n",
      "p. 52).  EN 25  EN the European Parliament and of the Council42, Directive (EU) 2016/797  of the \n",
      "European Parliament and of the Council43, Regulation (EU) 2018/858  of the European \n",
      "Parliament and of the Council44, Regulation (EU) 2018/1139 of the European \n",
      "Parliament and of the Council45, and Regulation (EU) 2019/2144  of the European \n",
      "Parliament and of the Council46, it is appropriate to amend those acts to ensure that the \n",
      "Commission take s into account, on the basis of the technical and regulatory \n",
      "specificities of each sector, and without interfering with existing governance, \n",
      "conformity assessment and enforcement mechanisms and authorities established \n",
      "therein, the mandatory requirements fo r high -risk AI systems laid down in this \n",
      "Regulation when adopting any relevant future delegated or implementing acts on the \n",
      "basis of those acts.  \n",
      "(30) As regards AI systems that are safety components of products, or which are \n",
      "themselves products, falling wi thin the scope of certain Union harmonisation \n",
      "legislation, it is appropriate to classify them as high -risk under this Regulation if the \n",
      "product in question undergoes the conformity assessment procedure with a third -party \n",
      "conformity assessment body pursuant  to that relevant Union harmonisation legislation. \n",
      "In particular, such products are machinery, toys, lifts, equipment and protective \n",
      "systems intended for use in potentially explosive atmospheres, radio equipment, \n",
      "pressure equipment, recreational craft equi pment, cableway installations,  appliances \n",
      "burning gaseous fuels, medical devices, and in vitro diagnostic medical devices.  \n",
      "(31) The classification of an AI system as high -risk pursuant to this Regulation should not \n",
      "necessarily mean that the product whose s afety component is the AI system, or the AI \n",
      "system itself as a product, is considered ‘high -risk’ under the criteria established in the \n",
      "relevant Union harmonisation legislation that applies to the product. This is notably \n",
      "the case for Regulation (EU) 2017/ 745 of the European Parliament and of the \n",
      "                                                 \n",
      "42 Directive 2014/90/EU of the European Parliament and of the Council of 23 July 2014 on marine \n",
      "equipment and repealing Council Directive 96/98/EC (OJ L 257, 28.8.2014, p. 146).  \n",
      "43 Directive (EU) 2016/797 of the Europ ean Parliament and of the Council of 11 May 2016 on the \n",
      "interoperability of the rail system within the European Union (OJ L 138, 26.5.2016, p. 44).  \n",
      "44 Regulation (EU) 2018/858 of the European Parliament and of the Council of 30 May 2018 on the \n",
      "approval and market surveillance of motor vehicles and their trailers, and of systems, components and \n",
      "separate technical units intended for such vehicles, amending Regulations (EC) No 715/2007 and (EC) \n",
      "No 595/2009 and repealing Directive 2007/46/EC (OJ L 151, 14.6.2018 , p. 1).  \n",
      "45 Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common \n",
      "rules in the field of civil aviation and establishing a European Union Aviation Safety Agency, and \n",
      "amending Regulations (EC) No 2111/2005, (EC) No 1 008/2008, (EU) No 996/2010, (EU) No 376/2014 \n",
      "and Directives 2014/30/EU and 2014/53/EU of the European Parliament and of the Council, and \n",
      "repealing Regulations (EC) No 552/2004 and (EC) No 216/2008 of the European Parliament and of the \n",
      "Council and Council R egulation (EEC) No 3922/91 (OJ L 212, 22.8.2018, p. 1).  \n",
      "46 Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November 2019 on \n",
      "type-approval requirements for motor vehicles and their trailers, and systems, components and separate \n",
      "technical units intended for such vehicles, as regards their general safety and the protection of vehicle \n",
      "occupants and vulnerable road users, amending Regulation (EU) 2018/858 of the European Parliament \n",
      "and of the Council and repealing Regulations (EC) No  78/2009, (EC) No 79/2009 and (EC) No \n",
      "661/2009 of the European Parliament and of the Council and Commission Regulations (EC) No \n",
      "631/2009, (EU) No 406/2010, (EU) No 672/2010, (EU) No 1003/2010, (EU) No 1005/2010, (EU) No \n",
      "1008/2010, (EU) No 1009/2010, (EU) N o 19/2011, (EU) No 109/2011, (EU) No 458/2011, (EU) No \n",
      "65/2012, (EU) No 130/2012, (EU) No 347/2012, (EU) No 351/2012, (EU) No 1230/2012 and (EU) \n",
      "2015/166 (OJ L 325, 16.12.2019, p. 1).  EN 26  EN Council47 and Regulation (EU) 2017/746 of the European Parliament and of the \n",
      "Council48, where a third -party conformity assessment is provided for medium -risk and \n",
      "high-risk products.   \n",
      "(32) As regards stand -alone AI sy stems, meaning high -risk AI systems other than those that \n",
      "are safety components of products, or which are themselves products, it is appropriate \n",
      "to classify them as high -risk if, in the light of their intended purpose, they pose a high \n",
      "risk of harm to the health and safety or the fundamental rights of persons, taking into \n",
      "account both the severity of the possible harm and its probability of occurrence and \n",
      "they are used in a number of specifically pre -defined areas specified in the Regulation. \n",
      "The identifica tion of those systems is based on the same methodology and criteria \n",
      "envisaged also for any future amendments of the list of high -risk AI systems.  \n",
      "(33) Technical inaccuracies of AI systems intended for the remote biometric identification \n",
      "of natural persons can lead to biased results and entail discriminatory effects. This is \n",
      "particularly relevant when it comes to age, ethnicity, sex or disabilities. Therefore, \n",
      "‘real-time’ and ‘post’ remote biometric identification systems should be classified as \n",
      "high-risk. In view of the risks that they pose, both types of remote biometric \n",
      "identification systems should be subject to specific requirements on logging \n",
      "capabilities and human oversight.   \n",
      "(34) As regards the management and operation of critical infrastructure, it i s appropriate to \n",
      "classify as high -risk the AI systems intended to be used as safety components in the \n",
      "management and operation of road traffic and the supply of water, gas, heating and \n",
      "electricity, since their failure or malfunctioning may put at risk the life and health of \n",
      "persons at large scale and lead to appreciable disruptions in the ordinary conduct of \n",
      "social and economic activities.  \n",
      "(35) AI systems used in education or vocational training, notably for determining access or \n",
      "assigning persons to educa tional and vocational training institutions or to evaluate \n",
      "persons on tests as part of or as a precondition for their education should be considered \n",
      "high-risk, since they may determine the educational and professional course of a \n",
      "person’s life and therefor e affect their ability to secure their livelihood. When \n",
      "improperly designed and used, such systems may violate the right to education and \n",
      "training as well as the right not to be discriminated against and perpetuate historical \n",
      "patterns of discrimination.  \n",
      "(36) AI systems used in employment, workers management and access to self -employment, \n",
      "notably for the recruitment and selection of persons, for making decisions on \n",
      "promotion and termination and for task allocation, monitoring or evaluation of persons \n",
      "in work -related contractual relationships, should also be classified as high -risk, since \n",
      "those systems may appreciably impact future career prospects and livelihoods of these \n",
      "persons. Relevant work -related contractual relationships should involve employees \n",
      "and pe rsons providing services through platforms as referred to in the Commission \n",
      "Work Programme 2021. Such persons should in principle not be considered users \n",
      "within the meaning of this Regulation. Throughout the recruitment process and in the \n",
      "                                                 \n",
      "47 Regulation (EU) 2017/745 of the European Parliament and of the Council  of 5 April 2017 on medical \n",
      "devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No \n",
      "1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (OJ L 117, 5.5.2017, p. 1).  \n",
      "48 Regulation (EU) 2017/746 of the Europea n Parliament and of the Council of 5 April 2017 on in vitro \n",
      "diagnostic medical devices and repealing Directive 98/79/EC and Commission Decision 2010/227/EU \n",
      "(OJ L 117, 5.5.2017, p. 176).  EN 27  EN evaluation, promo tion, or retention of persons in work -related contractual \n",
      "relationships, such systems may perpetuate historical patterns of discrimination, for \n",
      "example against women, certain age groups, persons with disabilities, or persons of \n",
      "certain racial or ethnic ori gins or sexual orientation. AI systems used to monitor the \n",
      "performance and behaviour of these persons may also impact their rights to data \n",
      "protection and privacy.  \n",
      "(37) Another area in which the use of AI systems deserves special consideration is the \n",
      "acces s to and enjoyment of certain essential private and public services and benefits \n",
      "necessary for people  to fully participate in society or to improve one’s standard of \n",
      "living . In particular, AI systems used to evaluate the credit score or creditworthiness of  \n",
      "natural persons should be classified as high -risk AI systems, since they determine \n",
      "those persons’ access to financial resources or essential services such as housing, \n",
      "electricity, and telecommunication services. AI systems used for this purpose may lead \n",
      "to discrimination of persons or groups and perpetuate historical patterns of \n",
      "discrimination, for example based on racial or ethnic origins, disabilities, age, sexual \n",
      "orientation, or create new forms of discriminatory impacts. Considering the very \n",
      "limited sc ale of the impact and the available alternatives on the market, it is \n",
      "appropriate to exempt AI systems for the purpose of creditworthiness assessment and \n",
      "credit scoring when put into service by small -scale providers for their own use. \n",
      "Natural persons apply ing for or receiving public assistance benefits and services from \n",
      "public authorities are typically dependent on those benefits and services and in a \n",
      "vulnerable position in relation to the responsible authorities. If AI systems are used for \n",
      "determining whet her such benefits and services should be denied, reduced, revoked or \n",
      "reclaimed by authorities, they may have a significant impact on persons’ livelihood \n",
      "and may infringe their fundamental rights, such as the right to social protection, non -\n",
      "discrimination, human dignity or an effective remedy. Those systems should therefore \n",
      "be classified as high -risk. Nonetheless, this Regulation should not hamper the \n",
      "development and use of innovative approaches in the public administration, which \n",
      "would stand to benefit from  a wider use of compliant and safe AI systems, provided \n",
      "that those systems do not entail a high risk to legal and natural persons. Finally, AI \n",
      "systems used to dispatch or establish priority in the dispatching of emergency first \n",
      "response services should als o be classified as high -risk since they make decisions in \n",
      "very critical situations for the life and health of persons and their property.  \n",
      "(38) Actions by law enforcement authorities involving certain uses of AI systems are \n",
      "characterised by a significant de gree of power imbalance and may lead to surveillance, \n",
      "arrest or deprivation of a natural person’s liberty as well as other adverse impacts on \n",
      "fundamental rights guaranteed in the Charter. In particular, if the AI system is not \n",
      "trained with high quality dat a, does not meet adequate requirements in terms of its \n",
      "accuracy or robustness, or is not properly designed and tested before being put on the \n",
      "market or otherwise put into service, it may single out people in a discriminatory or \n",
      "otherwise incorrect or unjus t manner. Furthermore, the exercise of important \n",
      "procedural fundamental rights, such as the right to an effective remedy and to a fair \n",
      "trial as well as the right of defence and the presumption of innocence, could be \n",
      "hampered, in particular, where such AI s ystems are not sufficiently transparent, \n",
      "explainable and documented.  It is therefore appropriate to classify as high -risk a \n",
      "number of AI systems intended to be used in the law enforcement context where \n",
      "accuracy, reliability and transparency is particularly  important to avoid adverse \n",
      "impacts, retain public trust and ensure accountability and effective redress. In view of \n",
      "the nature of the activities in question and the risks relating thereto, those high -risk AI \n",
      "systems should include in particular AI systems  intended to be used by law EN 28  EN enforcement authorities for individual risk assessments, polygraphs and similar tools \n",
      "or to detect the emotional state of natural person, to detect ‘deep fakes’,  for the \n",
      "evaluation of the reliability of evidence in criminal proc eedings, for predicting the \n",
      "occurrence or reoccurrence of an actual or potential criminal offence based on \n",
      "profiling of natural persons, or assessing personality traits and characteristics or past \n",
      "criminal behaviour of natural persons or groups, for profil ing in the course of \n",
      "detection, investigation or prosecution of criminal offences, as well as for  crime \n",
      "analytics regarding natural persons.  AI systems specifically intended to be used for \n",
      "administrative proceedings by tax and customs authorities should no t be considered \n",
      "high-risk AI systems used by law enforcement authorities for the purposes of \n",
      "prevention, detection, investigation and prosecution of criminal offences.   \n",
      "(39) AI systems used in migration, asylum and border control management affect people \n",
      "who are often in particularly vulnerable position and who are dependent on the \n",
      "outcome of the actions of the competent public authorities. The accuracy, non -\n",
      "discriminatory nature and transparency of the AI systems used in those contexts are \n",
      "therefore partic ularly important to guarantee the respect of the fundamental rights of \n",
      "the affected persons, notably their rights to free movement, non -discrimination, \n",
      "protection of private life and personal data, international protection and good \n",
      "administration. It is th erefore appropriate to classify as high -risk AI systems intended \n",
      "to be used by the competent public authorities charged with tasks in the fields of \n",
      "migration, asylum and border control management as polygraphs and similar tools or \n",
      "to detect the emotional s tate of a natural person; for assessing certain risks posed by \n",
      "natural persons entering the territory of a Member State or applying for visa or \n",
      "asylum;  for verifying the authenticity of the relevant documents of natural persons; for \n",
      "assisting competent pub lic authorities for the examination of applications for asylum, \n",
      "visa and residence permits and associated complaints with regard to the objective to \n",
      "establish the eligibility of the natural persons applying for a status.  AI systems in the \n",
      "area of migration , asylum and border control management covered by this Regulation \n",
      "should comply with the relevant procedural requirements set by the Directive \n",
      "2013/32/EU  of the European Parliament and of the Council49, the Regulation (EC) No \n",
      "810/2009 of the European Parliament and of the Council50 and other relevant \n",
      "legislation.  \n",
      "(40) Certain AI systems  intended for the administration of justice and democratic processes  \n",
      "should be classified as high -risk, considering their potentially significant impact on \n",
      "democracy, rul e of law, individual freedoms as well as the right to an effective remedy \n",
      "and to a fair trial. In particular, to address the risks of potential biases, errors and \n",
      "opacity, it is appropriate to qualify as high -risk AI systems intended to assist judicial \n",
      "authorities in researching and interpreting facts and the law and in applying the law to \n",
      "a concrete set of facts. Such qualification should not extend, however, to AI systems \n",
      "intended for purely ancillary administrative activities that do not affect the actua l \n",
      "administration of justice in individual cases, such as anonymisation or \n",
      "pseudonymisation of judicial decisions, documents or data, communication between \n",
      "personnel, administrative tasks or allocation of resources.  \n",
      "                                                 \n",
      "49  Directive 2013/32/EU of the European Parliament and of the Council of 26  June 2013 on common \n",
      "procedures for granting and withdrawing international protection ( OJ L 180, 29.6.2013, p. 60).  \n",
      "50  Regulation (EC) No  810/2009 of the European Parliament and of the Council of 13  July 2009 \n",
      "establishing a Community Code on Visas (Vi sa Code) ( OJ L 243, 15.9.2009, p.  1). \n",
      " EN 29  EN (41) The fact that an AI system is classi fied as high risk under this Regulation should not \n",
      "be interpreted as indicating that the use of the system is necessarily lawful under other \n",
      "acts of Union law or under national law compatible with Union law, such as on the \n",
      "protection of personal data, on t he use of polygraphs and similar tools or other systems \n",
      "to detect the emotional state of natural persons . Any such use should continue to occur \n",
      "solely in accordance with the applicable requirements resulting from the Charter and \n",
      "from the applicable acts of  secondary Union law and national law. This Regulation \n",
      "should not be understood as providing for the legal ground for processing of personal \n",
      "data, including special categories of personal data,  where relevant .  \n",
      "(42) To mitigate the risks from high -risk AI systems placed or otherwise put into service on \n",
      "the Union market for users and affected persons, certain mandatory requirements \n",
      "should apply, taking into account the intended purpose of the use of the system and \n",
      "according to the risk management system to be established by the provider.  \n",
      "(43) Requirements should apply to high -risk AI systems as regards the quality of data sets \n",
      "used, technical documentation and record -keeping, transparency and the provision of \n",
      "informati on to users, human oversight, and robustness, accuracy and cybersecurity. \n",
      "Those requirements are necessary to effectively mitigate the risks for health, safety \n",
      "and fundamental rights, as applicable in the light of the intended purpose of the \n",
      "system, and no  other less trade restrictive measures are reasonably available, thus \n",
      "avoiding unjustified restrictions to trade.  \n",
      "(44) High data quality is essential for the performance of many AI systems, especially \n",
      "when techniques involving the training of models are u sed, with a view to ensure that \n",
      "the high -risk AI system performs as intended and safely and it does not become the \n",
      "source of discrimination prohibited by Union law. High quality training, validation \n",
      "and testing data sets require the implementation of appro priate data governance and \n",
      "management practices. Training, validation and testing data sets should be sufficiently \n",
      "relevant, representative and free of errors and complete in view of the intended \n",
      "purpose of the system. They should also have the appropriate  statistical properties, \n",
      "including as regards the persons or groups of persons on which the high -risk AI \n",
      "system is intended to be used. In particular, training, validation and testing data sets \n",
      "should take into account, to the extent required in the light of their intended purpose, \n",
      "the features, characteristics or elements that are particular to the specific geographical, \n",
      "behavioural or functional setting or context within which the AI system is intended to \n",
      "be used.  In order to protect the right of others f rom the discrimination that might result \n",
      "from the bias in AI systems, the providers shouldbe able to process also special \n",
      "categories of personal data, as a matter of substantial public interest, in order to ensure \n",
      "the bias monitoring, detection and correct ion in relation to high -risk AI systems.  \n",
      "(45) For the development of high -risk AI systems, certain actors, such as providers, \n",
      "notified bodies and other relevant entities, such as digital innovation hubs, testing \n",
      "experimentation facilities and researchers, should be able to access and use high \n",
      "quality datasets within their respective fields of activities which are related to this \n",
      "Regulation. European common data spaces established by the Commission and the \n",
      "facilitation of data sharing between businesses and with government in the public \n",
      "interest will be instrumental to provide trustful, accountable and non -discriminatory \n",
      "access to high quality data for the training, validation and testing of AI systems. For \n",
      "example, in health, the European health data space w ill facilitate non -discriminatory \n",
      "access to health data and the training of artificial intelligence algorithms on those \n",
      "datasets, in a privacy -preserving, secure, timely, transparent and trustworthy manner, \n",
      "and with an appropriate institutional governance.  Relevant competent authorities, EN 30  EN including sectoral ones, providing or supporting the access to data may also support \n",
      "the provision of high -quality data for the training, validation and testing of AI systems.  \n",
      "(46) Having information on how high -risk AI sys tems have been developed and how they \n",
      "perform throughout their lifecycle is essential to verify compliance with the \n",
      "requirements under this Regulation. This requires keeping records and the availability \n",
      "of a technical documentation, containing information which is necessary to assess the \n",
      "compliance of the AI system with the relevant requirements. Such information should \n",
      "include the general characteristics, capabilities and limitations of the system, \n",
      "algorithms, data, training, testing and validation process es used as well as \n",
      "documentation on the relevant risk management system. The technical documentation \n",
      "should be kept up to date.  \n",
      "(47) To address the opacity that may make certain AI systems incomprehensible to or too \n",
      "complex for natural persons, a certain d egree of transparency should be required for \n",
      "high-risk AI systems. Users should be able to interpret the system output and use it \n",
      "appropriately. High -risk AI systems should therefore be accompanied by relevant \n",
      "documentation and instructions of use and incl ude concise and clear information, \n",
      "including in relation to possible risks to fundamental rights and discrimination, where \n",
      "appropriate.  \n",
      "(48) High -risk AI systems should be designed and developed in such a way that natural \n",
      "persons can oversee their function ing. For this purpose, appropriate human oversight \n",
      "measures should be identified by the provider of the system before its placing on the \n",
      "market  or putting into service . In particular, where appropriate, such measures should \n",
      "guarantee that the system is sub ject to in -built operational constraints that cannot be \n",
      "overridden by the system itself and is responsive to the human operator, and that the \n",
      "natural persons to whom human oversight has been assigned have the necessary \n",
      "competence, training and authority to  carry out that role.   \n",
      "(49) High -risk AI systems should perform consistently throughout their lifecycle and meet \n",
      "an appropriate level of accuracy, robustness and cybersecurity in accordance with the \n",
      "generally acknowledged state of the art. The level of acc uracy and accuracy metrics \n",
      "should be communicated to the users.  \n",
      "(50) The technical robustness is a key requirement for high -risk AI systems. They should \n",
      "be resilient against risks connected to the limitations of the system (e.g. errors, faults, \n",
      "inconsiste ncies, unexpected situations) as well as against malicious actions that may \n",
      "compromise the security of the AI system and result in harmful or otherwise \n",
      "undesirable behaviour. Failure to protect against these risks could lead to safety \n",
      "impacts or negatively  affect the fundamental rights, for example due to erroneous \n",
      "decisions or wrong or biased outputs generated by the AI system.  \n",
      "(51) Cybersecurity plays a crucial role in ensuring that AI systems are resilient against \n",
      "attempts to alter their use, behaviour, performance or compromise their security \n",
      "properties by malicious third parties exploiting the system’s vulnerabilities. \n",
      "Cyberattacks against AI systems can leverage AI specific assets, such as training data \n",
      "sets (e.g. data poisoning) or trained models (e.g . adversarial attacks), or exploit \n",
      "vulnerabilities in the AI system’s digital assets or the underlying ICT infrastructure. \n",
      "To ensure a level of cybersecurity appropriate to the risks, suitable measures should \n",
      "therefore be taken by the providers of high -risk AI systems, also taking into account as \n",
      "appropriate the underlying ICT infrastructure.  EN 31  EN (52) As part of Union harmonisation legislation, rules applicable to the placing on the \n",
      "market, putting into service and use of high -risk AI systems should be laid dow n \n",
      "consistently with Regulation (EC) No 765/2008 of the European Parliament and of the \n",
      "Council51 setting out the requirements for accreditation and the market surveillance of \n",
      "products, Decision No 768/2008/EC of the European Parliament and of the Council52 \n",
      "on a common framework for the marketing of products and Regulation (EU) \n",
      "2019/1020 of the European Parliament and of the Council53 on market surveillance \n",
      "and compliance of products  (‘New Legislative Framework for the marketing of \n",
      "products’).  \n",
      "(53) It is appropr iate that a specific natural or legal person, defined as the provider, takes \n",
      "the responsibility for the placing on the market or putting into service of a high -risk AI \n",
      "system, regardless of whether that natural or legal person is the person who designed \n",
      "or developed the system.  \n",
      "(54) The provider should establish a sound quality management system, ensure the \n",
      "accomplishment of the required conformity assessment procedure, draw up the \n",
      "relevant documentation and establish a robust post -market monitoring system. Public \n",
      "authorities which pu t into service high -risk AI systems for their own use may adopt \n",
      "and implement the rules for the quality management system as part of the quality \n",
      "management system adopted at a national or regional level, as appropriate, taking into \n",
      "account the specificitie s of the sector and the competences and organisation of the \n",
      "public authority in question.  \n",
      "(55) Where a high -risk AI system that is a safety component of a product which is covered \n",
      "by a relevant New Legislative Framework sectorial legislation is not placed  on the \n",
      "market or put into service independently from the product, the manufacturer of the \n",
      "final product as defined under the relevant New Legislative Framework legislation \n",
      "should comply with the obligations of the provider established in this Regulation a nd \n",
      "notably ensure that the AI system embedded in the final product complies with the \n",
      "requirements of this Regulation.  \n",
      "(56) To enable enforcement of this Regulation and create a level -playing field for \n",
      "operators, and taking into account the different forms of making available of digital \n",
      "products, it is important to ensure that, under all circumstances, a person established in \n",
      "the Union can provide authorities with all the necessary information on the compliance \n",
      "of an AI system. Therefore, prior to making the ir AI systems available in the Union, \n",
      "where an importer cannot be identified, providers established outside the Union shall, \n",
      "by written mandate, appoint an authorised representative established in the Union.  \n",
      "(57) In line with New Legislative Framework pri nciples, specific obligations for relevant \n",
      "economic operators, such as importers and distributors, should be set to ensure legal \n",
      "certainty and facilitate regulatory compliance by those relevant operators.  \n",
      "                                                 \n",
      "51 Regulation (EC) No 765/2008 of the European Parliament and of the Council of 9 July 2008 setting out \n",
      "the requirements for accreditation and market surveillance relating to the marketing of products and \n",
      "repealing Reg ulation (EEC) No 339/93 (OJ L 218, 13.8.2008, p. 30).  \n",
      "52 Decision No 768/2008/EC of the European Parliament and of the Council of 9 July 2008 on a common \n",
      "framework for the marketing of products, and repealing Council Decision 93/465/EEC (OJ L 218, \n",
      "13.8.2008 , p. 82).  \n",
      "53 Regulation (EU) 2019/1020 of the European Parliament and of the Council of 20 June 2019 on market \n",
      "surveillance and compliance of products and amending Directive 2004/42/EC and Regulations (EC) No \n",
      "765/2008 and (EU) No 305/2011 (Text with EEA rel evance) (OJ L 169, 25.6.2019, p. 1 –44). EN 32  EN (58) Given the nature of AI systems and the risks t o safety and fundamental rights possibly \n",
      "associated with their use, including as regard the need to ensure proper monitoring of \n",
      "the performance of an AI system in a real -life setting, it is appropriate to set specific \n",
      "responsibilities for users. Users shou ld in particular use high -risk AI systems in \n",
      "accordance with the instructions of use and certain other obligations should be \n",
      "provided for with regard to monitoring of the functioning of the AI systems and with \n",
      "regard to record -keeping, as appropriate.   \n",
      "(59) It is appropriate to envisage that the user of the AI system should be the natural or \n",
      "legal person, public authority, agency or other body under whose authority the AI \n",
      "system is operated except where the use is made in the course of a personal non -\n",
      "profes sional activity.  \n",
      "(60) In the light of the complexity of the artificial intelligence value chain, relevant third \n",
      "parties, notably the ones involved in the sale and the supply of software, software tools \n",
      "and components, pre -trained models and data, or provid ers of network services, should \n",
      "cooperate, as appropriate, with providers and users to enable their compliance with the \n",
      "obligations under this Regulation and with competent authorities established under this \n",
      "Regulation.  \n",
      "(61) Standardisation should play a k ey role to provide technical solutions to providers to \n",
      "ensure compliance with this Regulation. Compliance with harmonised standards as \n",
      "defined in Regulation (EU) No 1025/2012 of the European Parliament and of the \n",
      "Council54 should be a means for providers to  demonstrate conformity with the \n",
      "requirements of this Regulation. However, the Commission could adopt common \n",
      "technical specifications in areas where no harmonised standards exist or where they \n",
      "are insufficient.   \n",
      "(62) In order to ensure a high level of trus tworthiness of high -risk AI systems, those \n",
      "systems should be subject to a conformity assessment prior to their placing on the \n",
      "market or putting into service.  \n",
      "(63) It is appropriate that, in order to minimise the burden on operators and avoid any \n",
      "possible d uplication, for high -risk AI systems related to products which are covered by \n",
      "existing Union harmonisation legislation following the New Legislative Framework \n",
      "approach, the compliance of those AI systems with the requirements of this Regulation \n",
      "should be a ssessed as part of the conformity assessment already foreseen under that \n",
      "legislation. The applicability of the requirements of this Regulation should thus not \n",
      "affect the specific logic, methodology or general structure of conformity assessment \n",
      "under the re levant specific New Legislative Framework legislation. This approach is \n",
      "fully reflected in the interplay between this Regulation and the [Machinery \n",
      "Regulation]. While safety risks of AI systems ensuring safety functions in machinery \n",
      "are addressed by the re quirements of this Regulation, certain specific requirements in \n",
      "the [Machinery Regulation] will ensure the safe integration of the AI system into the \n",
      "overall machinery, so as not to compromise the safety of the machinery as a whole. \n",
      "                                                 \n",
      "54 Regulation (EU) No 1025/2012 of the European Parliament and of the Council of 25 October 2012 on \n",
      "European standardisation, amending Council Directives 89/686/EEC and 93/15/EEC and Directives \n",
      "94/9/EC, 94/25/EC, 95/1 6/EC, 97/23/EC, 98/34/EC, 2004/22/EC, 2007/23/EC, 2009/23/EC and \n",
      "2009/105/EC of the European Parliament and of the Council and repealing Council Decision \n",
      "87/95/EEC and Decision No 1673/2006/EC of the European Parliament and of the Council (OJ L 316, \n",
      "14.11. 2012, p. 12).  EN 33  EN The [Machinery Regulati on] applies the same definition of AI system as this \n",
      "Regulation.  \n",
      "(64) Given the more extensive experience of professional pre -market certifiers in the field \n",
      "of product safety and the different nature of risks involved, it is appropriate to limit, at \n",
      "least in an initial phase of application of this Regulation, the scope of application of \n",
      "third -party conformity assessment for high -risk AI systems other than those related to \n",
      "products. Therefore, the conformity assessment of such systems should be carried out \n",
      "as a general rule by the provider under its own responsibility, with the only exception \n",
      "of AI systems intended to be used for the remote biometric identification of persons, \n",
      "for which the involvement of a notified body in the conformity assessment should b e \n",
      "foreseen, to the extent they are not prohibited.  \n",
      "(65) In order to carry out third -party conformity assessment for AI systems intended to be \n",
      "used for the remote biometric identification of persons, notified bodies should be \n",
      "designated under this Regulati on by the national competent authorities, provided they \n",
      "are compliant with a set of requirements, notably on independence, competence and \n",
      "absence of conflicts of interests.  \n",
      "(66) In line with the commonly established notion of substantial modification for products \n",
      "regulated by Union harmonisation legislation, it is appropriate that an AI system \n",
      "undergoes a new conformity assessment whenever a change occurs which may affect \n",
      "the compliance of the system with this Regulation or when the intended purpose of the  \n",
      "system changes. In addition, as regards AI systems which continue to ‘learn’ after \n",
      "being placed on the market or put into service (i.e. they automatically adapt how \n",
      "functions are carried out), it is necessary to provide rules establishing that changes to \n",
      "the algorithm and its performance that have been pre -determined by the provider and \n",
      "assessed at the moment of the conformity assessment should not constitute a \n",
      "substantial modification.   \n",
      "(67) High -risk AI systems should bear the CE marking to indicate thei r conformity with \n",
      "this Regulation so that they can move freely within the internal market. Member States \n",
      "should not create unjustified obstacles to the placing on the market or putting into \n",
      "service of high -risk AI systems that comply with the requirements laid down in this \n",
      "Regulation and bear the CE marking.  \n",
      "(68) Under certain conditions, rapid availability of innovative technologies may be crucial \n",
      "for health and safety of persons and for society as a whole. It is thus appropriate that \n",
      "under exceptional rea sons of public security or protection of life and health of natural \n",
      "persons and the protection of industrial and commercial property, Member States \n",
      "could authorise the placing on the market or putting into service of AI systems which \n",
      "have not undergone a c onformity assessment.  \n",
      "(69) In order to facilitate the work of the Commission and the Member States in the \n",
      "artificial intelligence field as well as to increase the transparency towards the public, \n",
      "providers of high -risk AI systems other than those related t o products falling within \n",
      "the scope of relevant existing Union harmonisation legislation, should be required to \n",
      "register their high -risk AI system in a EU database , to be established and managed by \n",
      "the Commission. The Commission should be the controller of  that database, in \n",
      "accordance with Regulation (EU) 2018/1725 of the European Parliament and of the EN 34  EN Council55. In order to ensure the full functionality of the database, when deployed, the \n",
      "procedure for setting the database should include the elaboration of functional \n",
      "specifications by the Commission and an independent audit report.   \n",
      "(70) Certain AI systems intended to interact with natural persons or to generate content \n",
      "may pose specific risks of impersonation or deception irrespective of whether they \n",
      "qualif y as high -risk or not. In certain circumstances, the use of these systems should \n",
      "therefore be subject to specific transparency obligations without prejudice to the \n",
      "requirements and obligations for high -risk AI systems. In particular, natural persons \n",
      "should  be notified that they are interacting with an AI system, unless this is obvious \n",
      "from the circumstances and the context of use. Moreover, natural persons should be \n",
      "notified when they are exposed to an emotion recognition system or a biometric \n",
      "categorisatio n system. Such information and notifications should be provided in \n",
      "accessible formats for persons with disabilities. Further, users, who use an AI system \n",
      "to generate or manipulate image, audio or video content that appreciably resembles \n",
      "existing persons, p laces or events and would falsely appear to a person to be authentic, \n",
      "should disclose that the content has been artificially created or manipulated by \n",
      "labelling the artificial intelligence output accordingly and disclosing its artificial \n",
      "origin.  \n",
      "(71) Artificial intelligence is a rapidly developing family of technologies that requires \n",
      "novel forms of regulatory oversight and a safe space for experimentation, while \n",
      "ensuring responsible innovation and integration of appropriate safeguards and risk \n",
      "mitigati on measures. To ensure a legal framework that is innovation -friendly, future -\n",
      "proof and resilient to disruption, national competent authorities from one or more \n",
      "Member States should be encouraged to establish artificial intelligence regulatory \n",
      "sandboxes to facilitate the development and testing of innovative AI systems under \n",
      "strict regulatory oversight before these systems are placed on the market or otherwise \n",
      "put into service.  \n",
      "(72) The objectives of the regulatory sandboxes should be to foster AI innovatio n by \n",
      "establishing a controlled experimentation and testing environment in the development \n",
      "and pre -marketing phase with a view to ensuring compliance of the innovative AI \n",
      "systems  with this Regulation and other relevant Union and Member States legislation; \n",
      "to enhance legal certainty for innovators and the competent authorities’ oversight and \n",
      "understanding of the opportunities, emerging risks and the impacts of AI use, and to \n",
      "accelerate access to markets, including by removing barriers for small and medium \n",
      "enterprises (SMEs) and start -ups. To ensure uniform implementation across the Union \n",
      "and economies of scale, it is appropriate to establish common rules for the regulatory \n",
      "sandboxes’ implementation and a framework for cooperation between the relevant \n",
      "authorit ies involved in the supervision of the sandboxes. This Regulation should \n",
      "provide the legal basis for the use of personal data collected for other purposes for \n",
      "developing certain AI systems in the public interest within the AI regulatory sandbox, \n",
      "in line wi th Article 6(4) of Regulation (EU) 2016/679, and Article 6 of Regulation \n",
      "(EU) 2018/1725, and without prejudice to Article 4(2) of Directive (EU) 2016/680. \n",
      "Participants in the sandbox should ensure appropriate safeguards and cooperate with \n",
      "the competent aut horities, including by following their guidance and acting \n",
      "                                                 \n",
      "55 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the \n",
      "protection of natural persons with regard to the processing of personal data and on the free movement of \n",
      "such data, and repealing Directive 95/46 /EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, \n",
      "p. 1).  EN 35  EN expeditiously and in good faith to mitigate any high -risks to safety and fundamental \n",
      "rights that may arise during the development and experimentation in the sandbox. The \n",
      "conduct of the participants  in the sandbox should be taken into account when \n",
      "competent authorities decide whether to impose an administrative fine under Article \n",
      "83(2) of Regulation 2016/679 and Article 57 of Directive 2016/680.  \n",
      "(73) In order to promote and protect innovation, it is  important that the interests of small -\n",
      "scale providers and users of AI systems are taken into particular account. To this \n",
      "objective, Member States should develop initiatives, which are targeted at those \n",
      "operators, including on awareness raising and informa tion communication. Moreover, \n",
      "the specific interests and needs of small -scale providers shall be taken into account \n",
      "when Notified Bodies set conformity assessment fees.  Translation costs related to \n",
      "mandatory documentation and communication with authorities  may constitute a \n",
      "significant cost for providers and other operators, notably those of a smaller scale. \n",
      "Member States should possibly ensure that one of the languages determined and \n",
      "accepted by them for relevant providers’ documentation and for communicati on with \n",
      "operators is one which is broadly understood by the largest possible number of cross -\n",
      "border users.  \n",
      "(74) In order to minimise the risks to implementation resulting from lack of knowledge and \n",
      "expertise in the market as well as to facilitate complian ce of providers and notified \n",
      "bodies with their obligations under this Regulation, the AI -on demand platform, the \n",
      "European Digital Innovation Hubs and the Testing and Experimentation Facilities \n",
      "established by the Commission and the Member States at national  or EU level should \n",
      "possibly contribute to the implementation of this Regulation. Within their respective \n",
      "mission and fields of competence, they may provide in particular technical and \n",
      "scientific support to providers and notified bodies.  \n",
      "(75) It is approp riate that the Commission facilitates, to the extent possible, access to \n",
      "Testing and Experimentation Facilities to bodies, groups or laboratories established or \n",
      "accredited pursuant to any relevant Union harmonisation legislation and which fulfil \n",
      "tasks in t he context of conformity assessment of products or devices covered by that \n",
      "Union harmonisation legislation. This is notably the case for expert panels, expert \n",
      "laboratories and reference laboratories in the field of medical devices pursuant to \n",
      "Regulation (E U) 2017/745 and Regulation (EU) 2017/746.  \n",
      "(76) In order to facilitate a smooth, effective and harmonised implementation of this \n",
      "Regulation a European Artificial Intelligence Board should be established. The Board \n",
      "should be responsible for a number of advi sory tasks, including issuing opinions, \n",
      "recommendations, advice or guidance on matters related to the implementation of this \n",
      "Regulation, including on technical specifications or existing standards regarding the \n",
      "requirements established in this Regulation a nd providing advice to and assisting the \n",
      "Commission on specific questions related to artificial intelligence.  \n",
      "(77) Member States hold a key role in the application and enforcement of this Regulation. \n",
      "In this respect, each Member State should designate one  or more national competent \n",
      "authorities for the purpose of supervising the application and implementation of this \n",
      "Regulation. In order to increase organisation efficiency on the side of Member States \n",
      "and to set an official point of contact vis -à-vis the pu blic and other counterparts at \n",
      "Member State and Union levels, in each Member State one national authority should \n",
      "be designated as national supervisory authority.  \n",
      "(78) In order to ensure that providers of high -risk AI systems can take into account the \n",
      "exper ience on the use of high -risk AI systems for improving their systems and the EN 36  EN design and development process or can take any possible corrective action in a timely \n",
      "manner, all providers should have a post -market monitoring system in place. This \n",
      "system is al so key to ensure that the possible risks emerging from AI systems which \n",
      "continue to ‘learn’ after being placed on the market or put into service can be more \n",
      "efficiently and timely addressed. In this context, providers should also be required to \n",
      "have a syst em in place to report to the relevant authorities any serious incidents or any \n",
      "breaches to national and Union law protecting fundamental rights resulting from the \n",
      "use of their AI systems.   \n",
      "(79) In order to ensure an appropriate and effective enforcement of  the requirements and \n",
      "obligations set out by this Regulation, which is Union harmonisation legislation, the \n",
      "system of market surveillance and compliance of products established by Regulation \n",
      "(EU) 2019/1020 should apply in its entirety. Where necessary for their mandate, \n",
      "national public authorities or bodies, which supervise the application of Union law \n",
      "protecting fundamental rights, including equality bodies, should also have access to \n",
      "any documentation created under this Regulation.  \n",
      "(80) Union legislation on financial services includes internal governance and risk \n",
      "management rules and requirements which are applicable to regulated financial \n",
      "institutions in the course of provision of those services, including when they make use \n",
      "of AI systems. In order to ens ure coherent application and enforcement of the \n",
      "obligations under this Regulation and relevant rules and requirements of the Union \n",
      "financial services legislation, the authorities responsible for the supervision and \n",
      "enforcement of the financial services leg islation,  including where applicable the \n",
      "European Central Bank , should be designated as competent authorities for the purpose \n",
      "of supervising the implementation of this Regulation, including for market \n",
      "surveillance activities, as regards AI systems provided  or used by regulated and \n",
      "supervised financial institutions. To further enhance the consistency between this \n",
      "Regulation and the rules applicable to credit institutions regulated under Directive \n",
      "2013/36/EU of the European Parliament and of the Council56, it is also appropriate to \n",
      "integrate the conformity assessment procedure and some of the providers’ procedural \n",
      "obligations in relation to risk management, post marketing monitoring and \n",
      "documentation into the existing obligations and procedures under Directive \n",
      "2013/36/EU. In order to avoid overlaps, limited derogations should also be envisaged \n",
      "in relation to the quality management system of providers and the monitoring \n",
      "obligation placed on users of high -risk AI systems to the extent that these apply to \n",
      "credit in stitutions regulated by Directive 2013/36/EU.  \n",
      "(81) The development of AI systems other than high -risk AI systems in accordance with \n",
      "the requirements of this Regulation may lead to a larger uptake of trustworthy artificial \n",
      "intelligence in the Union. Provide rs of non -high-risk AI systems should be encouraged \n",
      "to create codes of conduct intended to foster the voluntary application of the \n",
      "mandatory requirements applicable to high -risk AI systems. Providers should also be \n",
      "encouraged to apply on a voluntary basis additional requirements related, for example, \n",
      "to environmental sustainability, accessibility to persons with disability, stakeholders’ \n",
      "participation in the design and development of AI systems, and diversity of the \n",
      "development teams. The Commission may dev elop initiatives, including of a sectorial \n",
      "                                                 \n",
      "56 Directive 2013/36/EU of the European Parliament and of the Council of 26 June 2013 on access to the \n",
      "activity of credit institutions and the prudential supervision of credit institutions  and investment firms, \n",
      "amending Directive 2002/87/EC and repealing Directives 2006/48/EC and 2006/49/EC (OJ L 176, \n",
      "27.6.2013, p. 338).  EN 37  EN nature, to facilitate the lowering of technical barriers hindering cross -border exchange \n",
      "of data for AI development, including on data access infrastructure, semantic and \n",
      "technical interoperability of different ty pes of data.  \n",
      "(82) It is important that AI systems related to products that are not high -risk in accordance \n",
      "with this Regulation and thus are not required to comply with the requirements set out \n",
      "herein are nevertheless safe when placed on the market or put into service. To \n",
      "contribute to this objective,  the Directive 2001/95/EC of the European Parliament and \n",
      "of the Council57  would apply as a safety net.  \n",
      "(83) In order to ensure trustful and constructive cooperation of competent authorities on \n",
      "Union and nationa l level, all parties involved in the application of this Regulation \n",
      "should respect the confidentiality of information and data obtained in carrying out \n",
      "their tasks.  \n",
      "(84) Member States should take all necessary measures to ensure that the provisions of thi s \n",
      "Regulation are implemented, including by laying down effective, proportionate and \n",
      "dissuasive penalties for their infringement. For certain specific infringements, Member \n",
      "States should take into account the margins and criteria set out in this Regulation.  The \n",
      "European Data Protection Supervisor should have the power to impose fines on Union \n",
      "institutions, agencies and bodies falling within the scope of this Regulation.  \n",
      "(85) In order to ensure that the regulatory framework can be adapted where necessary, the  \n",
      "power to adopt acts in accordance with Article 290 TFEU should be delegated to the \n",
      "Commission to amend the techniques and approaches referred to in Annex I to define \n",
      "AI systems, the Union harmonisation legislation listed in Annex II, the high -risk AI \n",
      "systems listed in Annex III, the provisions regarding technical documentation listed in \n",
      "Annex IV, the content of the EU declaration of conformity in Annex V, the provisions \n",
      "regarding the conformity assessment procedures in Annex VI and VII and the \n",
      "provisions e stablishing the high -risk AI systems to which the conformity assessment \n",
      "procedure based on assessment of the quality management system and assessment of \n",
      "the technical documentation should apply. It is of particular importance that the \n",
      "Commission carry out appropriate consultations during its preparatory work, including \n",
      "at expert level, and that those consultations be conducted in accordance with the \n",
      "principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better \n",
      "Law-Making58. In particula r, to ensure equal participation in the preparation of \n",
      "delegated acts, the European Parliament and the Council receive all documents at the \n",
      "same time as Member States’ experts, and their experts systematically have access to \n",
      "meetings of Commission expert g roups dealing with the preparation of delegated acts.  \n",
      "(86) In order to ensure uniform conditions for the implementation of this Regulation, \n",
      "implementing powers should be conferred on the Commission. Those powers should \n",
      "be exercised in accordance with Regul ation (EU) No 182/2011 of the European \n",
      "Parliament and of the Council59. \n",
      "(87) Since the objective of this Regulation cannot be sufficiently achieved by the Member \n",
      "States and can rather, by reason of the scale or effects of the action, be better achieved \n",
      "                                                 \n",
      "57  Directive 2001/95/EC of the European Parliament and of the Council of 3 December 2001 on general \n",
      "product safety (OJ L 11, 15.1.2002, p. 4).  \n",
      "58 OJ L 123, 12.5.2016, p. 1.  \n",
      "59 Regulation (EU) No 182/2011 of the European Parliament and of the Council of 16 February 2011 \n",
      "laying down the rules and general principles concerning mechanisms for control by the Member States \n",
      "of the Commission's exercise of implementing powers (OJ L 55, 28.2.2011, p.13).  EN 38  EN at Union level, the Union may adopt measures in accordance with the principle of \n",
      "subsidiarity as set out in Article 5 TEU. In accordance with the principle of \n",
      "proportionality as set out in that Article, this Regulation does not go beyond what is \n",
      "necessary in o rder to achieve that objective.  \n",
      "(88) This Regulation should apply from … [ OP – please insert the date established in Art. \n",
      "85]. However, the infrastructure related to the governance and the conformity \n",
      "assessment system should be operational before that date , therefore the provisions on \n",
      "notified bodies and governance structure should apply from … [OP – please insert the \n",
      "date – three months following the entry into force of this Regulation ]. In addition, \n",
      "Member States should lay down and notify to the Commissi on the rules on penalties, \n",
      "including administrative fines, and ensure that they are properly and effectively \n",
      "implemented by the date of application of this Regulation. Therefore the provisions on \n",
      "penalties should apply from [ OP – please insert the date – twelve months following the \n",
      "entry into force of this Regulation ]. \n",
      "(89) The European Data Protection Supervisor and the European Data Protection Board \n",
      "were consulted in accordance with Article 42(2) of Regulation (EU) 2018/1725 and \n",
      "delivered an opinion on [ …]”.  \n",
      "HAVE ADOPTED THIS REGULATION:  \n",
      "TITLE  I \n",
      "GENERAL  PROVISIONS  \n",
      "Article 1  \n",
      "Subject matter  \n",
      "This Regulation lays down:  \n",
      "(a) harmonised rules for the placing on the market, the putting into service and the \n",
      "use of artificial intelligence systems (‘AI systems’) in the Union;  \n",
      "(a) prohibitions of certain artificial intelligence practices;  \n",
      "(b) specific requirements for high -risk AI systems and obligations for operators of \n",
      "such systems;  \n",
      "(c) harmonised transparency rules for AI systems intended to interact with natural \n",
      "persons, emotion recognition systems and biometric categorisation systems, \n",
      "and AI systems used to generate or manipulate image, audio or video content;  \n",
      "(d) rules on market monitoring and surveillance.  \n",
      "Article 2  \n",
      "Scope  \n",
      "1. This Regulation applies to:  \n",
      "(a) providers placing on the market or putting into service AI systems in the \n",
      "Union, irrespective of whether those providers are established within the Union \n",
      "or in a third country;  \n",
      "(b) users of AI systems located within the Union;  EN 39  EN (c) providers and users of AI  systems that are located in a third country, where the \n",
      "output produced by the system is used in the Union;  \n",
      "2. For high -risk AI systems that are safety components of products or systems, or which \n",
      "are themselves products or systems, falling within the scope  of the following acts, \n",
      "only Article 84 of this Regulation shall apply:  \n",
      "(a) Regulation (EC) 300/2008;  \n",
      "(b) Regulation  (EU) No 167/2013;  \n",
      "(c) Regulation (EU) No 168/2013;  \n",
      "(d) Directive 2014/90/EU;  \n",
      "(e) Directive (EU) 2016/797;  \n",
      "(f) Regulation (EU) 2018/858;  \n",
      "(g) Regulation (EU) 201 8/1139;  \n",
      "(h) Regulation (EU) 2019/2144.  \n",
      "3. This Regulation shall not apply to AI systems developed or used exclusively for \n",
      "military purposes.  \n",
      "4. This Regulation shall not apply to public authorities in a third country nor to \n",
      "internati onal organisations falling within the scope of this Regulation pursuant to \n",
      "paragraph 1, where those authorities or organisations use AI systems in the \n",
      "framework of international agreements for law enforcement and judicial cooperation \n",
      "with the Union or with  one or more Member States.  \n",
      "5. This Regulation shall not affect the application of the provisions on the liability of \n",
      "intermediary service providers set out in Chapter II, Section IV of Directive \n",
      "2000/31/EC of the European Parliament and of the Council60 [as to be replaced by  \n",
      "the corresponding provisions of the Digital Services Act ]. \n",
      "Article 3  \n",
      "Definitions  \n",
      "For the purpose of this Regulation, the following definitions apply:  \n",
      "(1) ‘artificial intelligence system’ (AI system) means software that is developed with  one \n",
      "or more of the techniques and approaches listed in Annex I and can, for a given set of \n",
      "human -defined objectives, generate outputs such as content, predictions, \n",
      "recommendations, or decisions influencing the environments they interact with;  \n",
      "(1) ‘provider’ m eans a natural or legal person, public authority, agency or other body \n",
      "that develops an AI system or that has an AI system developed with a view to \n",
      "placing it on the market or putting it into service under its own name or trademark, \n",
      "whether for payment or free of charge;  \n",
      "                                                 \n",
      "60 Directive 2000/31/EC of the European Parliament and of the Council of 8 June 2000 on certain legal \n",
      "aspects of information society services, in particular electronic commerce, in the  Internal Market \n",
      "('Directive on electronic commerce') (OJ L 178, 17.7.2000, p. 1).  EN 40  EN (3) ‘small -scale provider’ means a provider that is a micro or small enterprise within the \n",
      "meaning of Commission Recommendation 2003/361/EC61; \n",
      "(4) ‘user’ means any natural or legal person, public authority, agency or other body \n",
      "using an AI s ystem under its authority, except where the AI system is used in the \n",
      "course of a personal non -professional activity;  \n",
      "(5) ‘authorised representative’ means any natural or legal person established in the \n",
      "Union who has received a written mandate from a provid er of an AI system to, \n",
      "respectively, perform and carry out on its behalf the obligations and procedures \n",
      "established by this Regulation;  \n",
      "(6) ‘importer’ means any natural or legal person established in the Union that places on \n",
      "the market or puts into service  an AI system that bears the name or trademark of a \n",
      "natural or legal person established outside the Union;  \n",
      "(7) ‘distributor’ means any natural or legal person in the supply chain, other than the \n",
      "provider or the importer, that makes an AI system available o n the Union market \n",
      "without affecting its properties;  \n",
      "(8) ‘operator’ means the provider, the user, the authorised representative, the importer \n",
      "and the distributor;  \n",
      "(9) ‘placing on the market’ means the first making available of an AI system on the \n",
      "Union market;  \n",
      "(10) ‘making available on the market’ means any supply of an AI system for distribution \n",
      "or use on the Union market in the course of a commercial activity, whether in return \n",
      "for payment or free of charge;  \n",
      "(11) ‘putting into service’ means the supply  of an AI system for first use directly to the \n",
      "user or for own use on the Union market for its intended purpose;  \n",
      "(12) ‘intended purpose’ means the use for which an AI system is intended by the provider, \n",
      "including the specific context and conditions of use,  as specified in the information \n",
      "supplied by the provider in the instructions for use, promotional or sales materials \n",
      "and statements, as well as in the technical documentation;  \n",
      "(13) ‘reasonably foreseeable misuse’ means the use of an AI system in a way tha t is not in \n",
      "accordance with its intended purpose, but which may result from reasonably \n",
      "foreseeable human behaviour or interaction with other systems;  \n",
      "(14) ‘safety component of a product or system’ means a component of a product or of a \n",
      "system which fulfils  a safety function for that product or system or the failure or \n",
      "malfunctioning of which endangers the health and safety of persons or property;  \n",
      "(15) ‘instructions for use’ means the information provided by the provider to inform the \n",
      "user of in particular a n AI system’s intended purpose and proper use, inclusive of the \n",
      "specific geographical, behavioural or functional setting within which the high -risk AI \n",
      "system is intended to be used;  \n",
      "(16) ‘recall of an AI system’ means any measure aimed at achieving the ret urn to the \n",
      "provider of an AI system made available to users;  \n",
      "                                                 \n",
      "61 Commission Recommendation of 6 May 2003 concerning the definition of micro, small and medium -\n",
      "sized enterprises (OJ L 124, 20.5.2003, p. 36).  EN 41  EN (17) ‘withdrawal of an AI system’ means any measure aimed at preventing the \n",
      "distribution, display and offer of an AI system;  \n",
      "(18) ‘performance of an AI system’ means the ability of an AI system t o achieve its \n",
      "intended purpose;  \n",
      "(19) ‘notifying authority’ means the national authority responsible for setting up and \n",
      "carrying out the necessary procedures for the assessment, designation and \n",
      "notification of conformity assessment bodies and for their monitoring;  \n",
      "(20) ‘conformity assessment’ means the process of verifying whether the requirements set \n",
      "out in Title III, Chapter 2 of this Regulation relating to an AI system have been \n",
      "fulfilled;  \n",
      "(21) ‘conformity assessment body’ means a body that performs t hird-party conformity \n",
      "assessment activities, including testing, certification and inspection;  \n",
      "(22) ‘notified body’ means a conformity assessment body designated in accordance with \n",
      "this Regulation and other relevant Union harmonisation legislation;  \n",
      "(23) ‘substantial modification’ means a change to the AI system following its placing on \n",
      "the market or putting into service which affects the compliance of the AI system with \n",
      "the requirements set out in Title III, Chapter 2 of this Regulation or results in a \n",
      "modif ication to the intended purpose for which the AI system has been assessed;  \n",
      "(24) ‘CE marking of conformity’ (CE marking) means a marking by which a provider \n",
      "indicates that an AI system is in conformity with the requirements set out in Title III, \n",
      "Chapter 2 o f this Regulation and other applicable Union legislation harmonising the \n",
      "conditions for the marketing of products (‘Union harmonisation legislation’) \n",
      "providing for its affixing;  \n",
      "(25) ‘post -market monitoring’ means all activities carried out by providers of  AI systems \n",
      "to proactively collect and review experience gained from the use of AI systems they \n",
      "place on the market or put into service for the purpose of identifying any need to \n",
      "immediately apply any necessary corrective or preventive actions;  \n",
      "(26) ‘marke t surveillance authority’ means the national authority carrying out the \n",
      "activities and taking the measures pursuant to Regulation (EU) 2019/1020;  \n",
      "(27) ‘harmonised standard’ means a European standard as defined in Article 2(1)(c) of \n",
      "Regulation (EU) No 1025/ 2012;  \n",
      "(28) ‘common specifications’ means a document, other than a standard, containing \n",
      "technical solutions providing a means to, comply with certain requirements and \n",
      "obligations established under this Regulation;  \n",
      "(29) ‘training data’ means data used for tr aining an AI system through fitting its learnable \n",
      "parameters, including the weights of a neural network;  \n",
      "(30) ‘validation data’ means data used for providing an evaluation of the trained AI \n",
      "system and for tuning its non -learnable parameters and its learnin g process, among \n",
      "other things, in order to prevent overfitting; whereas the validation dataset can be a \n",
      "separate dataset or part of the training dataset, either as a fixed or variable split;  \n",
      "(31) ‘testing data’ means data used for providing an independent evaluation of the trained \n",
      "and validated AI system in order to confirm the expected performance of that system \n",
      "before its placing on the market or putting into service;  EN 42  EN (32) ‘input data’ means data provided to or directly acquired by an AI system on the ba sis \n",
      "of which the system produces an output;  \n",
      "(33) ‘biometric data’ means personal data resulting from specific technical processing \n",
      "relating to the physical, physiological or behavioural characteristics of a natural \n",
      "person, which allow or confirm the unique  identification of that natural person, such \n",
      "as facial images or dactyloscopic data;  \n",
      "(34) ‘emotion recognition system’ means an AI system for the purpose of identifying or \n",
      "inferring emotions or intentions of natural persons on the basis of their biometric \n",
      "data;  \n",
      "(35) ‘biometric categorisation system’ means an AI system for the purpose of assigning \n",
      "natural persons to specific categories, such as sex, age, hair colour, eye colour, \n",
      "tattoos, ethnic origin or sexual or political orientation, on the basis of their  biometric \n",
      "data;  \n",
      "(36) ‘remote biometric identification system’ means  an AI system  for the purpose of \n",
      "identifying natural persons at a distance through the comparison of a person’s \n",
      "biometric data with the biometric data contained in a reference database, an d without \n",
      "prior knowledge of the user of the AI system whether the person will be present and \n",
      "can be identified ;  \n",
      "(37) ‘‘real -time’ remote biometric identification system’ means a remote biometric \n",
      "identification system whereby the capturing of biometric da ta, the comparison and \n",
      "the identification all occur without a significant delay. This comprises not only \n",
      "instant identification, but also limited short delays in order to avoid circumvention.  \n",
      "(38) ‘‘post’ remote biometric identification system’ means a remote biometric \n",
      "identification system other than a ‘real -time’ remote biometric identification system;  \n",
      "(39) ‘publicly accessible space’ means any physical place accessible to the public, \n",
      "regardless of whether certain conditions for access may apply;  \n",
      "(40) ‘law enforcement authority’ means:   \n",
      "(a) any public authority competent for the prevention, investigation, detection or \n",
      "prosecution of criminal offences or the execution of criminal penalties, \n",
      "including the safeguarding against and the prevention of threat s to public \n",
      "security; or  \n",
      "(b) any other body or entity entrusted by Member State law to exercise public \n",
      "authority and public powers for the purposes of the prevention, investigation, \n",
      "detection or prosecution of criminal offences or the execution of criminal  \n",
      "penalties, including the safeguarding against and the prevention of threats to \n",
      "public security;  \n",
      "(41) ‘law enforcement’ means activities carried out by law enforcement authorities for the \n",
      "prevention, investigation, detection or prosecution of criminal offences or the \n",
      "execution of criminal penalties, including the safeguarding against and the \n",
      "prevention of threats to public security;  \n",
      "(42) ‘national supervisory authority’ means the authority to which a Member State assigns \n",
      "the responsibility for the imple mentation and application of this Regulation, for \n",
      "coordinating the activities entrusted to that Member State, for acting as the single \n",
      "contact point for the Commission, and for representing the Member State at the \n",
      "European Artificial Intelligence Board;  EN 43  EN (43) ‘national competent authority’ means the national supervisory authority, the \n",
      "notifying authority and the market surveillance authority;  \n",
      "(44) ‘serious incident’ means any incident that directly or indirectly leads, might have led \n",
      "or might lead to any of  the following:  \n",
      "(a) the death of a person or serious damage to a person’s health,  to property or the \n",
      "environment,  \n",
      "(b) a serious and irreversible disruption of the management and operation of \n",
      "critical infrastructure.  \n",
      "Article 4  \n",
      "Amendments to Annex I  \n",
      "The Com mission is empowered to adopt delegated acts in accordance with Article 73 to \n",
      "amend the list of techniques and approaches listed in Annex I, in order to update that list to \n",
      "market and technological developments on the basis of characteristics that are simi lar to the \n",
      "techniques and approaches listed therein.  \n",
      "TITLE  II \n",
      "PROHIBITED  ARTIFICIAL  INTELLIGENCE  PRACTICES  \n",
      "Article 5  \n",
      "1. The following artificial intelligence practices shall be prohibited:  \n",
      "(a) the placing on the market, putting into service or use of an A I system that \n",
      "deploys subliminal techniques beyond a person’s consciousness in order to \n",
      "materially distort a person’s behaviour in a manner that causes or is likely to \n",
      "cause that person or another person physical or psychological harm;  \n",
      "(b) the placing on t he market, putting into service or use of an AI system that \n",
      "exploits any of the vulnerabilities of a specific group of persons due to their \n",
      "age, physical or mental disability, in order to materially distort the behaviour of \n",
      "a person pertaining to that grou p in a manner that causes or is likely to cause \n",
      "that person or another person physical or psychological harm;  \n",
      "(c) the placing on the market, putting into service or use of AI systems by public \n",
      "authorities or on their behalf for the evaluation or classifica tion of the \n",
      "trustworthiness of natural persons over a certain period of time based on their \n",
      "social behaviour or known or predicted personal or personality characteristics, \n",
      "with the social score leading to either or both of the following:  \n",
      "(i) detrimental or  unfavourable treatment of certain natural persons or whole \n",
      "groups thereof in social contexts which are unrelated to the contexts in \n",
      "which the data was originally generated or collected;  \n",
      "(ii) detrimental or unfavourable treatment of certain natural person s or whole \n",
      "groups thereof that is unjustified or disproportionate to their social \n",
      "behaviour or its gravity;  \n",
      "(d) the use of  ‘real-time’ remote biometric identification systems in publicly \n",
      "accessible spaces for the purpose of law enforcement, unless and in a s far as \n",
      "such use is strictly necessary for one of the following objectives : EN 44  EN (i) the targeted search for specific potential victims of crime, including \n",
      "missing children;  \n",
      "(ii) the prevention of a specific, substantial and imminent  threat to the life or \n",
      "physical safety of natural persons or of a terrorist attack;  \n",
      "(iii) the detection, localisation, identification or prosecution of a perpetrator \n",
      "or suspect of a criminal offence referred to  in Article 2(2) of Council \n",
      "Framework Decision 2002/584/JHA62 and punish able in the Member \n",
      "State concerned by a custodial sentence or a detention order for a \n",
      "maximum period of at least three years, as determined by the law of that \n",
      "Member State.  \n",
      "2. The use of ‘real -time’ remote biometric identification systems in publicly acces sible \n",
      "spaces for the purpose of law enforcement for any of the objectives referred to in \n",
      "paragraph 1 point d) shall take into account the following elements:  \n",
      "(a) the nature of the situation giving rise to the possible use, in particular the \n",
      "seriousness, pr obability and scale of the harm caused in the absence of the use \n",
      "of the system;  \n",
      "(b) the consequences of the use of the system for the rights and freedoms of all \n",
      "persons concerned, in particular the seriousness, probability and scale of those \n",
      "consequences.  \n",
      "In addition, the use of ‘real -time’ remote biometric identification systems in publicly \n",
      "accessible spaces for the purpose of law enforcement for any of the objectives \n",
      "referred to in paragraph 1 point d) shall comply with necessary and proportionate \n",
      "safegu ards and conditions in relation to the use, in particular as regards the temporal, \n",
      "geographic and personal limitations.  \n",
      "3. As regards paragraphs 1, point (d) and 2, each individual use for the purpose of law \n",
      "enforcement of a ‘real-time’ remote biometric identification system in publicly \n",
      "accessible spaces shall be subject to a prior authorisation granted by a judicial \n",
      "authority or by an independent administrative authority of the Member State in \n",
      "which the use is to take place,  issued upon a reasoned request and in accordance with \n",
      "the detailed rules of national law referred to in paragraph 4. However, in a duly \n",
      "justified situation of urgency, the use of the system may be commenced without an \n",
      "authorisation and the authorisation m ay be requested only during or after the use.   \n",
      "The competent judicial or administrative authority shall only grant the authorisation \n",
      "where it is satisfied, based on objective evidence or clear indications presented to it, \n",
      "that the use of the ‘real -time’ r emote biometric identification system at issue is \n",
      "necessary for and proportionate to achieving one of the objectives specified in \n",
      "paragraph 1, point (d), as identified in the request. In deciding on the request, the \n",
      "competent judicial or administrative aut hority shall  take into account the elements \n",
      "referred to in paragraph 2.  \n",
      "4. A Member State may decide to provide for the possibility to fully or partially \n",
      "authorise the use of ‘real -time’ remote biometric identification systems in publicly \n",
      "accessible space s for the purpose of law enforcement within the limits and under the \n",
      "                                                 \n",
      "62 Council Framework Decision 2002/584/JHA  of 13 June 2002 on the European arrest warrant and the \n",
      "surrender procedures between Member States ( OJ L 190, 18.7.2002, p. 1) . \n",
      " EN 45  EN conditions listed in paragraphs 1, point (d), 2 and 3. That Member State shall lay \n",
      "down in its national law the necessary detailed rules for the request, issuance and \n",
      "exercise of, as well  as supervision relating to, the authorisations referred to in \n",
      "paragraph 3. Those rules shall also specify in respect of which of the objectives listed \n",
      "in paragraph 1, point (d), including which of the criminal offences referred to in \n",
      "point (iii) thereof, the competent authorities may be authorised to use those systems \n",
      "for the purpose of law enforcement.  \n",
      "TITLE  III \n",
      "HIGH -RISK  AI SYSTEMS  \n",
      "CHAPTER 1 \n",
      "CLASSIFICATION  OF AI SYSTEMS  AS HIGH -RISK  \n",
      "Article 6  \n",
      "Classification rules for high -risk AI systems  \n",
      "1. Irrespective of whether an AI system is placed on the market or put into service \n",
      "independently from the products referred to in points (a) and (b), that AI system shall \n",
      "be considered high -risk where both of the following conditions are fulfilled:  \n",
      "(a) the AI system is i ntended to be used as a safety component of a product, or is \n",
      "itself a product, covered by the Union harmonisation legislation listed in Annex \n",
      "II;  \n",
      "(b) the product  whose safety component is the AI system, or the AI system itself as \n",
      "a product, is required to  undergo a third -party conformity assessment with a \n",
      "view to the placing on the market or putting into service of that product \n",
      "pursuant to the Union harmonisation legislation listed in Annex II.  \n",
      "2. In addition to the high -risk AI systems referred to in para graph 1, AI systems \n",
      "referred to in Annex III shall also be considered high -risk. \n",
      "Article 7  \n",
      "Amendments to Annex III  \n",
      "1. The Commission is empowered to adopt delegated acts in accordance with Article 73 \n",
      "to update the list in Annex III by adding high -risk AI s ystems where both of the \n",
      "following conditions are fulfilled:  \n",
      "(a) the AI systems are intended to be used in any of the areas listed in points 1 to 8 \n",
      "of Annex III;  \n",
      "(b) the AI systems pose a risk of harm to the health and safety, or a risk of adverse \n",
      "impact o n fundamental rights, that is, in respect of its severity and probability \n",
      "of occurrence, equivalent to or greater than the risk of harm or of adverse \n",
      "impact posed by the high -risk AI systems already referred to in Annex III.  \n",
      "2. When assessing for the purpo ses of paragraph 1 whether an AI system poses a risk of \n",
      "harm to the health and safety or a risk of adverse impact on fundamental rights that is \n",
      "equivalent to or greater than the risk of harm posed by the high -risk AI systems EN 46  EN already referred to in Annex II I, the Commission shall take into account the \n",
      "following criteria:  \n",
      "(a) the intended purpose of the AI system;  \n",
      "(b) the extent to which an AI system has been used or is likely to be used;  \n",
      "(c) the extent to which the use of an AI system has already caused harm  to the \n",
      "health and safety or adverse impact on the fundamental rights or has given rise \n",
      "to significant concerns in relation to the materialisation of such harm or \n",
      "adverse impact, as demonstrated by reports or documented allegations \n",
      "submitted to national co mpetent authorities;  \n",
      "(d) the potential extent of such harm or such adverse impact, in particular in terms \n",
      "of its intensity and its ability to affect a plurality of persons;  \n",
      "(e) the extent to which potentially harmed or adversely impacted persons are \n",
      "depend ent on the outcome produced with an AI system, in particular because \n",
      "for practical or legal reasons it is not reasonably possible to opt -out from that \n",
      "outcome;  \n",
      "(f) the extent to which potentially harmed or adversely impacted persons are in a \n",
      "vulnerable pos ition in relation to the user of an AI system, in particular due to \n",
      "an imbalance of power, knowledge, economic or social circumstances, or age;  \n",
      "(g) the extent to which the outcome produced with an AI system is easily \n",
      "reversible, whereby outcomes having an impact on the health or safety of \n",
      "persons shall not be considered as easily reversible;  \n",
      "(h) the extent to which existing Union legislation provides for:  \n",
      "(i) effective measures of redress in relation to the risks posed by an AI \n",
      "system, with the exclusion of  claims for damages;  \n",
      "(ii) effective measures to prevent or substantially minimise those risks.  \n",
      "CHAPTER 2 \n",
      "REQUIREMENTS FOR HIG H-RISK AI SYSTEMS  \n",
      "Article 8  \n",
      "Compliance with the requirements  \n",
      "1. High -risk AI systems shall comply with the requirements established  in this Chapter.  \n",
      "2. The intended purpose of the high -risk AI system and the risk management system \n",
      "referred to in Article 9 shall be taken into account when ensuring compliance with \n",
      "those requirements.  \n",
      "Article 9  \n",
      "Risk management system  \n",
      "1. A risk management  system shall be established, implemented, documented and \n",
      "maintained in relation to high -risk AI systems.  \n",
      "2. The risk management system shall consist of a continuous iterative process run \n",
      "throughout the entire lifecycle of a high -risk AI system, requiring regular systematic \n",
      "updating. It shall comprise the following steps:  EN 47  EN (a) identification and analysis of the known and foreseeable risks associated with \n",
      "each high -risk AI system;  \n",
      "(b) estimation and evaluation of the risks that may emerge when the high -risk A I \n",
      "system is used in accordance with its intended purpose and under conditions of \n",
      "reasonably foreseeable misuse;  \n",
      "(c) evaluation of other possibly arising risks based on the analysis of data gathered \n",
      "from the post -market monitoring system referred to in Art icle 61;  \n",
      "(d) adoption of suitable risk management measures in accordance with the \n",
      "provisions of the following paragraphs.  \n",
      "3. The risk management measures referred to in paragraph 2, point (d) shall give due \n",
      "consideration to the effects and possible interac tions resulting from the combined \n",
      "application of the requirements set out in this Chapter 2. They shall take into account \n",
      "the generally acknowledged state of the art, including as reflected in relevant \n",
      "harmonised standards or common specifications.  \n",
      "4. The risk management measures referred to in paragraph 2, point (d) shall be such that \n",
      "any residual risk associated with each hazard as well as the overall residual risk of \n",
      "the high -risk AI systems is judged acceptable, provided that the high -risk AI system \n",
      "is used in accordance with its intended purpose or under conditions of reasonably \n",
      "foreseeable misuse. Those residual risks shall be communicated to the user.  \n",
      "In identifying the most appropriate risk management measures, the following shall be \n",
      "ensured:  \n",
      "(a) elimination or reduction of risks as far as possible through adequate design and \n",
      "development;  \n",
      "(b) where appropriate, implementation of adequate mitigation and control \n",
      "measures in relation to risks that cannot be eliminated;  \n",
      "(c) provision of adequate informati on pursuant to Article 13, in particular as \n",
      "regards the risks referred to in paragraph 2, point (b) of this Article, and, where \n",
      "appropriate, training to users.  \n",
      "In eliminating or reducing risks related to the use of the high -risk AI system, due \n",
      "consideratio n shall be given to the technical knowledge, experience, education, \n",
      "training to be expected by the user and the environment in which the system is \n",
      "intended to be used.  \n",
      "5. High -risk AI systems shall be tested for the purposes of identifying the most \n",
      "appropr iate risk management measures. Testing shall ensure that high -risk AI \n",
      "systems perform consistently for their intended purpose and they are in compliance \n",
      "with the requirements set out in this Chapter.  \n",
      "6. Testing procedures shall be suitable to achieve the i ntended purpose of the AI system \n",
      "and do not need to go beyond what is necessary to achieve that purpose.  \n",
      "7. The testing of the high -risk AI systems shall be performed, as appropriate, at any \n",
      "point in time throughout the development process, and, in any eve nt, prior to the \n",
      "placing on the market or the putting into service. Testing shall be made against \n",
      "preliminarily defined metrics and probabilistic thresholds that are appropriate to the \n",
      "intended purpose of the high -risk AI system.  EN 48  EN 8. When implementing the r isk management system described in paragraphs 1 to 7, \n",
      "specific consideration shall be given to whether the high -risk AI system is likely to \n",
      "be accessed by or have an impact on children.  \n",
      "9. For credit institutions regulated by Directive 2013/36/EU, the aspe cts described in \n",
      "paragraphs 1 to 8 shall be part of the risk management procedures established by \n",
      "those institutions pursuant to Article 74 of that Directive.  \n",
      "Article 10  \n",
      "Data and data governance  \n",
      "1. High -risk AI systems which make use of techniques involvin g the training of models \n",
      "with data shall be developed on the basis of training, validation and testing data sets \n",
      "that meet the quality criteria referred to in paragraphs 2 to 5.  \n",
      "2. Training, validation and testing data sets shall be subject to appropriate data \n",
      "governance and management practices. Those practices shall concern in particular,  \n",
      "(a) the relevant design choices;  \n",
      "(b) data collection;  \n",
      "(c) relevant data preparation processing operations, such as annotation, labelling, \n",
      "cleaning, enrichment and aggregation ; \n",
      "(d) the formulation of relevant assumptions, notably with respect to the \n",
      "information that the data are supposed to measure and represent;  \n",
      "(e) a prior assessment of  the availability, quantity and suitability of the data sets \n",
      "that are needed;  \n",
      "(f) examination in view of possible biases;  \n",
      "(g) the identification of any possible data gaps or shortcomings, and how those \n",
      "gaps and shortcomings can be addressed.  \n",
      "3. Training, validation and testing data sets shall be relevant, representative, free of \n",
      "errors and complete. They shall have the appropriate statistical properties, including, \n",
      "where applicable, as regards the persons or groups of persons on which the high -risk \n",
      "AI syst em is intended to be used. These characteristics of the data sets may be met at \n",
      "the level of individual data sets or a combination thereof.  \n",
      "4. Training, validation and testing data sets shall take into account, to the extent \n",
      "required by the intended purpos e, the characteristics or elements that are particular to \n",
      "the specific geographical, behavioural or functional setting within which the high -\n",
      "risk AI system is intended to be used.  \n",
      "5. To the extent that it is strictly necessary for the purposes of ensuring  bias monitoring, \n",
      "detection and correction in relation to the high -risk AI systems, the providers of such \n",
      "systems may process special categories of personal data  referred to in Article 9(1) of \n",
      "Regulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680  and Article 10(1) \n",
      "of Regulation (EU) 2018/1725, subject to appropriate safeguards for the fundamental \n",
      "rights and freedoms of natural persons, including technical limitations on the re -use \n",
      "and use of state -of-the-art security and privacy -preserving measure s, such as \n",
      "pseudonymisation, or encryption where anonymisation may significantly affect the \n",
      "purpose pursued.  EN 49  EN 6. Appropriate data governance and management practices shall apply for the \n",
      "development of high -risk AI systems other than those which make use of techniques \n",
      "involving the training of models in order to ensure that those high -risk AI systems \n",
      "comply with paragraph 2.  \n",
      "Article 11  \n",
      "Technical documentation  \n",
      "1. The technical documentation of a high -risk AI system shall be drawn up before that \n",
      "system is pla ced on the market or put into service and shall be kept up -to date.  \n",
      "The technical documentation shall be drawn up in such a way to demonstrate that the \n",
      "high-risk AI system complies with the requirements set out in this Chapter and \n",
      "provide national competen t authorities and notified bodies with all the necessary \n",
      "information to assess the compliance of the AI system with those requirements. It \n",
      "shall contain, at a minimum, the elements set out in Annex IV.  \n",
      "2. Where a high -risk AI system related to a product, t o which the legal acts listed in \n",
      "Annex II, section A apply, is placed on the market or put into service one single \n",
      "technical documentation shall be drawn up containing all the information set out in \n",
      "Annex IV as well as the information required under those legal acts.  \n",
      "3. The Commission is empowered to adopt delegated acts in accordance with Article 73 \n",
      "to amend Annex IV where necessary to ensure that, in the light of technical progress, \n",
      "the technical documentation provides all the necessary information to ass ess the \n",
      "compliance of the system with the requirements set out in this Chapter.  \n",
      "Article 12  \n",
      "Record -keeping  \n",
      "1. High -risk AI systems shall be designed and developed with capabilities enabling the \n",
      "automatic recording of events (‘logs’) while the high -risk AI s ystems is operating. \n",
      "Those logging capabilities shall conform to recognised standards or common \n",
      "specifications.  \n",
      "2. The logging capabilities shall ensure a level of traceability of the AI system’s \n",
      "functioning throughout its lifecycle that is appropriate to the intended purpose of the \n",
      "system.  \n",
      "3. In particular, logging capabilities shall enable the monitoring of the operation of the \n",
      "high-risk AI system with respect to the occurrence of situations that may result in the \n",
      "AI system presenting a risk within the m eaning of Article 65(1) or lead to a \n",
      "substantial modification, and facilitate the post -market monitoring referred to in \n",
      "Article 61.  \n",
      "4. For high -risk AI systems referred to in paragraph 1, point (a) of Annex III, the \n",
      "logging capabilities shall provide, at a minimum:  \n",
      "(a) recording of the period of each use of the system (start date and time and end \n",
      "date and time of each use);  \n",
      "(b) the reference database against which input data has been checked by the \n",
      "system;  \n",
      "(c) the input data for which the search has led to a match;  EN 50  EN (d) the identification of the natural persons involved in the verification of the \n",
      "results, as referred to in Article 14 (5).  \n",
      "Article 13  \n",
      "Transparency and provision of information to users  \n",
      "1. High -risk AI systems shall be designed and developed in such a way to ensure that \n",
      "their operation is sufficiently transparent to enable users to interpret the system’s \n",
      "output and use it appropriately. An appropriate type and degree of transparency shall \n",
      "be ensured, with a view to achieving compliance with th e relevant obligations of the \n",
      "user and of the provider set out  in Chapter 3 of this Title . \n",
      "2. High -risk AI systems shall be accompanied by instructions for use in an appropriate \n",
      "digital format or otherwise that include concise, complete, correct and clear \n",
      "information that is relevant, accessible and comprehensible to users.  \n",
      "3. The information referred to in paragraph 2 shall specify:  \n",
      "(a) the identity and the contact details of the provider and, where applicable, of its \n",
      "authorised representative;  \n",
      "(b) the cha racteristics, capabilities and limitations of performance of the high -risk \n",
      "AI system, including:  \n",
      "(i) its intended purpose;  \n",
      "(ii) the level of accuracy, robustness and cybersecurity referred to in Article \n",
      "15 against which the high -risk AI system has been tested and validated \n",
      "and which can be expected, and any known and foreseeable \n",
      "circumstances that may have an impact on that expected level of \n",
      "accuracy, robustness and cybersecurity;  \n",
      "(iii) any known or foreseeable circumstance, related to the use of the hig h-\n",
      "risk AI system in accordance with its intended purpose or under \n",
      "conditions of reasonably foreseeable misuse, which may lead to risks to \n",
      "the health and safety or fundamental rights;  \n",
      "(iv) its performance as regards the persons or groups of persons on which  the \n",
      "system is intended to be used;  \n",
      "(v) when appropriate, specifications for the input data,  or any other relevant \n",
      "information in terms of the training, validation and testing data sets used, \n",
      "taking into account the intended purpose of the AI system . \n",
      "(c) the changes to the high -risk AI system and its performance which have been \n",
      "pre-determined by the provider at the moment of the initial conformity \n",
      "assessment, if any;   \n",
      "(d) the human oversight measures referred to in Article 14, including the technical \n",
      "measur es put in place to facilitate the interpretation of the outputs of AI \n",
      "systems by the users;  \n",
      "(e) the expected lifetime of the high -risk AI system and any necessary \n",
      "maintenance and care measures to ensure the proper functioning of that AI \n",
      "system, including a s regards software updates.  EN 51  EN Article 14  \n",
      "Human oversight  \n",
      "1. High -risk AI systems shall be designed and developed in such a way, including with \n",
      "appropriate human -machine interface tools, that they can be effectively overseen by \n",
      "natural persons during the peri od in which the AI system is in use.  \n",
      "2. Human oversight shall aim at preventing or minimising the risks to health, safety or \n",
      "fundamental rights that may emerge when a high -risk AI system is used in \n",
      "accordance with its intended purpose or under conditions of reasonably foreseeable \n",
      "misuse, in particular when such risks persist notwithstanding the application of other \n",
      "requirements set out in this Chapter . \n",
      "3. Human oversight shall be ensured through either one or all of the following \n",
      "measures:  \n",
      "(a) identified a nd built, when technically feasible, into the high -risk AI system by \n",
      "the provider before it is placed on the market or put into service;  \n",
      "(b) identified by the provider  before placing the high -risk AI system on the market \n",
      "or putting it into service and tha t are appropriate to be implemented by the \n",
      "user.  \n",
      "4. The measures referred to in paragraph 3 shall enable the individuals  to whom human \n",
      "oversight is assigned to do the following, as appropriate to the circumstances:  \n",
      "(a) fully understand the capacities and l imitations of the high -risk AI system and \n",
      "be able to duly monitor its operation, so that signs of anomalies, dysfunctions \n",
      "and unexpected performance can be detected and addressed as soon as \n",
      "possible;  \n",
      "(b) remain aware of the possible tendency of automatical ly relying or over -relying \n",
      "on the output produced by a high -risk AI system (‘automation bias’), in \n",
      "particular for high -risk AI systems used to provide information or \n",
      "recommendations for decisions to be taken by natural persons;  \n",
      "(c) be able to correctly int erpret the high -risk AI system’s output, taking into \n",
      "account in particular the characteristics of the system and the interpretation \n",
      "tools and methods available;  \n",
      "(d) be able to decide, in any particular situation, not to use the high -risk AI system \n",
      "or other wise disregard, override or reverse the output of the high -risk AI \n",
      "system;  \n",
      "(e) be able to intervene on the operation of the high -risk AI system or interrupt the \n",
      "system through a “stop” button or a similar procedure.  \n",
      "5. For high -risk AI systems referred to in point 1(a) of Annex III, the measures referred \n",
      "to in paragraph 3 shall be such as to ensure that, in addition, no action or decision is \n",
      "taken by the user on the basis of the identification resulting from the system unless \n",
      "this has been verified and conf irmed by at least two natural persons.  \n",
      "Article 15  \n",
      "Accuracy, robustness and cybersecurity  \n",
      "1. High -risk AI systems shall be designed and developed in such a way that they \n",
      "achieve, in the light of their intended purpose, an appropriate level of accuracy, EN 52  EN robustness and cybersecurity, and perform consistently in those respects throughout \n",
      "their lifecycle.  \n",
      "2. The levels of accuracy and the relevant accuracy metrics of high -risk AI systems \n",
      "shall be declared in the accompanying instructions of use.  \n",
      "3. High -risk AI systems shall be resilient as regards errors, faults or inconsistencies that \n",
      "may occur within the system or the environment in which the system operates, in \n",
      "particular due to their interaction with natural persons or other systems.  \n",
      "The robustness of high -risk AI systems may be achieved through technical \n",
      "redundancy solutions, which may include backup or fail -safe plans.  \n",
      "High -risk AI systems that continue to learn after being placed on the market or put \n",
      "into service shall be developed in such a way to ensure that possibly biased outputs \n",
      "due to outputs used as an input for future operations (‘feedback loops’) are duly \n",
      "addressed with appropriate mitigation measures.  \n",
      "4. High -risk AI systems shall be resilient as regards attempts by unauthorised third \n",
      "parties to a lter their use or performance by exploiting the system vulnerabilities.  \n",
      "The technical solutions aimed at ensuring the cybersecurity of high -risk AI systems \n",
      "shall be appropriate to the relevant circumstances and the risks.  \n",
      "The technical solutions to address AI specific vulnerabilities shall include, where \n",
      "appropriate, measures to prevent and control for attacks trying to manipulate the \n",
      "training dataset  (‘data poisoning’), inputs designed to cause the model to make a \n",
      "mistake  (‘adversarial examples’), or model flaws.  \n",
      "CHAPTER 3 \n",
      "OBLIGATIONS OF PROVI DERS AND USERS OF HI GH-RISK AI SYSTEMS AND \n",
      "OTHER PARTIES  \n",
      "Article 16  \n",
      "Obligations of providers of high -risk AI systems  \n",
      "Providers of high -risk AI systems shall:  \n",
      "(a) ensure that their hig h-risk AI systems are compliant with the requirements set out in \n",
      "Chapter 2 of this Title;  \n",
      "(b) have a quality management system in place which complies with Article 17;  \n",
      "(c) draw -up the technical documentation of the high -risk AI system;  \n",
      "(d) when under their  control, keep the logs automatically generated by their high -risk AI \n",
      "systems;  \n",
      "(e) ensure that the high -risk AI system undergoes the relevant conformity assessment \n",
      "procedure, prior to its placing on the market or putting into service;  \n",
      "(f) comply with the r egistration obligations referred to in Article 51;  \n",
      "(g) take the necessary corrective actions , if the high -risk AI system is not in conformity \n",
      "with the requirements set out in Chapter 2 of this Title ; EN 53  EN (h) inform the national competent authorities of the Mem ber States in which they made \n",
      "the AI system available or put it into service and, where applicable, the notified body \n",
      "of the non -compliance and of any corrective actions taken;  \n",
      "(i) to affix the CE marking to their high -risk AI systems to indicate the confo rmity with \n",
      "this Regulation in accordance with Article 49;  \n",
      "(j) upon request of a national competent authority, demonstrate the conformity of the \n",
      "high-risk AI system with the requirements set out in Chapter 2 of this Title . \n",
      "Article 17  \n",
      "Quality management syst em  \n",
      "1. Providers of high -risk AI systems shall put a quality management system in place \n",
      "that ensures compliance with this Regulation. That system shall be documented in a \n",
      "systematic and orderly manner in the form of written policies, procedures and \n",
      "instruc tions, and shall include at least the following aspects:  \n",
      "(a) a strategy for regulatory compliance, including compliance with conformity \n",
      "assessment procedures and procedures for the management of modifications to \n",
      "the high -risk AI system;  \n",
      "(b) techniques, pro cedures and systematic actions to be used for the design, design \n",
      "control and design verification of the high -risk AI system;  \n",
      "(c) techniques, procedures and systematic actions to be used for the development, \n",
      "quality control and quality assurance of the high -risk AI system;  \n",
      "(d) examination, test and validation procedures to be carried out before, during and \n",
      "after the development of the high -risk AI system, and the frequency with which \n",
      "they have to be carried out;  \n",
      "(e) technical specifications, including standa rds, to be applied and, where the \n",
      "relevant harmonised standards are not applied in full, the means to be used to \n",
      "ensure that the high -risk AI system complies with the requirements set out in \n",
      "Chapter 2 of this Title;  \n",
      "(f) systems and procedures for data management, including data collection, data \n",
      "analysis, data labelling, data storage, data filtration, data mining, data \n",
      "aggregation, data retention and any other operation regarding the data that is \n",
      "performed before and for the purposes of the placing on th e market or putting \n",
      "into service of high -risk AI systems;  \n",
      "(g) the risk management system referred to in Article 9;  \n",
      "(h) the setting -up, implementation and maintenance of a post -market monitoring \n",
      "system, in accordance with Article 61;  \n",
      "(i) procedures related to the reporting of serious incidents and of malfunctioning \n",
      "in accordance with Article 62;  \n",
      "(j) the handling of communication with national competent authorities, competent \n",
      "authorities, including sectoral ones, providing or supporting the access to data, \n",
      "notified bodies, other operators, customers or other interested parties;  \n",
      "(k) systems and procedures for record keeping of all relevant documentation and \n",
      "information;  \n",
      "(l) resource management, including security of supply related measures;  EN 54  EN (m) an accountabil ity framework setting out the responsibilities of the management \n",
      "and other staff with regard to all aspects listed in this paragraph.  \n",
      "2. The implementation of aspects referred to in paragraph 1 shall be proportionate to the \n",
      "size of the provider’s organisat ion.  \n",
      "3. For providers that are credit institutions regulated by Directive 2013/36/ EU, the \n",
      "obligation to put a quality management system in place shall be deemed to be \n",
      "fulfilled by complying with the rules on internal governance arrangements, processes \n",
      "and mechanisms pursuant to Article 74 of that Directive. In that context, any \n",
      "harmonised standards referred to in Article 40 of this Regulation shall be taken into \n",
      "account.  \n",
      "Article 18  \n",
      "Obligation to draw up technical documentation  \n",
      "1. Providers of high -risk AI  systems shall draw up the technical documen tation referred \n",
      "to in Article 11 in accordance with Annex IV.  \n",
      "2. Providers that are credit institutions regulated by Directive 2013/36/EU shall \n",
      "maintain the technical documentation as part of the documentation c oncerning \n",
      "internal governance, arrangements, processes and mechanisms pursuant to Article 74 \n",
      "of that Directive.  \n",
      "Article 19  \n",
      "Conformity assessment  \n",
      "1. Providers of high -risk AI systems shall ensure that their systems undergo the relevant \n",
      "conformity assessmen t procedure in accordance with Article 43, prior to their placing \n",
      "on the market or putting into service. Where the compliance of the AI systems with \n",
      "the requirements set out in Chapter 2 of this Title has been demonstrated following \n",
      "that conformity assessm ent, the providers shall draw up an EU declaration of \n",
      "conformity in accordance with Article 48 and affix the CE marking of conformity in \n",
      "accordance with Article 49.  \n",
      "2. For high -risk AI systems referred to in point 5(b) of Annex III that are placed on the \n",
      "market or put into service by providers that are credit institutions regulated by \n",
      "Directive 2013/36/EU, the conformity assessment  shall be carried out as part of the \n",
      "procedure referred to in Articles 97 to101 of that Directive.  \n",
      "Article 20  \n",
      "Automatically gen erated logs  \n",
      "1. Providers of high -risk AI systems shall keep the logs automatically generated by \n",
      "their high -risk AI systems, to the extent such logs are under their control by virtue of \n",
      "a contractual arrangement with the user or otherwise by law. The logs s hall be kept \n",
      "for a period that is appropriate in the light of the intended purpose of high -risk AI \n",
      "system and applicable legal obligations under Union or national law.  \n",
      "2. Providers that are credit institutions regulated by Directive 2013/36/EU shall \n",
      "mainta in the logs automatically generated by their high -risk AI systems as part of the \n",
      "documentation under Articles 74 of that Directive.  EN 55  EN Article 21  \n",
      "Corrective actions  \n",
      "Providers of high -risk AI systems which consider o r have reason to consider that a high -risk \n",
      "AI system which they have placed on the market or put into service is not in conformity with \n",
      "this Regulation shall immediately take the necessary corrective actions to bring that system \n",
      "into conformity, to withdraw it or to recall it, as appropriate. They s hall inform the \n",
      "distributors of the high -risk AI system in question and, where applicable, the authorised \n",
      "representative and importers accordingly.  \n",
      "Article 22  \n",
      "Duty of information  \n",
      "Where the high-risk AI system presents a risk within the meaning of Article 6 5(1) and that \n",
      "risk is known to the provider of the system, that provider shall immediately inform the \n",
      "national competent authorities of the Member States in which it made the system available \n",
      "and, where applicable, the notified body that issued a certifica te for the  high-risk AI system, \n",
      "in particular of the non -compliance and of any corrective actions taken.   \n",
      "Article 23  \n",
      "Cooperation with competent authorities  \n",
      "Providers of high -risk AI systems shall, upon request by a national competent authority, \n",
      "provide tha t authority with all the information and documentation necessary to demonstrate \n",
      "the conformity of the high -risk AI system with the requirements set out in Chapter 2 of this \n",
      "Title , in an official Union language determined by the Member State concerned. Upon  a \n",
      "reasoned request from a national competent authority, providers shall also give that authority \n",
      "access to the logs automatically generated by the high -risk AI system, to the extent such logs \n",
      "are under their control by virtue of a contractual arrangement with the user or otherwise by \n",
      "law. \n",
      "Article 24  \n",
      "Obligations of product manufacturers  \n",
      "Where a high -risk AI system related to products to which the legal acts listed in Annex II, \n",
      "section A, apply, is placed on the market or put into service together with the product \n",
      "manufactured in accordance with those legal acts and under the name of the product \n",
      "manufacturer, the manufacturer of the product shall take the responsibility of the compliance \n",
      "of the AI system with this Regulation and, as far as the AI system is c oncerned, have the same \n",
      "obligations imposed by the present Regulation on the provider.  \n",
      "Article 25  \n",
      "Authorised representatives  \n",
      "1. Prior to making their systems available on the Union market, where an importer \n",
      "cannot be identified, providers established outside the Union shall, by written \n",
      "mandate, appoint an authorised representative which is established in the Union.  \n",
      "2. The authorised representative shall perform the tasks specified in the mandate \n",
      "received from the provider. The mandate shall empower the  authorised representative \n",
      "to carry out the following tasks:  EN 56  EN (a) keep a copy of the EU declaration of conformity and the technical \n",
      "documentation at the disposal of the national competent authorities and \n",
      "national authorities referred to in Article 63(7);  \n",
      "(b) provide a national competent authority, upon a reasoned request, with all the \n",
      "information and documentation necessary to demonstrate the conformity of a \n",
      "high-risk AI system with the requirements set out in Chapter 2 of this Title, \n",
      "including access to the  logs automatically generated by the high -risk AI system  \n",
      "to the extent such logs are under the control of the provider by virtue of a \n",
      "contractual arrangement with the user or otherwise by law ; \n",
      "(c) cooperate with competent national authorities, upon a reaso ned request, on any \n",
      "action the latter takes in relation to the high -risk AI system.  \n",
      "Article 26  \n",
      "Obligations of importers  \n",
      "1. Before placing a high -risk AI system on the market, importers of such system shall \n",
      "ensure that:  \n",
      "(a) the appropriate conformity assess ment procedure has been carried out by the \n",
      "provider of that AI system  \n",
      "(b) the provider has drawn up the technical documentation in accordance with \n",
      "Annex IV;  \n",
      "(c) the system bears the required conformity marking and is accompanied by the \n",
      "required documenta tion and instructions of use.  \n",
      "2. Where an importer considers or has reason to consider that a high -risk AI system is \n",
      "not in conformity with this Regulation, it shall not place that system on the market \n",
      "until that AI system has been brought into conformity . Where the high -risk AI \n",
      "system presents a risk within the meaning of Article 65(1), the importer shall inform \n",
      "the provider of the AI system and the market surveillance authorities to that effect.  \n",
      "3. Importers shall indicate their name, registered trade na me or registered trade mark, \n",
      "and the address at which they can be contacted on the high -risk AI system or, where \n",
      "that is not possible, on its packaging or its accompanying documentation, as \n",
      "applicable.  \n",
      "4. Importers shall ensure that, while a high -risk AI s ystem is under their responsibility, \n",
      "where applicable, storage or transport conditions do not jeopardise its compliance \n",
      "with the requirements set out in Chapter 2 of this Title . \n",
      "5. Importers shall provide national competent authorities, upon a reasoned req uest, with \n",
      "all necessary information and documentation to demonstrate the conformity of a \n",
      "high-risk AI system with the requirements set out in Chapter 2 of this Title in a \n",
      "language which can be easily understood by that national competent authority, \n",
      "includ ing access to the logs automatically generated by the high -risk AI system to the \n",
      "extent such logs are under the control of the provider by virtue of a contractual \n",
      "arrangement with the user or otherwise by law. They shall also cooperate with those \n",
      "authoriti es on any action national competent authority takes in relation to that \n",
      "system.  EN 57  EN Article 27  \n",
      "Obligations of distributors  \n",
      "1. Before making a high -risk AI system available on the market, distributors shall \n",
      "verify that the high -risk AI system bears the required  CE conformity marking, that it \n",
      "is accompanied by the required documentation and instruction of use, and that the \n",
      "provider and the importer of the system, as applicable, have complied with the \n",
      "obligations set out in this Regulation.  \n",
      "2. Where a distributor considers or has reason to consider that a high -risk AI system is \n",
      "not in conformity with the requirements set out in Chapter 2 of this Title, it shall not \n",
      "make the high -risk AI system available on the market until that system has been \n",
      "brought into conformi ty with those requirements. Furthermore, where the system \n",
      "presents a risk within the meaning of Article 65(1), the distributor shall inform the \n",
      "provider or the importer of the system, as applicable, to that effect.  \n",
      "3. Distributors shall ensure that, while a high -risk AI system is under their \n",
      "responsibility, where applicable, storage or transport conditions do not jeopardise the \n",
      "compliance of the system with the requirements set out in Chapter 2 of this Title . \n",
      "4. A distributor that considers or has reason to  consider that a high -risk AI system \n",
      "which it has made available on the market is not in conformity with the requirements \n",
      "set out in Chapter 2 of this Title shall take the corrective actions necessary to bring \n",
      "that system into conformity with those require ments, to withdraw it or recall it or \n",
      "shall ensure that the provider, the importer or any relevant operator, as appropriate, \n",
      "takes those corrective actions. Where the high -risk AI system presents a risk within \n",
      "the meaning of Article 65(1), the distributor shall immediately inform the national \n",
      "competent authorities of the Member States in which it has made the product \n",
      "available to that effect, giving details, in particular, of the non -compliance and of any \n",
      "corrective actions taken.  \n",
      "5. Upon a reasoned request  from a national competent authority, distributors of high -\n",
      "risk AI systems shall provide that authority with all the information and \n",
      "documentation necessary to demonstrate the conformity of a high -risk system with \n",
      "the requirements set out in Chapter 2 of t his Title . Distributors shall also cooperate \n",
      "with that national competent authority on any action taken by that authority.  \n",
      "Article 28  \n",
      "Obligations of distributors, importers, users or any other third -party  \n",
      "1. Any distributor, importer, user or other third -party shall be considered a provider for \n",
      "the purposes of this Regulation and shall be subject to the obligations of the provider \n",
      "under Article 16, in any of the following circumstances:  \n",
      "(a) they place on the market or put into service a high -risk AI syste m under their \n",
      "name or trademark;  \n",
      "(b) they modify the intended purpose of a high -risk AI system already placed on \n",
      "the market or put into service;  \n",
      "(c) they make a substantial modification to the high -risk AI system.  \n",
      "2. Where the circumstances referred to in paragraph 1, point (b) or (c), occur, the \n",
      "provider that initially placed the high -risk AI system on the market or put it into \n",
      "service shall no longer be considered a provider for the purposes of this Regulation.  EN 58  EN Article 29  \n",
      "Obligations of users of high -risk AI systems  \n",
      "1. Users of high -risk AI systems shall use such systems in accordance with the \n",
      "instructions of use accompanying the systems, pursuant to paragraphs 2 and 5.  \n",
      "2. The obligations in paragraph 1 are without prejudice to other user obligations under \n",
      "Union or national law and to the user’s discretion  in organising its own resources and \n",
      "activities for the purpose of implementing the human oversight measures indicated \n",
      "by the provider.  \n",
      "3. Without prejudice to paragraph 1, to the extent the user exer cises control over the \n",
      "input data, that user shall ensure that input data is relevant in view of the intended \n",
      "purpose of the high -risk AI system.  \n",
      "4. Users shall monitor the operation of the high -risk AI system on the basis of the \n",
      "instructions of use. When  they have reasons to consider that the use in accordance \n",
      "with the instructions of use may result in the AI system presenting a risk within the \n",
      "meaning of Article 65(1) they shall inform the provider or distributor and suspend \n",
      "the use of the system. They s hall also inform the provider or distributor when they \n",
      "have identified any serious incident or any malfunctioning within the meaning of \n",
      "Article 62 and interrupt the use of the AI system. In case the user is not able to reach \n",
      "the provider, Article 62 shall apply mutatis mutandis.  \n",
      "For users that are credit institutions regulated by Directive 2013/36/EU, the \n",
      "monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled \n",
      "by complying with the rules on internal governance arrangements, processes and \n",
      "mechanisms pursuant to Article 74 of that Directive.  \n",
      "5. Users of high -risk AI systems shall keep the logs automatically generated by that \n",
      "high-risk AI system, to the extent such logs are under their control. The logs shall be \n",
      "kept for a perio d that is appropriate in the light of the intended purpose of the high -\n",
      "risk AI system and applicable legal obligations under Union or national law.  \n",
      "Users that are credit institutions regulated by Directive 2013/36/EU shall maintain \n",
      "the logs as part of the documentation concerning internal governance arrangements, \n",
      "processes and mechanisms  pursuant to Article 74 of that Directive.  \n",
      "6. Users of high -risk AI systems shall use the information provided under Article 13 to \n",
      "comply with their obligation to carry out a data protection impact assessment under \n",
      "Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, \n",
      "where applicable.  \n",
      "CHAPTER 4 \n",
      "NOTIFIYING AUTHORITI ES AND NOTIFIED BODI ES \n",
      "Article 30  \n",
      "Notifying authorities  \n",
      "1. Each Member State shall de signate or establish a notifying authority responsible for \n",
      "setting up and carrying out the necessary procedures for the assessment, designation \n",
      "and notification of conformity assessment bodies and for their monitoring .  \n",
      "2. Member States may designate a nat ional accreditation body referred to in Regulation \n",
      "(EC) No 765/2008 as a notifying authority.  EN 59  EN 3. Notifying authorities shall be established, organised and operated in such a way that \n",
      "no conflict of interest arises with conformity assessment bodies and the objectivity \n",
      "and impartiality of their activities are safeguarded.  \n",
      "4. Notifying authorities shall be organised in such a way that decisions relating to the \n",
      "notification of conformity assessment bodies are taken by competent persons \n",
      "different from those who carried out the assessment of those bodies.  \n",
      "5. Notifying authorities shall not offer or provide any activities that conformity \n",
      "assessment bodies perform or any consultancy services on a commercial or \n",
      "competitive basis.  \n",
      "6. Notifying authorities shall safegu ard the confidentiality of the information they \n",
      "obtain.  \n",
      "7. Notifying authorities shall have a sufficient number of competent personnel at their \n",
      "disposal for the proper performance of their tasks.  \n",
      "8. Notifying authorities shall make sure that conformity ass essments are carried out in a \n",
      "proportionate manner, avoiding unnecessary burdens for providers and that notified \n",
      "bodies perform their activities taking due account of the size of an undertaking, the \n",
      "sector in which it operates, its structure and the degree  of complexity of the AI \n",
      "system in question.  \n",
      "Article 31  \n",
      "Application of a conformity assessment body for notification  \n",
      "1. Conformity assessment bodies shall submit an application for notification to the \n",
      "notifying authority of the Member State in which they are established.  \n",
      "2. The application for notification shall be accompanied by a description of the \n",
      "conformity assessment activities, the conformity assessment module or modules and \n",
      "the artificial intelligence technologies for which the conformity assessment  body \n",
      "claims to be competent, as well as by an accreditation certificate, where one exists, \n",
      "issued by a national accreditation body attesting that the conformity assessment body \n",
      "fulfils the requirements laid down in Article 33. Any valid document related t o \n",
      "existing designations of the applicant notified body under any other Union \n",
      "harmonisation legislation shall be added.  \n",
      "3. Where the conformity assessment body concerned cannot provide an accreditation \n",
      "certificate, it shall provide the notifying authority with the documentary evidence \n",
      "necessary for the verification, recognition and regular monitoring of its compliance \n",
      "with the requirements laid down in Article 33. For notified bodies which are \n",
      "designated under any other Union harmonisation legislation, all documents and \n",
      "certificates linked to those designations may be used to support their designation \n",
      "procedure under this Regulation, as appropriate.  \n",
      "Article 32  \n",
      "Notification procedure  \n",
      "1. Notifying authorities may notify only conformity assessment bodies which have \n",
      "satisfied the requirements laid down in Article 33.  \n",
      "2. Notifying authorities shall notify the Commission and the other Member States using \n",
      "the electronic notification tool devel oped and managed by the Commission.  EN 60  EN 3. The notification shall include full details of the conformity assessment activities, the \n",
      "conformity assessment module or modules and the artificial intelligence technologies \n",
      "concerned.  \n",
      "4. The conformity assessment b ody concerned may perform the activities of a notified \n",
      "body only where no objections are raised by the Commission or the other Member \n",
      "States within one month of a notification.  \n",
      "5. Notifying authorities shall notify the Commission and the other Member States of \n",
      "any subsequent relevant changes to the notification.  \n",
      "Article 33  \n",
      "Notified bodies  \n",
      "1. Notified bodies shall verify the conformity of high -risk AI system in accordance with \n",
      "the conformity assessment procedures referred to in Article 43.  \n",
      "2. Notified bodies shall satisfy the organisational, quality management, resources and \n",
      "process requirements that are necessary to fulfil their tasks.  \n",
      "3. The organisational structure, allocation of responsibilities, reporting lines and \n",
      "operation of notified bodies shal l be such as to ensure that there is confidence in the \n",
      "performance by and in the results of the conformity assessment activities that the \n",
      "notified bodies conduct.  \n",
      "4. Notified bodies shall be independent of the provider of a high-risk AI system in \n",
      "relation to which it performs conformity assessment activities. Notified bodies shall \n",
      "also be independent of any other operator having an economic interest in the high -\n",
      "risk AI system that is assessed, as well as of any competitors of the provider.  \n",
      "5. Notified bodie s shall be organised and operated so as to safeguard the independence, \n",
      "objectivity and impartiality of their activities. Notified bodies shall document and \n",
      "implement a structure and procedures to safeguard impartiality and to promote and \n",
      "apply the principl es of impartiality throughout their organisation, personnel and \n",
      "assessment activities.  \n",
      "6. Notified bodies shall have documented procedures in place ensuring that their \n",
      "personnel, committees, subsidiaries, subcontractors and any associated body or \n",
      "personnel  of external bodies respect the confidentiality of the information which \n",
      "comes into their possession during the performance of conformity assessment \n",
      "activities, except when disclosure is required by law. The staff of notified bodies \n",
      "shall be bound to obser ve professional secrecy with regard to all information \n",
      "obtained in carrying out their tasks under this Regulation, except in relation to the \n",
      "notifying authorities of the Member State in which their activities are carried out.  \n",
      "7. Notified bodies shall have  procedures for the performance of activities which take \n",
      "due account of the size of an undertaking, the sector in which it operates, its \n",
      "structure, the degree of complexity of the AI system in question.  \n",
      "8. Notified bodies shall take out appropriate liabili ty insurance for their conformity \n",
      "assessment activities, unless liability is assumed by the Member State concerned in \n",
      "accordance with national law or that Member State is directly responsible for the \n",
      "conformity assessment.  \n",
      "9. Notified bodies shall be capab le of carrying out all the tasks falling to them under \n",
      "this Regulation with the highest degree of professional integrity and the requisite EN 61  EN competence in the specific field, whether those tasks are carried out by notified \n",
      "bodies themselves or on their behal f and under their responsibility.  \n",
      "10. Notified bodies shall have sufficient internal competences to be able to effectively \n",
      "evaluate the tasks conducted by external parties on their behalf. To that end, at all \n",
      "times and for each conformity assessment proced ure and each type of high-risk AI \n",
      "system in relation to which they have been designated, the notified body shall have \n",
      "permanent availability of sufficient administrative, technical and scientific personnel \n",
      "who possess experience and knowledge relating to t he relevant artificial intelligence \n",
      "technologies, data and data computing and to the requirements set out in Chapter 2 of \n",
      "this Title . \n",
      "11. Notified bodies shall participate in coordination activities as referred to in Article 38. \n",
      "They shall also take part d irectly or be represented in European standardisation \n",
      "organisations, or ensure that they are aware and up to date in respect of relevant \n",
      "standards.  \n",
      "12. Notified bodies shall make available and submit upon request all relevant \n",
      "documentation, including the p roviders’ documentation, to the notifying authority \n",
      "referred to in Article 30 to allow it to conduct its assessment, designation, \n",
      "notification, monitoring and surveillance activities and to facilitate the assessment \n",
      "outlined in this Chapter.  \n",
      "Article 34  \n",
      "Subsidiaries of and subcontracting by notified bodies  \n",
      "1. Where a notified body subcontracts specific tasks connected with the conformity \n",
      "assessment or has recourse to a subsidiary, it shall ensure that the subcontractor or \n",
      "the subsidiary meets the requirement s laid down in Article 33 and shall inform the \n",
      "notifying authority accordingly.  \n",
      "2. Notified bodies shall take full responsibility for the tasks performed by \n",
      "subcontractors or subsidiaries wherever these are established.  \n",
      "3. Activities may be subcontracted or carried out by a subsidiary only with the \n",
      "agreement of the provider.  \n",
      "4. Notified bodies shall keep at the disposal of the notifying authority the relevant \n",
      "documents concerning the assessment of the qualifications of the subcontractor or the \n",
      "subsidiary a nd the work carried out by them under this Regulation.  \n",
      "Article 35  \n",
      "Identification numbers and lists of notified bodies designated under this Regulation  \n",
      "1. The Commission shall assign an identification number to notified bodies. It shall \n",
      "assign a single numb er, even where a body is notified under several Union acts.  \n",
      "2. The Commission shall make publicly available the list of the bodies notified under \n",
      "this Regulation, including the identification numbers that have been assigned to them \n",
      "and the activities for w hich they have been notified. The Commission shall ensure \n",
      "that the list is kept up to date.  EN 62  EN Article 36  \n",
      "Changes to notifications  \n",
      "1. Where a notifying authority has suspicions or has been informed that a notified body \n",
      "no longer meets the requirements laid do wn in Article 33, or that it is failing to fulfil \n",
      "its obligations, that authority shall without delay investigate the matter with the \n",
      "utmost diligence. In that context, it shall inform the notified body concerned about \n",
      "the objections raised and give it the  possibility to make its views known. If the \n",
      "notifying authority comes to the conclusion that the notified body investigation no \n",
      "longer meets the requirements laid down in Article 33 or that it is failing to fulfil its \n",
      "obligations, it shall restrict, suspe nd or withdraw the notification as appropriate, \n",
      "depending on the seriousness of the failure. It shall also immediately inform the \n",
      "Commission and the other Member States accordingly.  \n",
      "2. In the event of restriction, suspension or withdrawal of notification, or where the \n",
      "notified body has ceased its activity, the notifying authority shall take appropriate \n",
      "steps to ensure that the files of that notified body are either taken over by another \n",
      "notified body or kept available for the responsible notifying authoriti es at their \n",
      "request.  \n",
      "Article 37  \n",
      "Challenge to the competence of notified bodies  \n",
      "1. The Commission shall, where necessary, investigate all cases where there are reasons \n",
      "to doubt whether a notified body complies with the requirements  laid down in Article \n",
      "33. \n",
      "2. The Notifying authority shall provide the Commission, on request, with all relevant \n",
      "information relating to the notification of the notified body concerned.  \n",
      "3. The Commission shall ensure that all confidential information obtained in the course \n",
      "of its i nvestigations pursuant to this Article is treated confidentially.  \n",
      "4. Where the Commission ascertains that a notified body does not meet or no longer \n",
      "meets the requirements  laid down in Article 33, it shall adopt a reasoned decision \n",
      "requesting the notifying  Member State to take the necessary corrective measures, \n",
      "including withdrawal of notification if necessary. That implementing act shall be \n",
      "adopted in accordance with the examination procedure referred to in Article 74(2).  \n",
      "Article 38  \n",
      "Coordination of notifie d bodies  \n",
      "1. The Commission shall ensure that, with regard to the areas covered by this \n",
      "Regulation, appropriate coordination and cooperation between notified bodies active \n",
      "in the conformity assessment procedures of AI systems pursuant to this Regulation \n",
      "are put in place and properly operated in the form of a sectoral group of notified \n",
      "bodies . \n",
      "2. Member States shall ensure that the bodies notified by them participate in the work \n",
      "of that group, directly or by means of designated representatives.  EN 63  EN Article 39  \n",
      "Conformity assessment bodies of third countries  \n",
      "Conformity assessment bodies established under the law of a third country with which the \n",
      "Union has concluded an agreement may be authorised to carry out the activities of notified \n",
      "Bodies under this Regulation.  \n",
      "CHAPTER 5 \n",
      "STANDARDS,  CONFORMITY  ASSESSMENT,  CERTIFICATES,  REGISTRATION  \n",
      "Article 40  \n",
      "Harmonised standards  \n",
      "High -risk AI systems which are in conformity with harmonised standards or parts thereof the \n",
      "references of which have been published in the Official Journa l of the European Union shall \n",
      "be presumed to be in conformity with the requirements set out in Chapter 2 of this Title , to the \n",
      "extent those standards cover those requirements.  \n",
      "Article 41  \n",
      "Common specifications  \n",
      "1. Where harmonised standards referred to in Ar ticle 40 do not exist or where the \n",
      "Commission considers that the relevant harmonised standards are insufficient or that \n",
      "there is a need to address specific safety or fundamental right concerns, the \n",
      "Commission may, by means of implementing acts, adopt commo n specifications in \n",
      "respect of the requirements set out in Chapter 2 of this Title . Those implementing \n",
      "acts shall be adopted in accordance with the examination procedure referred to in \n",
      "Article 74(2).  \n",
      "2. The Commission, when preparing the common specificati ons referred to in \n",
      "paragraph 1, shall gather the views of relevant bodies or expert groups established \n",
      "under relevant sectorial Union law.  \n",
      "3. High -risk AI systems which are in conformity with the common specifications \n",
      "referred to in paragraph 1 shall be pr esumed to be in conformity with the \n",
      "requirements set out in Chapter 2 of this Title,  to the extent those common \n",
      "specifications cover those requirements.  \n",
      "4. Where providers do not comply with the common specifications referred to in \n",
      "paragraph 1, they shall duly justify that they have adopted technical solutions that are \n",
      "at least equivalent thereto.  \n",
      "Article 42  \n",
      "Presumption of conformity with certain requirements  \n",
      "1. Taking into account their intended purpose, high -risk AI systems that have been \n",
      "trained and test ed on data concerning the specific geographical, behavioural and \n",
      "functional setting within which they are intended to be used shall be presumed to be \n",
      "in compliance with the requirement set out in Article 10(4).  EN 64  EN 2. High -risk AI systems that have been certi fied or for which a statement of conformity \n",
      "has been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 \n",
      "of the European Parliament and of the Council63 and the references of which have \n",
      "been published in the Official Journal of the Euro pean Union shall be presumed to be \n",
      "in compliance with the cybersecurity requirements set out in Article 15 of this \n",
      "Regulation in so far as the cybersecurity certificate or statement of conformity or \n",
      "parts thereof cover those requirements.  \n",
      "Article 43  \n",
      "Confor mity assessment  \n",
      "1. For high -risk AI systems listed in point 1 of Annex III, where, in demonstrating the \n",
      "compliance of a high -risk AI system with the requirements set out in Chapter 2 of \n",
      "this Title, the provider has applied harmonised standards referred to in Article 40, or, \n",
      "where applicable, common specifications referred to in Article 41, the provider shall \n",
      "follow one of the following procedures:  \n",
      "(a) the conformity assessment procedure based on internal control referred to in \n",
      "Annex VI;  \n",
      "(b) the conformity assessment procedure based on assessment of the quality \n",
      "management system and assessment of the technical documentation, with the \n",
      "involvement of a notified body, referred to in Annex VII.  \n",
      "Where, in demonstrating the compliance of a high -risk AI system with  the \n",
      "requirements set out in Chapter 2 of this Title , the provider has not applied or has \n",
      "applied only in part harmonised standards referred to in Article 40, or where such \n",
      "harmonised standards do not exist and common specifications referred to in Article \n",
      "41 are not available, the provider shall follow the conformity assessment procedure \n",
      "set out in Annex VII.  \n",
      "For the purpose of the conformity assessment procedure referred to in Annex VII, the \n",
      "provider may choose any of the notified bodies. However, when the  system is \n",
      "intended to be put into service by law enforcement, immigration or asylum \n",
      "authorities as well as EU institutions, bodies or agencies, the market surveillance \n",
      "authority referred to in Article 63(5) or (6), as applicable, shall act as a notified b ody. \n",
      "2. For high -risk AI systems referred to in points 2 to 8 of Annex III, providers shall \n",
      "follow the conformity assessment procedure based on internal control as referred to \n",
      "in Annex VI, which does not provide for the involvement of a notified body.  For \n",
      "high-risk AI systems referred to in point 5(b) of Annex III, placed on the market or \n",
      "put into service by credit institutions regulated by Directive 2013/36/EU, the \n",
      "conformity assessment  shall be carried out as part of the procedure referred to in \n",
      "Articles 97 to101 of that Directive.  \n",
      "3. For high -risk AI systems, to which legal acts listed in Annex II, section A, apply, the \n",
      "provider shall follow the relevant conformity assessment as required under those \n",
      "legal acts. The requirements set out in Chapter 2 of thi s Title  shall apply to those \n",
      "                                                 \n",
      "63 Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 April 2019 on ENISA \n",
      "(the European Uni on Agency for Cybersecurity) and on information and communications technology \n",
      "cybersecurity certification and repealing Regulation (EU) No 526/2013 (Cybersecurity Act) (OJ L 151, \n",
      "7.6.2019, p. 1).  EN 65  EN high-risk AI systems and shall be part of that assessment. Points 4.3., 4.4., 4.5. and \n",
      "the fifth paragraph of point 4.6 of Annex VII shall also apply.  \n",
      "For the purpose of that assessment, notified bodies which have been notifie d under \n",
      "those legal acts shall be entitled to control the conformity of the high -risk AI systems \n",
      "with the requirements set out in Chapter 2 of this Title , provided that the compliance \n",
      "of those notified bodies with requirements laid down in Article 33(4), ( 9) and (10) \n",
      "has been assessed in the context of the notification procedure under those legal acts.  \n",
      "Where the legal acts listed in Annex II, section A, enable the manufacturer of the \n",
      "product to opt out from a third -party conformity assessment, provided that  that \n",
      "manufacturer has applied all harmonised standards covering all the relevant \n",
      "requirements, that manufacturer  may make use of that option only if he has also \n",
      "applied harmonised standards or, where applicable, common specifications referred \n",
      "to in Articl e 41, covering the requirements set out in Chapter 2 of this Title.  \n",
      "4. High -risk AI systems shall undergo a new conformity assessment procedure \n",
      "whenever they are substantially modified, regardless of whether the modified system \n",
      "is intended to be further distributed or continues to be used by the current user.  \n",
      "For high -risk AI systems that continue to learn after being placed on the market or \n",
      "put into service, changes to the high -risk AI system and its performance that have \n",
      "been pre -determined by the provi der at the moment of the initial conformity \n",
      "assessment and are part of the information contained in the technical documentation \n",
      "referred to in point 2(f) of Annex IV, shall not constitute a substantial modification.  \n",
      "5. The Commission is empowered to adopt delegated acts in accordance with Article 73 \n",
      "for the purpose of updating Annexes VI and Annex VII in order to introduce \n",
      "elements of the conformity assessment procedures that become necessary in light of \n",
      "technical progress.  \n",
      "6. The Commission is empowered to  adopt delegated acts to amend paragraphs 1 and 2 \n",
      "in order to subject high -risk AI systems referred to in points 2 to 8 of Annex III to \n",
      "the conformity assessment procedure referred to in Annex VII or parts thereof. The \n",
      "Commission shall adopt such delegated  acts taking into account the effectiveness of \n",
      "the conformity assessment procedure based on internal control referred to in Annex \n",
      "VI in preventing or minimizing the risks to health and safety and protection of \n",
      "fundamental rights posed by such systems as we ll as the availability of adequate \n",
      "capacities and resources among notified bodies.  \n",
      "Article 44  \n",
      "Certificates  \n",
      "1. Certificates issued by notified bodies in accordance with Annex VII shall be drawn -\n",
      "up in an official Union language determined by the Member State  in which the \n",
      "notified body is established or in an official Union language otherwise acceptable to \n",
      "the notified body.  \n",
      "2. Certificates shall be valid for the period they indicate, which shall not exceed five \n",
      "years. On application by the provider, the vali dity of a certificate may be extended \n",
      "for further periods, each not exceeding five years, based on a re -assessment in \n",
      "accordance with the applicable conformity assessment procedures.  \n",
      "3. Where a notified body finds that an AI system no longer meets the requirements set \n",
      "out in Chapter 2 of this Title , it shall, taking account of the principle of EN 66  EN proportionality, suspend or withdraw the certificate issued or impose any restrictions \n",
      "on it, unless compliance with those requirements is ensured by appropriate cor rective \n",
      "action taken by the provider of the system within an appropriate deadline set by the \n",
      "notified body. The notified body shall give reasons for its decision.  \n",
      "Article 45  \n",
      "Appeal against decisions of notified bodies  \n",
      "Member States shall ensure that an app eal procedure against decisions of the notified bodies \n",
      "is available to parties having a legitimate interest in that decision . \n",
      "Article 46  \n",
      "Information obligations of notified bodies  \n",
      "1. Notified bodies shall inform the notifying authority of the following:  \n",
      "(a) any Union technical documentation assessment certificates, any supplements to \n",
      "those certificates, quality management system approvals issued in accordance \n",
      "with the requirements of Annex VII;  \n",
      "(b) any refusal, restriction, suspension or withdrawal of a Un ion technical \n",
      "documentation assessment certificate or a quality management system approval \n",
      "issued in accordance with the requirements of Annex VII;  \n",
      "(c) any circumstances affecting the scope of or conditions for notification;  \n",
      "(d) any request for informatio n which they have received from market surveillance \n",
      "authorities regarding conformity assessment activities;  \n",
      "(e) on request, conformity assessment activities performed within the scope of \n",
      "their notification and any other activity performed, including cross -border \n",
      "activities and subcontracting.  \n",
      "2. Each notified body shall inform the other notified bodies of:  \n",
      "(a) quality management system approvals which it has refused, suspended or \n",
      "withdrawn, and, upon request, of quality system approvals which it has issued;  \n",
      "(b) EU technical documentation assessment certificates or any supplements thereto \n",
      "which it has refused, withdrawn, suspended or otherwise restricted, and, upon \n",
      "request, of the certificates and/or supplements thereto which it has issued.  \n",
      "3. Each notified body shall provide the other notified bodies carrying out similar \n",
      "conformity assessment activities covering the same artificial intelligence \n",
      "technologies with relevant information on issues relating to negative and, on request, \n",
      "positive confo rmity assessment results.  \n",
      "Article 47  \n",
      "Derogation from conformity assessment procedure  \n",
      "1. By way of derogation from Article 43, any market surveillance authority may \n",
      "authorise the placing on the market or putting into service of specific high -risk AI \n",
      "systems  within the territory of the Member State concerned, for exceptional reasons \n",
      "of public security or the protection of life and health of persons, environmental \n",
      "protection and the protection of key industrial and infrastructural assets. That \n",
      "authorisation sh all be for a limited period of time, while the necessary conformity EN 67  EN assessment procedures are being carried out, and shall terminate once those \n",
      "procedures have been completed.  The completion of those procedures shall be \n",
      "undertaken without undue delay.  \n",
      "2. The authorisation referred to in paragraph 1 shall be issued only if the market \n",
      "surveillance authority concludes that the high -risk AI system complies with the \n",
      "requirements of Chapter 2 of this Title. The market surveillance authority shall \n",
      "inform the Commi ssion and the other Member States of any authorisation issued \n",
      "pursuant to paragraph 1.  \n",
      "3. Where, within 15 calendar days of receipt of the information referred to in paragraph \n",
      "2, no objection has been raised by either a Member State or the Commission in \n",
      "respect of an authorisation issued by a market surveillance authority of a Member \n",
      "State in accordance with paragraph 1, that authorisation shall be deemed justified.  \n",
      "4. Where, within 15 calendar days of receipt of the notification referred to in paragraph \n",
      "2, objections are raised by a Member State against an authorisation issued by a \n",
      "market surveillance authority of another Member State, or where the Commission \n",
      "considers the authorisation to be contrary to Union law or the conclusion of the \n",
      "Member States rega rding the compliance of the system as referred to in paragraph 2 \n",
      "to be unfounded, the Commission shall without delay enter into consultation with the \n",
      "relevant Member State; the operator(s) concerned shall be consulted and have the \n",
      "possibility to present th eir views. In view thereof, the Commission shall decide \n",
      "whether the authorisation is justified or not. The Commission shall address its \n",
      "decision to the Member State concerned and the relevant operator or operators.  \n",
      "5. If the authorisation is considered unj ustified, this shall be withdrawn by the market \n",
      "surveillance authority of the Member State concerned.  \n",
      "6. By way of derogation from paragraphs 1 to 5, for high -risk AI systems intended to be \n",
      "used as safety components of devices, or which are themselves devi ces, covered by \n",
      "Regulation (EU) 2017/745 and Regulation (EU) 2017/746, Article 59 of Regulation \n",
      "(EU) 2017/745 and Article 54 of Regulation (EU) 2017/746 shall apply also with \n",
      "regard to the derogation from the conformity assessment of the compliance with th e \n",
      "requirements set out in Chapter 2 of this Title.  \n",
      "Article 48  \n",
      "EU declaration of conformity  \n",
      "1. The provider shall draw up a written EU declaration of conformity for each AI \n",
      "system and keep it at the disposal of the national competent authorities for 10 years \n",
      "after the AI system has been placed on the market or put into service. The EU \n",
      "declaration of co nformity shall identify the AI system for which it has been drawn \n",
      "up. A copy of the EU declaration of conformity shall be given to the relevant \n",
      "national competent authorities upon request.  \n",
      "2. The EU declaration of conformity shall state that the high -risk AI system in question \n",
      "meets the requirements set out in Chapter 2 of this Title . The EU declaration of \n",
      "conformity shall contain the information set out in Annex V and shall be translated \n",
      "into an official Union language or languages required by the Member S tate(s) in \n",
      "which the high -risk AI system is made available.  \n",
      "3. Where high -risk AI systems are subject to other Union harmonisation legislation \n",
      "which also requires an EU declaration of conformity, a single EU declaration of \n",
      "conformity shall be drawn up in respect of all Union legislations applicable to the EN 68  EN high-risk AI system. The declaration shall contain all the information required for \n",
      "identification of the Union harmonisation legislation to which the declaration relates.  \n",
      "4. By drawing up the EU declara tion of conformity, the provider shall assume \n",
      "responsibility for compliance with the requirements set out in Chapter 2 of this Title . \n",
      "The provider shall keep the EU declaration of conformity up -to-date as appropriate.  \n",
      "5. The Commission shall be empowered t o adopt delegated acts in accordance with \n",
      "Article 73 for the purpose of updating the content of the EU declaration of \n",
      "conformity set out in Annex V in order to introduce elements that become necessary \n",
      "in light of technical progress.  \n",
      "Article 49  \n",
      "CE marking o f conformity  \n",
      "1. The CE marking shall be affixed visibly, legibly and indelibly for high -risk AI \n",
      "systems. Where that is not possible or not warranted on account of the nature of the \n",
      "high-risk AI system, it shall be affixed to the packaging or to the accompa nying \n",
      "documentation, as appropriate.  \n",
      "2. The CE marking referred to in paragraph 1 of this Article shall be subject to the \n",
      "general principles set out in Article 30 of Regulation (EC) No 765/2008.  \n",
      "3. Where applicable, the CE marking shall be followed by the  identification number of \n",
      "the notified body responsible for the conformity assessment procedures set out in \n",
      "Article 43. The identification number shall also be indicated in any promotional \n",
      "material which mentions that the high -risk AI system fulfils the re quirements for CE \n",
      "marking.  \n",
      "Article 50  \n",
      "Document retention  \n",
      "The provider shall, for a period ending 10 years after the AI system has been placed on the \n",
      "market or put into service, keep at the disposal of the national competent authorities:  \n",
      "(a) the technical documentation referred to in Article 11;  \n",
      "(b) the documentation concerning the quality management system referred to Article 17;  \n",
      "(c) the documentation concerning the changes approved by notified bodies where \n",
      "applicable;  \n",
      "(d) the decisions and other documen ts issued by the notified bodies where applicable;  \n",
      "(e) the EU declaration of conformity referred to in Article 48.  \n",
      "Article 51  \n",
      "Registration  \n",
      "Before placing on the market or putting into service a high -risk AI system referred to in \n",
      "Article 6(2), the provider  or, where applicable, the authorised representative shall register that \n",
      "system in the EU database referred to in Article 60.  EN 69  EN TITLE  IV \n",
      "TRANSPARENCY  OBLIGATIONS  FOR  CERTAIN  AI SYSTEMS  \n",
      "Article 52  \n",
      "Transparency obligations for certain AI systems  \n",
      "1. Providers shall ensure that AI systems intended to interact with natural persons are \n",
      "designed and developed in such a way that natural persons are informed that they are \n",
      "interacting with an AI system, unless this is obvious from the circumstances and the \n",
      "context of use. This obligation shall not apply to AI systems authorised by law to \n",
      "detect, prevent, investigate and prosecute criminal offences, unless those systems are \n",
      "available for the public to report a criminal offence.  \n",
      "2. Users of an emotion recognition system or a biometric categorisation system shall \n",
      "inform of the operation of the system the natural persons exposed thereto. This \n",
      "obligation shall not apply to AI systems used for biometric categorisation, which are \n",
      "permitted by law to detect, prevent and investi gate criminal offences.  \n",
      "3. Users of an AI system that generates or manipulates image, audio or video content \n",
      "that appreciably resembles  existing persons, objects, places or other entities or events  \n",
      "and would falsely appear to a person to be authentic or tr uthful (‘deep fake’), shall \n",
      "disclose  that the content has been artificially generated or manipulated.  \n",
      "However, the first subparagraph shall not apply where the use is authorised by law to \n",
      "detect, prevent, investigate and prosecute criminal offences  or it is necessary for the \n",
      "exercise of the right to freedom of expression and the right to freedom of the arts and \n",
      "sciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to \n",
      "appropriate safeguards for the rights and freedoms of third part ies. \n",
      "4. Paragraphs 1, 2 and 3 shall not affect the requirements and obligations set out in Title \n",
      "III of this Regulation.  \n",
      "TITLE V  \n",
      "MEASURES  IN SUPPORT  OF INNOVATION  \n",
      "Article 53  \n",
      "AI regulatory sandboxes  \n",
      "1. AI regulatory sandboxes established by one or more Member States competent \n",
      "authorities or the European Data Protection Supervisor shall provide a controlled \n",
      "environment that facilitates the development, testing and validation of innovative AI \n",
      "systems for a  limited time before their placement on the market or putting into \n",
      "service  pursuant to a specific plan. This shall take place under the direct supervision \n",
      "and guidance by the competent authorities  with a view to ensuring compliance with \n",
      "the requirements of  this Regulation and, where relevant, other Union and Member \n",
      "States legislation supervised within the sandbox.  \n",
      "2. Member States shall ensure that to the extent the innovative AI systems involve the \n",
      "processing of personal data or otherwise fall under the s upervisory remit of other \n",
      "national authorities or competent authorities providing or supporting access to data, EN 70  EN the national data protection authorities and those other national authorities are \n",
      "associated to the operation of the AI regulatory sandbox.  \n",
      "3. The AI regulatory sandboxes shall not affect the supervisory and corrective powers \n",
      "of the competent authorities. Any significant risks to health and safety and \n",
      "fundamental rights identified during the development and testing of such systems \n",
      "shall result in immediate mitigation and, failing that, in the suspension of the \n",
      "development and testing process until such mitigation takes place.  \n",
      "4. Participants in the AI regulatory sandbox shall remain liable under applicable Union \n",
      "and Member States liability legislat ion for any harm inflicted on third parties as a \n",
      "result from the experimentation taking place in the sandbox.  \n",
      "5. Member States’ competent authorities that have established AI regulatory sandboxes \n",
      "shall coordinate their activities and cooperate within the f ramework of the European \n",
      "Artificial Intelligence Board. They shall submit annual reports to the Board and the \n",
      "Commission on the results from the implementation of those scheme, including good \n",
      "practices, lessons learnt and recommendations on their setup and , where relevant, on \n",
      "the application of this Regulation and other Union legislation supervised within the \n",
      "sandbox.  \n",
      "6. The modalities and the conditions of the operation of the AI regulatory sandboxes, \n",
      "including the eligibility criteria and the procedure f or the application, selection, \n",
      "participation and exiting from the sandbox, and the rights and obligations of the \n",
      "participants shall be set out in implementing acts. Those implementing acts shall be \n",
      "adopted in accordance with the examination procedure refer red to in Article 74(2).  \n",
      "Article 54  \n",
      "Further processing of personal data for developing certain AI systems in the public interest in \n",
      "the AI regulatory sandbox  \n",
      "1. In the AI regulatory sandbox personal data lawfully collected for other purposes shall \n",
      "be proce ssed for the purposes of developing and testing certain innovative AI \n",
      "systems in the sandbox under the following conditions:  \n",
      "(a) the innovative AI systems shall be developed for safeguarding substantial \n",
      "public interest in one or more of the following areas : \n",
      "(i) the prevention, investigation, detection or prosecution of criminal \n",
      "offences or the execution of criminal penalties, including the \n",
      "safeguarding against and the prevention of threats to public security, \n",
      "under the control and responsibility of the comp etent authorities. The \n",
      "processing shall be based on Member State or Union law;  \n",
      "(ii) public safety and public health, including disease prevention, control and \n",
      "treatment;  \n",
      "(iii) a high level of protection and improvement of the quality of the \n",
      "environment;  \n",
      "(b) the data processed are necessary for complying with one or more of the \n",
      "requirements referred to in Title III, Chapter 2 where those requirements \n",
      "cannot be effectively fulfilled by processing anonymised, synthetic or other \n",
      "non-personal data;  EN 71  EN (c) there a re effective monitoring mechanisms to identify if any high risks to the \n",
      "fundamental rights of the data subjects may arise during the sandbox \n",
      "experimentation as well as response mechanism to promptly mitigate those \n",
      "risks and, where necessary, stop the proce ssing;  \n",
      "(d) any personal data to be processed in the context of the sandbox are in a \n",
      "functionally separate, isolated and protected data processing environment \n",
      "under the control of the participants and only authorised persons have access to \n",
      "that data;  \n",
      "(e) any personal data processed are not be transmitted, transferred or otherwise \n",
      "accessed by other parties;  \n",
      "(f) any processing of personal data in the context of the sandbox do not lead to \n",
      "measures or decisions affecting the data subjects;  \n",
      "(g) any personal da ta processed in the context of the sandbox are deleted once the \n",
      "participation in the sandbox has terminated or the personal data has reached the \n",
      "end of its retention period;  \n",
      "(h) the logs of the processing of personal data in the context of the sandbox are  \n",
      "kept for the duration of the participation in the sandbox and 1 year after its \n",
      "termination, solely for the purpose of and only as long as necessary for \n",
      "fulfilling accountability and documentation obligations under this Article or \n",
      "other application Union o r Member States legislation;  \n",
      "(i) complete and detailed description of the process and rationale behind the \n",
      "training, testing and validation of the AI system is kept together with the \n",
      "testing results as part of the technical documentation in Annex IV;  \n",
      "(j) a short summary of the AI project developed in the sandbox, its objectives and \n",
      "expected results published on the website of the competent authorities.  \n",
      "2. Paragraph 1 is without prejudice to Union or Member States legislation excluding \n",
      "processing for other p urposes than those explicitly mentioned in that legislation.  \n",
      "Article 55  \n",
      "Measures for small -scale providers and users  \n",
      "1. Member States shall undertake the following actions:  \n",
      "(a) provide small -scale providers and start -ups with priority access to the AI \n",
      "regulatory sandboxes to the extent that they fulfil the eligibility conditions;  \n",
      "(b) organise specific awareness raising activities about the application of this \n",
      "Regulation tailored to the needs of the small -scale providers and users ; \n",
      "(c) where appropriate, establish a dedicated channel for communication with \n",
      "small -scale providers and user and other innovators to provide guidance and \n",
      "respond to queries about the implementation of this Regulation.  \n",
      "2. The specific interests and needs of the s mall-scale providers shall be taken into \n",
      "account when setting the fees for conformity assessment under Article 43, reducing \n",
      "those fees proportionately to their size and market size.  EN 72  EN TITLE  VI \n",
      "GOVERNANCE  \n",
      "CHAPTER 1 \n",
      "EUROPEAN ARTIFICIAL INTELLIGENCE BOARD  \n",
      "Artic le 56  \n",
      "Establishment of the European Artificial Intelligence Board  \n",
      "1. A ‘European Artificial Intelligence Board’ (the ‘Board’) is established.  \n",
      "2. The Board shall provide advice and assistance to the Commission in order to:  \n",
      "(a) contribute to the effective co operation of the national supervisory authorities \n",
      "and the Commission with regard to matters covered by this Regulation;  \n",
      "(b) coordinate and contribute to guidance and analysis by the Commission and the \n",
      "national supervisory authorities and other competent au thorities on emerging \n",
      "issues across the internal market with regard to matters covered by this \n",
      "Regulation;  \n",
      "(c) assist the national supervisory authorities and the Commission in ensuring the \n",
      "consistent application of this Regulation.  \n",
      "Article 57  \n",
      "Structure of the Board  \n",
      "1. The Board shall be composed of the national supervisory authorities, who shall be \n",
      "represented by the head or equivalent high -level official of that authority, and the \n",
      "European Data Protection Supervisor. Other national authoritie s may be invited to \n",
      "the meetings, where the issues discussed are of relevance for them.  \n",
      "2. The Board shall adopt its rules of procedure by a simple majority of its members, \n",
      "following the consent of the Commission. The rules of procedure shall also contain \n",
      "the operational aspects related to the execution of the Board’s tasks as listed in \n",
      "Article 58. The Board may establish sub -groups as appropriate  for the purpose of \n",
      "examining specific questions.  \n",
      "3. The Board shall be chaired by the Commission. The Commissio n shall convene the \n",
      "meetings and prepare the agenda in accordance with the tasks of the Board pursuant \n",
      "to this Regulation and with its rules of procedure. The Commission shall provide \n",
      "administrative and analytical support for the activities of the Board pu rsuant to this \n",
      "Regulation.  \n",
      "4. The Board may invite external experts and observers to attend its meetings and may \n",
      "hold exchanges with interested third parties to inform its activities to an appropriate \n",
      "extent. To that end the Commission may facilitate excha nges between the Board and \n",
      "other Union bodies, offices, agencies and advisory groups.  EN 73  EN Article 58  \n",
      "Tasks of the Board  \n",
      "When providing advice and assistance to the Commission in the context of Article 56(2), the \n",
      "Board shall  in particular:  \n",
      "(a) collect and share  expertise and best practices among Member States;  \n",
      "(b) contribute to uniform administrative practices in the Member States, including for \n",
      "the functioning of regulatory sandboxes referred to in Article 53;  \n",
      "(c) issue opinions, recommendations or written cont ributions on matters related to the \n",
      "implementation of this Regulation, in particular  \n",
      "(i) on technical specifications or existing standards regarding the requirements set \n",
      "out in Title III, Chapter 2 ,  \n",
      "(ii) on the use of harmonised standards or common specif ications referred to in \n",
      "Articles 40 and 41,  \n",
      "(iii) on the preparation of guidance documents, including the guidelines concerning \n",
      "the setting of administrative fines referred to in Article 71.  \n",
      "CHAPTER  2 \n",
      "NATIONAL COMPETENT A UTHORITIES  \n",
      "Article 59  \n",
      "Designation o f national competent authorities  \n",
      "1. National competent authorities shall be established or designated by each Member \n",
      "State for the purpose of ensuring the application and implementation of this \n",
      "Regulation. National competent authorities shall be organised  so as to safeguard the \n",
      "objectivity and impartiality of their activities and tasks.  \n",
      "2. Each Member State shall designate a national supervisory authority among the \n",
      "national competent authorities.  The national supervisory authority shall act as \n",
      "notifying au thority and market surveillance authority unless a Member State has \n",
      "organisational and administrative reasons to designate more than one authority.  \n",
      "3. Member States shall inform the Commission of their designation or designations and, \n",
      "where applicable, the  reasons for designating more than one authority.  \n",
      "4. Member States shall ensure that national competent authorities are provided with \n",
      "adequate financial and human resources to fulfil their tasks under this Regulation. In \n",
      "particular, national competent aut horities shall have a sufficient number of personnel \n",
      "permanently available whose competences and expertise shall include an in -depth \n",
      "understanding of artificial intelligence technologies, data and data computing, \n",
      "fundamental rights, health and safety risks  and knowledge of existing standards and \n",
      "legal requirements.  \n",
      "5. Member States shall report to the Commission on an annual basis on the status of the \n",
      "financial and human resources of the national competent authorities with an \n",
      "assessment of their adequacy. The Commission shall transmit that information to the \n",
      "Board for discussion and possible recommendations.  \n",
      "6. The Commission shall facilitate the exchange of experience between national \n",
      "competent authorities.  EN 74  EN 7. National competent authorities may provide gu idance and advice on the \n",
      "implementation of this Regulation, including to small -scale providers. Whenever \n",
      "national competent authorities intend to provide guidance and advice with regard to \n",
      "an AI system in areas covered by other Union legislation, the compe tent national \n",
      "authorities under that Union legislation shall be consulted, as appropriate. Member \n",
      "States may also establish one central contact point for communication with operators.  \n",
      "8. When Union institutions, agencies and bodies fall within the scope of  this \n",
      "Regulation, the European Data Protection Supervisor shall act as the competent \n",
      "authority for their supervision.  \n",
      "TITLE VII  \n",
      "EU DATABASE  FOR  STAND -ALONE  HIGH -RISK  AI SYSTEMS  \n",
      "Article 60  \n",
      "EU database for stand -alone high -risk AI systems  \n",
      "1. The Commission s hall, in collaboration with the Member States, set up and maintain \n",
      "a EU database containing information referred to in paragraph 2 concerning high -risk \n",
      "AI systems referred to in Article 6(2) which are registered in accordance with Article \n",
      "51. \n",
      "2. The data listed in Annex VIII shall be entered into the EU database by the providers.  \n",
      "The Commission shall provide them with technical and administrative support.  \n",
      "3. Information contained in the EU database shall be accessible to the public.  \n",
      "4. The EU database sha ll contain personal data only insofar as necessary for collecting \n",
      "and processing information in accordance with this Regulation. That information \n",
      "shall include the names and contact details of natural persons who are responsible for \n",
      "registering the system and have the legal authority to represent the provider.  \n",
      "5. The Commission shall be the controller of the EU database. It shall also ensure to \n",
      "providers adequate technical and administrative support.  \n",
      "TITLE  VIII \n",
      "POST -MARKET  MONITORING,  INFORMATION  SHARING,  MARKET  \n",
      "SURVEILLANCE  \n",
      "CHAPTER 1 \n",
      "POST-MARKET MONITORING  \n",
      "Article 61  \n",
      "Post-market monitoring by providers and post -market monitoring plan for high -risk AI \n",
      "systems  \n",
      "1. Providers shall establish and document a post -market monitoring system in a manner \n",
      "that is propor tionate to the nature of the artificial intelligence technologies and the \n",
      "risks of the high -risk AI system.  EN 75  EN 2. The post -market monitoring system shall actively and systematically collect, \n",
      "document and analyse relevant data provided by users or collected th rough other \n",
      "sources on the performance of high -risk AI systems throughout their lifetime, and \n",
      "allow the provider to evaluate the continuous compliance of AI systems with the \n",
      "requirements set out in Title III, Chapter 2 . \n",
      "3. The post -market monitoring system  shall be based on a post -market monitoring plan. \n",
      "The post -market monitoring plan shall be part of the technical documentation \n",
      "referred to in Annex IV. The Commission shall adopt an implementing act laying \n",
      "down detailed provisions establishing a template f or the post -market monitoring plan \n",
      "and the list of elements to be included in the plan.  \n",
      "4. For high -risk AI systems covered by the legal acts referred to in Annex II, where a \n",
      "post-market monitoring system and plan is already established under that legislat ion, \n",
      "the elements described in paragraphs 1, 2 and 3 shall be integrated into that system \n",
      "and plan as appropriate.  \n",
      "The first subparagraph shall also apply to high -risk AI systems referred to in point \n",
      "5(b) of Annex III placed on the market or put into servi ce by credit institutions \n",
      "regulated by Directive 2013/36/EU.  \n",
      "CHAPTER 2 \n",
      "SHARING OF INFORMATIO N ON INCIDENTS AND M ALFUNCTIONING  \n",
      "Article 62  \n",
      "Reporting of serious incidents and of malfunctioning  \n",
      "1. Providers of high -risk AI systems placed on the Union market shall report any \n",
      "serious incident or any malfunctioning of those systems which constitutes a breach of \n",
      "obligations under Union law intended to protect fundamental rights to the market \n",
      "surveillanc e authorities of the Member States where that incident or breach occurred.  \n",
      "Such notification shall be made immediately after the provider has established a \n",
      "causal link between the AI system and the incident or malfunctioning or the \n",
      "reasonable likelihood o f such a link, and, in any event, not later than 15 days after the \n",
      "providers  becomes aware of the serious incident or of the malfunctioning.  \n",
      "2. Upon receiving a notification related to a breach of obligations under Union law \n",
      "intended to protect fundamental  rights, the market surveillance authority shall inform \n",
      "the national public authorities or bodies referred to in Article 64(3). The Commission \n",
      "shall develop dedicated guidance to facilitate compliance with the obligations set out \n",
      "in paragraph 1. That guida nce shall be issued 12 months after the entry into force of \n",
      "this Regulation, at the latest.  \n",
      "3. For high -risk AI systems referred to in point 5(b) of Annex III which are placed on \n",
      "the market or put into service by providers that are credit institutions regu lated by \n",
      "Directive 2013/36/EU and for high -risk AI systems which are safety components of \n",
      "devices, or are themselves devices, covered by Regulation (EU) 2017/745 and \n",
      "Regulation (EU) 2017/746, the notification of serious incidents or malfunctioning \n",
      "shall be  limited to those that that constitute a breach of obligations under Union law \n",
      "intended to protect fundamental rights.  EN 76  EN CHAPTER 3 \n",
      "ENFORCEMENT  \n",
      "Article 63  \n",
      "Market surveillance and control of AI systems in the Union market  \n",
      "1. Regulation (EU) 2019/1020 shall ap ply to AI systems covered by this Regulation. \n",
      "However, for the purpose of the effective enforcement of this Regulation:  \n",
      "(a) any reference to an economic operator under Regulation (EU) 2019/1020 shall \n",
      "be understood as including all operators identified in T itle III, Chapter 3 of this \n",
      "Regulation;  \n",
      "(b) any reference to a product under Regulation (EU) 2019/1020 shall be \n",
      "understood as including all AI systems falling within the scope of this \n",
      "Regulation.  \n",
      "2. The national supervisory authority shall report to the Commission on a regular basis \n",
      "the outcomes of relevant market surveillance activities. The national supervisory \n",
      "authority shall report, without delay, to the Commission and relevant national \n",
      "competition authorities any information identified in the course of market \n",
      "surveillance activities that may be of potential interest for the application of Union \n",
      "law on competition rules.  \n",
      "3. For high -risk AI systems, related to products to which legal acts listed in Annex II, \n",
      "section A apply, the market surveillance aut hority for the purposes of this Regulation \n",
      "shall be the authority responsible for market surveillance activities designated under \n",
      "those legal acts.  \n",
      "4. For AI systems placed on the market, put into service or used by financial institutions \n",
      "regulated by Unio n legislation on financial services, the market surveillance \n",
      "authority for the purposes of this Regulation shall be the relevant authority \n",
      "responsible for the financial supervision of those institutions under that legislation.  \n",
      "5. For AI systems listed in p oint 1(a) in so far as the systems are used for law \n",
      "enforcement purposes, points 6 and 7  of Annex III, Member States shall designate as \n",
      "market surveillance authorities for the purposes of this Regulation either the \n",
      "competent data protection supervisory aut horities under Directive (EU) 2016/680, or \n",
      "Regulation 2016/679 or the national competent authorities supervising the activities \n",
      "of the law enforcement, immigration or asylum authorities putting into service or \n",
      "using those systems.  \n",
      "6. Where Union institutio ns, agencies and bodies fall within the scope of this \n",
      "Regulation, the European Data Protection Supervisor shall act as their market \n",
      "surveillance authority.  \n",
      "7. Member States shall facilitate the coordination between market surveillance \n",
      "authorities designate d under this Regulation and other relevant national authorities or \n",
      "bodies which supervise the application of Union harmonisation legislation listed in \n",
      "Annex II  or other Union legislation that might be relevant for the high -risk AI \n",
      "systems referred to in An nex III . EN 77  EN Article 64  \n",
      "Access to data and documentation  \n",
      "1. Access to data and documentation i n the context of their activities, t he market \n",
      "surveillance authorities shall be granted full access to the training, validation and \n",
      "testing datasets used by the provi der, including through application programming \n",
      "interfaces (‘API’) or other appropriate technical means and tools enabling remote \n",
      "access.  \n",
      "2. Where necessary to assess the conformity of the high -risk AI system with the \n",
      "requirements set out in Title III, Chap ter 2 and upon a reasoned request, the market \n",
      "surveillance authorities shall be granted access to the source code of the AI system . \n",
      "3. National public authorities or bodies which supervise or enforce the respect of \n",
      "obligations under Union law protecting fu ndamental rights in relation to the use of \n",
      "high-risk AI systems referred to in Annex III shall have the power to request and \n",
      "access any documentation created or maintained under this Regulation when access \n",
      "to that documentation is necessary for the fulfilm ent of the competences under their \n",
      "mandate within the limits of their jurisdiction. The relevant public authority or body \n",
      "shall inform the market surveillance authority of the Member State concerned of any \n",
      "such request.  \n",
      "4. By 3 months after the entering in to force of this Regulation, each Member State shall \n",
      "identify the public authorities or bodies referred to in paragraph 3 and make a list \n",
      "publicly available on the website of the national supervisory authority. Member \n",
      "States shall notify the list to the Co mmission and all other Member States and keep \n",
      "the list up to date.  \n",
      "5. Where the documentation referred to in paragraph 3 is insufficient to ascertain \n",
      "whether a breach of obligations under Union law intended to protect fundamental \n",
      "rights has occurred, the public authority or body referred to paragraph 3 may make a \n",
      "reasoned request to the market surveillance authority to organise testing of the high -\n",
      "risk AI system through technical means. The market surveillance authority shall \n",
      "organise the testing with the close involvement of the requesting public authority or \n",
      "body within reasonable time following the request.  \n",
      "6. Any information and documentation obtained by the national public authorities or \n",
      "bodies referred to in paragraph 3 pursuant to the provisions of this Article shall be \n",
      "treated in compliance with the confidentiality obligations set out in Article 70.  \n",
      "Article 65  \n",
      "Procedure for dealing with AI systems presenting a risk at national level  \n",
      "1. AI systems presenting a risk shall be understood as a product pr esenting a risk \n",
      "defined in Article 3, point 19 of Regulation (EU) 2019/1020 insofar as risks to the \n",
      "health or safety or to the protection of fundamental rights of persons are concerned.  \n",
      "2. Where the market surveillance authority of a Member State has suffi cient reasons to \n",
      "consider that an AI system presents a risk as referred to in paragraph 1, they shall \n",
      "carry out an evaluation of the AI system concerned in respect of its compliance with \n",
      "all the requirements and obligations laid down in this Regulation. Wh en risks to the \n",
      "protection of fundamental rights are present, the market surveillance authority shall \n",
      "also inform the relevant national public authorities or bodies referred to in Article \n",
      "64(3).  The relevant operators shall cooperate as necessary with the market EN 78  EN surveillance authorities and the other national public authorities or bodies referred to \n",
      "in Article 64(3).  \n",
      "Where, in the course of that evaluation, the market surveillance authority finds that \n",
      "the AI system does not comply with the requirements and obligations laid down in \n",
      "this Regulation, it shall without delay require the relevant operator to take all \n",
      "appropriate corrective actions to bring the AI system into compliance, to withdraw \n",
      "the AI system from the market, or to recall it within a reasonable  period, \n",
      "commensurate with the nature of the risk, as it may prescribe.  \n",
      "The market surveillance authority shall inform the relevant notified body \n",
      "accordingly. Article 18 of Regulation (EU) 2019/1020 shall apply to the measures \n",
      "referred to in the second subparagraph.  \n",
      "3. Where the market surveillance authority considers that non -compliance is not \n",
      "restricted to its national territory, it shall inform the Commission and the other \n",
      "Member States of the results of the evaluation and of the actions which it has \n",
      "required the operator to take.  \n",
      "4. The operator shall ensure that all appropriate corrective action is taken in respect of \n",
      "all the AI systems concerned that it has made available on the market throughout the \n",
      "Union.  \n",
      "5. Where the operator of an AI system does  not take adequate corrective action within \n",
      "the period referred to in paragraph 2, the market surveillance authority shall take all \n",
      "appropriate provisional measures to prohibit or restrict the AI system's being made \n",
      "available on its national market, to wit hdraw the product from that market or to recall \n",
      "it. That authority shall inform the Commission and the other Member States, without \n",
      "delay, of those measures.  \n",
      "6. The information referred to in paragraph 5 shall include all available details, in \n",
      "particular t he data necessary for the identification of the non -compliant AI system, \n",
      "the origin of the AI system, the nature of the non -compliance alleged and the risk \n",
      "involved, the nature and duration of the national measures taken and the arguments \n",
      "put forward by th e relevant operator. In particular, the market surveillance authorities \n",
      "shall indicate whether the non -compliance is due to one or more of the following:  \n",
      "(a) a failure of the AI system to meet requirements set out in Title III, Chapter 2;  \n",
      "(b) shortcomings  in the harmonised standards or common specifications referred to \n",
      "in Articles 40 and 41 conferring a presumption of conformity.  \n",
      "7. The market surveillance authorities of the Member States other than the market \n",
      "surveillance authority of the Member State ini tiating the procedure shall without \n",
      "delay inform the Commission and the other Member States of any measures adopted \n",
      "and of any additional information at their disposal relating to the non -compliance of \n",
      "the AI system concerned, and, in the event of disagree ment with the notified national \n",
      "measure, of their objections.  \n",
      "8. Where, within three months of receipt of the information referred to in paragraph 5, \n",
      "no objection has been raised by either a Member State or the Commission in respect \n",
      "of a provisional measure taken by a Member State, that measure shall be deemed \n",
      "justified . This is without prejudice to the procedural rights of the concerned operator \n",
      "in accordance with Article 18 of Regulation (EU) 2019/1020.  EN 79  EN 9. The market surveillance authorities of all Member States shall ensure that \n",
      "appropriate restrictive measures are t aken in respect of the product concerned, such \n",
      "as withdrawal of the product from their market, without delay.  \n",
      "Article 66  \n",
      "Union safeguard procedure  \n",
      "1. Where, within three months of receipt of the notification referred to in Article 65(5), \n",
      "objections are rai sed by a Member State against a measure taken by another Member \n",
      "State, or where the Commission considers the measure to be contrary to Union law, \n",
      "the Commission shall without delay enter into consultation with the relevant Member \n",
      "State and operator or oper ators and shall evaluate the national measure. On the basis \n",
      "of the results of that evaluation, the Commission shall decide whether the national \n",
      "measure is justified or not within 9 months from the notification referred to in Article \n",
      "65(5) and notify such d ecision to the Member State concerned.  \n",
      "2. If the national measure is considered justified, all Member States shall take the \n",
      "measures necessary to ensure that the non -compliant AI system is withdrawn from \n",
      "their market, and shall inform the Commission accor dingly. If the national measure \n",
      "is considered unjustified, the Member State concerned shall withdraw the measure.  \n",
      "3. Where the national measure is considered justified and the non -compliance of the AI \n",
      "system is attributed to shortcomings in the harmonised standards or common \n",
      "specifications referred to in Articles 40 and 41 of this Regulation, the Commission \n",
      "shall apply the procedure provided for in Article 11 of Regulation (EU) No \n",
      "1025/2012.  \n",
      "Article 67  \n",
      "Compliant AI systems which present a risk  \n",
      "1. Where, hav ing performed an evaluation under Article 65, the market surveillance \n",
      "authority of a Member State finds that although an AI system is in compliance with \n",
      "this Regulation, it presents a risk to the health or safety of persons, to the compliance \n",
      "with obligati ons under Union or national law intended to protect fundamental rights \n",
      "or to other aspects of public interest protection, it shall require the relevant operator \n",
      "to take all appropriate measures to ensure that the AI system concerned, when placed \n",
      "on the mar ket or put into service, no longer presents that risk, to withdraw the AI \n",
      "system from the market or to recall it within a reasonable period, commensurate with \n",
      "the nature of the risk, as it may prescribe.  \n",
      "2. The provider or other relevant operators shall en sure that corrective action is taken in \n",
      "respect of all the AI systems concerned that they have made available on the market \n",
      "throughout the Union within the timeline prescribed by the market surveillance \n",
      "authority of the Member State referred to in paragrap h 1. \n",
      "3. The Member State shall immediately inform the Commission and the other Member \n",
      "States. That information shall include all available details, in particular the data \n",
      "necessary for the identification of the AI system concerned, the origin and the suppl y \n",
      "chain of the AI system, the nature of the risk involved and the nature and duration of \n",
      "the national measures taken.  \n",
      "4. The Commission shall without delay enter into consultation with the Member States \n",
      "and the relevant operator and shall evaluate the nati onal measures taken. On the basis EN 80  EN of the results of that evaluation, the Commission shall decide whether the measure is \n",
      "justified or not and, where necessary, propose appropriate measures.  \n",
      "5. The Commission shall address its decision to the Member States.  \n",
      "Article 68  \n",
      "Formal non -compliance  \n",
      "1. Where the market surveillance authority of a Member State makes one of the \n",
      "following findings, it shall require the relevant provider to put an end to the non -\n",
      "compliance concerned:  \n",
      "(a) the conformity marking has been aff ixed in violation of Article 49;  \n",
      "(b) the conformity marking has not been affixed;  \n",
      "(c) the EU declaration of conformity has not been drawn up;  \n",
      "(d) the EU declaration of conformity has not been drawn up correctly;  \n",
      "(e) the identification number of the notifie d body, which is involved in the \n",
      "conformity assessment procedure, where applicable, has not been affixed;  \n",
      "2. Where the non -compliance referred to in paragraph 1 persists, the Member State \n",
      "concerned shall take all appropriate measures to restrict or prohibi t the high -risk AI \n",
      "system being made available on the market or ensure that it is recalled or withdrawn \n",
      "from the market.  \n",
      "TITLE  IX \n",
      "CODES  OF CONDUCT  \n",
      "Article 69  \n",
      "Codes of conduct  \n",
      "1. The Commission and the Member States shall encourage and facilitate the drawing \n",
      "up of codes of conduct intended to foster the voluntary application to AI systems \n",
      "other than high -risk AI systems of the requirements set out in Title III, Chapter 2 on \n",
      "the bas is of technical specifications and solutions that are appropriate means of \n",
      "ensuring compliance with such requirements in light of the intended purpose of the \n",
      "systems.  \n",
      "2. The Commission and the Board shall encourage and facilitate the drawing up of \n",
      "codes o f conduct intended to foster the voluntary application to AI systems of \n",
      "requirements related for example to environmental sustainability, accessibility for \n",
      "persons with a disability, stakeholders participation in the design and development of \n",
      "the AI system s and diversity of development teams on the basis of clear objectives \n",
      "and key performance indicators to measure the achievement of those objectives.  \n",
      "3. Codes of conduct may be drawn up by individual providers of AI systems or by \n",
      "organisations representing them or by both, including with the involvement of users \n",
      "and any interested stakeholders and their representative organisations. Codes of \n",
      "conduct may cover one or more AI systems taking into account the similarity of the \n",
      "intended purpose of the relevant sy stems.  EN 81  EN 4. The Commission and the Board shall take into account the specific interests and \n",
      "needs of the small -scale providers and start -ups when encouraging and facilitating \n",
      "the drawing up of codes of conduct.  \n",
      "TITLE  X \n",
      "CONFIDENTIALITY  AND  PENALTIES   \n",
      "Article 70 \n",
      "Confidentiality  \n",
      "1. National competent authorities and notified bodies involved in the application of this \n",
      "Regulation shall respect the confidentiality of information and data obtained in \n",
      "carrying out their tasks and activities in such a manner as to pro tect, in particular:  \n",
      "(a) intellectual property rights, and confidential business information or trade \n",
      "secrets of a natural or legal person, including source code, except the cases \n",
      "referred to in Article 5 of Directive 2016/943 on the protection of undisclosed \n",
      "know -how and business  information (trade secrets) against their unlawful \n",
      "acquisition, use and disclosure apply.  \n",
      "(b) the effective implementation of this Regulation, in particular for the purpose of \n",
      "inspections, investigations or audits; (c) public and national security interes ts;  \n",
      "(c) integrity of criminal or administrative  proceedings.  \n",
      "2. Without prejudice to paragraph 1, information exchanged on a confidential basis \n",
      "between the national competent authorities and between national competent \n",
      "authorities and the Commission shall not be disclosed without the prior consultation \n",
      "of the originating national competent authority and the user when high -risk AI \n",
      "systems referred to in points 1, 6 and 7 of Annex III are used by law enforcement, \n",
      "immigration or asylum authorities, when such d isclosure would jeopardise public and \n",
      "national security interests.  \n",
      "When the law enforcement, immigration or asylum authorities are providers of high -\n",
      "risk AI systems referred to in points 1, 6 and 7 of Annex III, the technical \n",
      "documentation referred to in A nnex IV shall remain within the premises of those \n",
      "authorities. Those authorities shall ensure that the market surveillance authorities \n",
      "referred to in Article 63(5) and (6), as applicable, can, upon request, immediately \n",
      "access the documentation or obtain a copy thereof. Only staff of the market \n",
      "surveillance authority holding the appropriate level of security clearance shall be \n",
      "allowed to access that documentation or any copy thereof.  \n",
      "3. Paragraphs 1 and 2 shall not affect the rights and obligations of the Co mmission, \n",
      "Member States and notified bodies with regard to the exchange of information and \n",
      "the dissemination of warnings, nor the obligations of the parties concerned to provide \n",
      "information under criminal law of the Member States.  \n",
      "4. The Commission and Mem ber States may exchange, where necessary, confidential \n",
      "information with regulatory authorities of third countries with which they have \n",
      "concluded bilateral or multilateral confidentiality arrangements guaranteeing an \n",
      "adequate level of confidentiality.  EN 82  EN Artic le 71  \n",
      "Penalties  \n",
      "1. In compliance with the terms and conditions laid down in this Regulation, Member \n",
      "States shall lay down the rules on penalties, including administrative fines, applicable \n",
      "to infringements of this Regulation and shall take all measures nec essary to ensure \n",
      "that they are properly and effectively implemented. The penalties provided for shall \n",
      "be effective, proportionate, and dissuasive. They shall take into particular account the \n",
      "interests of small -scale providers and start -up and their economi c viability.  \n",
      "2. The Member States shall notify the Commission of those rules and of those measures \n",
      "and shall notify it, without delay, of any subsequent amendment affecting them.  \n",
      "3. The following infringements shall be subject to administrative fines of u p to 30 000 \n",
      "000 EUR or, if the offender is company, up to 6 % of its total worldwide annual \n",
      "turnover for the  preceding financial year, whichever is higher:  \n",
      "(a) non-compliance with the prohibition of the artificial intelligence practices \n",
      "referred to in Arti cle 5;  \n",
      "(b) non-compliance of the AI system with the requirements laid down in Article \n",
      "10. \n",
      "4. The non -compliance of the AI system with any requirements or obligations under \n",
      "this Regulation, other than those laid down in Articles 5 and 10, shall be subject t o \n",
      "administrative fines of up to 20 000 000 EUR or, if the offender is a company, up to \n",
      "4 % of its total worldwide annual turnover for the preceding financial year, \n",
      "whichever is higher.  \n",
      "5. The supply of incorrect, incomplete or misleading information to not ified bodies and \n",
      "national competent authorities in reply to a request shall be subject to administrative \n",
      "fines of up to 10 000 000 EUR or, if the offender is a company, up to 2 % of its total \n",
      "worldwide annual turnover for the preceding financial year, whic hever is higher.  \n",
      "6. When deciding on the amount of the administrative fine in each individual case, all \n",
      "relevant circumstances of the specific situation shall be taken into account and due \n",
      "regard shall be given to the following:  \n",
      "(a) the nature, gravity and  duration of the infringement and of its consequences;  \n",
      "(b) whether administrative fines have been already applied by other market \n",
      "surveillance authorities to the same operator for the same infringement.  \n",
      "(c) the size and market share of the operator committing the infringement;  \n",
      "7. Each Member State shall lay down rules on whether and to what extent \n",
      "administrative fines may be imposed on public authorities and bodies established in \n",
      "that Member State.  \n",
      "8. Depending on the legal system of the Member State s, the rules on administrative \n",
      "fines may be applied in such a manner that the fines are imposed by competent \n",
      "national courts of other bodies as applicable in those Member States.  The application \n",
      "of such rules in those Member States shall have  an equivalent effect.  EN 83  EN Article 72  \n",
      "Administrative fines on Union institutions, agencies and  bodies  \n",
      "1. The European Data Protection Supervisor may impose administrative fines on Union \n",
      "institutions, agencies and bodies falling within the scope of this Regulat ion. When \n",
      "deciding whether to impose an administrative fine and deciding on the amount of the \n",
      "administrative fine in each individual case, all relevant circumstances of the specific \n",
      "situation shall be taken into account and due regard shall be given to the  following:  \n",
      "(a) the nature, gravity and duration of the infringement and of its consequences;  \n",
      "(b) the cooperation with the European Data Protection Supervisor in order to \n",
      "remedy the infringement and mitigate the possible adverse effects of the \n",
      "infringement , including compliance with any of the measures previously \n",
      "ordered by the European Data Protection Supervisor against the Union \n",
      "institution or agency or body concerned with regard to the same subject matter;  \n",
      "(c) any similar previous infringements by the Un ion institution, agency or body;  \n",
      "2. The following infringements shall be subject to administrative fines of up to 500 000 \n",
      "EUR:  \n",
      "(a) non-compliance with the prohibition of the artificial intelligence practices \n",
      "referred to in Article 5;  \n",
      "(b) non-compliance of the AI system with the requirements laid down in Article \n",
      "10. \n",
      "3. The non -compliance of the AI system with any requirements or obligations under \n",
      "this Regulation, other than those laid down in Articles 5 and 10, shall be subject to \n",
      "administrative fines of up to 250 000 EUR.  \n",
      "4. Before taking decisions pursuant to this Article, the European Data Protection \n",
      "Supervisor shall give the Union institution, agency or body  which is the subject of \n",
      "the proceedings conducted by the European Data Protection Supervisor the \n",
      "opportunity of being heard on the matter regarding the possible infringement. The \n",
      "European Data Protection Supervisor shall base his or her decisions only on elements \n",
      "and circumstances on which the parties concerned have been able to comment. \n",
      "Complainants, if any, shall be associated closely with the proceedings.  \n",
      "5. The rights of defense of the parties concerned shall be fully respected in the \n",
      "proceedings. They shall be entitled to have access to the European Data Protection \n",
      "Supervisor’s file, subject to the  legitimate interest of individuals or undertakings in \n",
      "the protection of their personal data or business secrets.  \n",
      "6. Funds collected by imposition of fines in this Article shall be the income of the \n",
      "general budget of the Union.  \n",
      "TITLE  XI \n",
      "DELEGATION  OF POWER  AND  COMMITTEE  PROCEDURE   \n",
      "Article 73  \n",
      "Exercise of the delegation  \n",
      "1. The power to adopt delegated acts is conferred on the Commission subject to the \n",
      "conditions laid down in this Article.  EN 84  EN 2. The delegation of power referred to in Article 4, Article 7(1), Arti cle 11(3), Article \n",
      "43(5) and (6) and Article 48(5) shall be conferred on the Commission for an \n",
      "indeterminate period of time from [ entering into force of the Regulation ]. \n",
      "3. The delegation of power referred to in Article 4, Article 7(1), Article 11(3), Arti cle \n",
      "43(5) and (6) and Article 48(5) may be revoked at any time by the European \n",
      "Parliament or by the Council. A decision of revocation shall put an end to the \n",
      "delegation of power specified in that decision. It shall take effect the day following \n",
      "that of its  publication in the Official Journal of the European Union  or at a later date \n",
      "specified therein. It shall not affect the validity of any delegated acts already in force.  \n",
      "4. As soon as it adopts a delegated act, the Commission shall notify it simultaneously  to \n",
      "the European Parliament and to the Council.  \n",
      "5. Any delegated act adopted pursuant to Article 4, Article 7(1), Article 11(3), Article \n",
      "43(5) and (6) and Article 48(5) shall enter into force only if no objection has been \n",
      "expressed by either the European P arliament or the Council within a period of three \n",
      "months of notification of that act to the European Parliament and the Council or if, \n",
      "before the expiry of that period, the European Parliament and the Council have both \n",
      "informed the Commission that they wil l not object. That period shall be extended by \n",
      "three months at the initiative of the European Parliament or of the Council.  \n",
      "Article 74  \n",
      "Committee procedure  \n",
      "1. The Commission shall be assisted by a committee. That committee shall be a \n",
      "committee within the me aning of Regulation (EU) No 182/2011.  \n",
      "2. Where reference is made to this paragraph, Article 5 of Regulation (EU) No \n",
      "182/2011 shall apply.  \n",
      "TITLE  XII \n",
      "FINAL  PROVISIONS   \n",
      "Article 75  \n",
      "Amendment to Regulation (EC) No 300/2008  \n",
      "In Article 4(3) of Regulation (EC) No 300/2008, the following subparagraph is added:  \n",
      "“When adopting detailed measures related to technical specifications and procedures for \n",
      "approval and use of security equipment concerning Artificial Intelligence systems in the \n",
      "meaning of Regulation (EU) YYY/X X [on Artificial Intelligence]  of the European Parliament \n",
      "and of the Council* , the requirements set out in Chapter 2, Title III of that Regulation shall be \n",
      "taken into account.”  \n",
      "__________  \n",
      "* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”  \n",
      "Artic le 76  \n",
      "Amendment to Regulation (EU) No 167/2013  \n",
      "In Article 17(5) of Regulation (EU) No 167/2013, the following subparagraph is added:  EN 85  EN “When adopting delegated acts pursuant to the first subparagraph concerning artificial \n",
      "intelligence systems which are safet y components in the meaning of Regulation (EU) \n",
      "YYY/XX [on Artificial Intelligence]  of the European Parliament and of the Council*, the \n",
      "requirements set out in Title III, Chapter 2 of that Regulation  shall be taken into account.  \n",
      "__________  \n",
      "* Regulation (EU)  YYY/XX [on Artificial Intelligence] (OJ …).”  \n",
      "Article 77  \n",
      "Amendment to Regulation (EU) No 168/2013  \n",
      "In Article 22(5) of Regulation (EU) No 168/2013, the following subparagraph is added:  \n",
      "“When adopting delegated acts pursuant to the first subparagraph concern ing Artificial \n",
      "Intelligence systems which are safety components in the meaning of Regulation (EU) \n",
      "YYY/XX on [Artificial Intelligence]  of the European Parliament and of the Council*, the \n",
      "requirements set out in Title III, Chapter 2 of that Regulation  shall be taken into account.  \n",
      "__________  \n",
      "* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”  \n",
      "Article 78  \n",
      "Amendment to Directive 2014/90/EU  \n",
      "In Article 8 of Directive 2014/90/EU, the following paragraph is added:  \n",
      "“4. For Artificial Intelligence systems  which are safety components in the meaning of \n",
      "Regulation (EU) YYY/XX [on Artificial Intelligence]  of the European Parliament and of the \n",
      "Council*, when carrying out its activities pursuant to paragraph 1 and when adopting \n",
      "technical specifications and testing standards in accordance with paragraphs 2 and 3, the \n",
      "Commission shall take into account the requirements set out in Title III, Chapter 2 of that \n",
      "Regulation.  \n",
      "__________  \n",
      "* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.  \n",
      "Article 79  \n",
      "Amendment to Directive (EU) 2016/797  \n",
      "In Article 5 of Directive (EU) 2016/797, the following paragraph is added:  \n",
      "“12. When adopting delegated acts pursuant to paragraph 1 and implementing acts pursuant to \n",
      "paragraph 11 concerning Artificial Intelligence systems  which are  safety components in the \n",
      "meaning of Regulation (EU) YYY/XX [on Artificial Intelligence]  of the European Parliament \n",
      "and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation  shall be \n",
      "taken into account.  \n",
      "__________  \n",
      "* Regulatio n (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.  EN 86  EN Article 80  \n",
      "Amendment to Regulation (EU) 2018/858  \n",
      "In Article 5 of Regulation (EU) 2018/858 the following paragraph is added:  \n",
      "“4. When adopting delegated acts pursuant to paragraph 3 concerning Artificial Intelligence \n",
      "systems which are safety components in the meaning of Regulation (EU) YYY/XX [on \n",
      "Artificial Intelligence]  of the European Parliament and of the Council *, the requirements set \n",
      "out in Title III, Chapter 2 of that Regulation  shall be taken into account.  \n",
      "__________  \n",
      "* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.  \n",
      "Article 81  \n",
      "Amendment to Regulation (EU) 2018/1139  \n",
      "Regulation (EU) 2018/1139 is amended as follows:  \n",
      "(1) In Article 17, the following paragraph is added:  \n",
      "“3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph \n",
      "1 concerning Artificial Intelligence systems which are safety components in the meaning of \n",
      "Regulation (EU) YYY/XX [ on Artificial Intelligence ] of the European Parl iament and of the \n",
      "Council*, the requirements set out in Title III, Chapter 2 of that Regulation  shall be taken into \n",
      "account.  \n",
      "__________  \n",
      "* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”  \n",
      "(2) In Article 19, the following paragraph is added:  \n",
      "“4. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \n",
      "Intelligence systems which are safety components in the meaning of Regulation (EU) \n",
      "YYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of th at \n",
      "Regulation  shall be taken into account.”  \n",
      "(3) In Article 43, the following paragraph is added:  \n",
      "“4. When adopting implementing acts pursuant to paragraph 1 concerning Artificial \n",
      "Intelligence systems which are safety components in the meaning of Regulation  (EU) \n",
      "YYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that \n",
      "Regulation  shall be taken into account.”  \n",
      "(4) In Article 47, the following paragraph is added:  \n",
      "“3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \n",
      "Intelligence systems which are safety components in the meaning of Regulation (EU) \n",
      "YYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that \n",
      "Regulation  shall be taken into account.”  \n",
      "(5) In Article  57, the following paragraph is added:  \n",
      "“When adopting those implementing acts concerning Artificial Intelligence systems which are \n",
      "safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence], \n",
      "the requirements set out in Title II I, Chapter 2 of that Regulation  shall be taken into account.”  \n",
      "(6) In Article 58, the following paragraph is added:  EN 87  EN “3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \n",
      "Intelligence systems which are safety components in the  meaning of Regulation (EU) \n",
      "YYY/XX [on Artificial Intelligence] , the requirements set out in Title III, Chapter 2 of that \n",
      "Regulation  shall be taken into account.”.  \n",
      "Article 82  \n",
      "Amendment to Regulation (EU) 2019/2144  \n",
      "In Article 11 of Regulation (EU) 2019/214 4, the following paragraph is added:  \n",
      "“3. When adopting the implementing acts pursuant to paragraph 2,  concerning artificial \n",
      "intelligence systems which are safety components in the meaning of Regulation (EU) \n",
      "YYY/XX [on Artificial Intelligence] of the Europe an Parliament and of the Council*, the \n",
      "requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.  \n",
      "__________  \n",
      "* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.  \n",
      "Article 83  \n",
      "AI systems already placed on the marke t or put into service  \n",
      "1. This Regulation shall not apply to the AI systems which are components of the large -\n",
      "scale IT systems established by the legal acts listed in Annex IX that have been \n",
      "placed on the market or put into service before [12 months after t he date of \n",
      "application of this Regulation referred to in Article 85(2) ], unless the replacement or \n",
      "amendment of those legal acts leads to a significant change in the design or intended \n",
      "purpose of the AI system or AI systems concerned.  \n",
      "The requirements laid  down in this Regulation shall be taken into account, where \n",
      "applicable, in the evaluation of each large -scale IT systems established by the legal \n",
      "acts listed in Annex IX to be undertaken as provided for in those respective acts.  \n",
      "2. This Regulation shall ap ply to the high -risk AI systems, other than the ones referred \n",
      "to in paragraph 1, that have been placed on the market or put into service before \n",
      "[date of application of this Regulation referred to in Article 85(2) ], only if, from that \n",
      "date, those systems are subject to significant changes in their design or intended \n",
      "purpose.  \n",
      "Article 84  \n",
      "Evaluation and review  \n",
      "1. The Commission shall assess the need for amendment of the list in Annex III once a \n",
      "year following the entry into force of this Regulation.  \n",
      "2. By [three years after the date of application of this Regulation referred to in Article \n",
      "85(2) ] and every four years thereafter, the Commission shall submit a report on the \n",
      "evaluation and review of this Regulation to the European Parliament and to the \n",
      "Council. The reports shall be made public.   \n",
      "3. The reports referred to in paragraph 2 shall devote specific attention to the following:  \n",
      "(a) the status of the financial and human resources of the national competent \n",
      "authorities in order to effectively perform the ta sks assigned to them under this \n",
      "Regulation;  EN 88  EN (b) the state of penalties, and notably administrative fines as referred to in Article \n",
      "71(1), applied by Member States to infringements of the provisions of this \n",
      "Regulation.  \n",
      "4. Within [ three years after the date of application of this Regulation referred to in \n",
      "Article 85(2) ] and every four years thereafter, the Commission shall evaluate the \n",
      "impact and effectiveness of codes of conduct to foster the application of the \n",
      "requirements set out in Title III, Chapter 2 and possibly other additional requirements \n",
      "for AI systems other than high -risk AI systems.  \n",
      "5. For the purpose of paragraphs 1 to 4 the Board, the Member States and national \n",
      "competent authorities shall provide the Commission with information on its request.  \n",
      "6. In carrying out the evaluations and reviews referred to in paragraphs 1 to 4 the \n",
      "Commission shall take into account the positions and findings of the Board, of the \n",
      "European Parliament, of the Council, and of other relevant bodies or sources.  \n",
      "7. The Commi ssion shall, if necessary, submit appropriate proposals to amend this \n",
      "Regulation, in particular taking into account developments in technology and in the \n",
      "light of the state of progress in the information society.  \n",
      "Article 85  \n",
      "Entry into force and application  \n",
      "1. This Regulation shall enter into force on the twentieth day following that of its \n",
      "publication in the Official Journal of the European Union . \n",
      "2. This Regulation shall apply from [24 months following the entering into force of the \n",
      "Regulation].  \n",
      "3. By way of derogation from  paragraph 2:  \n",
      "(a) Title III, Chapter 4  and Title VI  shall apply from [three months following the \n",
      "entry into force of this Regulation];  \n",
      "(b) Article 71 shall apply from [twelve months following the entry into force of \n",
      "this Regulation].  \n",
      "This Regulation shall be binding in its entirety and directly applicable in all Member States.  \n",
      "Done at Brussels,  \n",
      "For the European Parliament  For the Council  \n",
      "The President  The President  EN 89  EN LEGISLATIVE FINANCIAL STATEMENT  \n",
      "1. FRAMEWORK  OF THE  PROPOSAL/INITIATIVE   \n",
      " 1.1.  Title of the proposal/initiative  \n",
      " 1.2.  Policy area(s) concerned  \n",
      " 1.3.  The proposal/initiative relates to:  \n",
      " 1.4.  Objective(s)  \n",
      "1.4.1.   General objective(s)  \n",
      " 1.4.2.  Specific objective(s)  \n",
      "1.4.3.  Expected result(s) and impact  \n",
      "1.4.4.  Indicators of performance  \n",
      " 1.5.  Grounds for the proposal/initiative  \n",
      " 1.5.1.   Requirement(s) to be met in the short or long term including a detailed \n",
      "timeline for roll -out of the implementation of the initiative  \n",
      "1.5.2.  Added v alue of Union involvement (it may result from different factors, e.g. \n",
      "coordination gains, legal certainty, greater effectiveness or complementarities). For \n",
      "the purposes of this point 'added value of Union involvement' is the value resulting \n",
      "from Union inte rvention which is additional to the value that would have been \n",
      "otherwise created by Member States alone  \n",
      "1.5.3.   Lessons learned from similar experiences in the past  \n",
      "1.5.4. Compatibility with the Multiannual Financial Framework and possible \n",
      "synergies with other appropriate instruments  \n",
      "1.5.5   Assessment of the different available financing options, including scope for \n",
      "redeployment  \n",
      "1.6.      Duration and financial impact of the proposal/initiative  \n",
      " 1.7.   Management mode(s) planned  \n",
      "2. MANAGEMENT  MEASURES   \n",
      " 2.1.  Monitoring and reporting rules  \n",
      " 2.2.  Management and control system  \n",
      "2.2.1. Justification of the management mode(s), the funding implementation \n",
      "mechanism(s), the payment modalities and the control strategy proposed  \n",
      "2.2.2. Information concerning the risks identified and the internal control system(s) \n",
      "set up to mitigate them  \n",
      "2.2.3. Estimation and justification of the cost -effectiveness of the controls (ratio of \n",
      "\"control costs ÷ value of the related funds managed\"), and assessment of the \n",
      "expected levels  of risk of error (at payment & at closure)  EN 90  EN  2.3. Measures to prevent fraud and irregularities  \n",
      "3. ESTIMATED  FINANCIAL  IMPACT  OF THE  PROPOSAL/INITIATIVE   \n",
      " 3.1. Heading(s) of the multiannual financial framework and expenditure budget \n",
      "line(s) affected  \n",
      " 3.2. Estimated financial impact of the proposal on appropriations   \n",
      " 3.2.1.  Summary of estimated impact on operational appropriations  \n",
      " 3.2.2.  Estimated output funded with operational appropriations  \n",
      " 3.2.3. Summary of estimated impact on administrative appropri ations  \n",
      " 3.2.4.  Compatibility with the current multiannual financial framework  \n",
      " 3.2.5.  Third -party contributions  \n",
      " 3.3. Estimated impact on revenueEN 91  EN LEGISLATIVE FINANCIAL STATEMENT  \n",
      "1. FRAMEWORK  OF THE  PROPOSAL/INITIATIVE   \n",
      "1.1. Title of the proposal/initiative  \n",
      "Regulation of the European Parliament and of the Council Laying Down Harmonised \n",
      "Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain \n",
      "Union Legislative Acts  \n",
      "1.2. Policy area(s) concerned  \n",
      "Communications Networks, Content and Technology;  \n",
      "Internal Market, Industry, Entrepreneurship and SMEs;  \n",
      "The budgetary impact concerns the new tasks entrusted with the Commission, \n",
      "including the support to the EU AI Board;  \n",
      "Activity: Shaping Europe's digital future.  \n",
      "1.3. The proposal/initiative relates to:  \n",
      "X  a new action   \n",
      " a new action following a pilot project/preparatory action64  \n",
      " the extension of an existing action   \n",
      " an action redirected towards a new action   \n",
      "1.4. Objective(s)  \n",
      "1.4.1.  General objective( s)  \n",
      "The general objective of the intervention is to ensure the proper functioning of the \n",
      "single market by creating the conditions for the development and use of trustworthy \n",
      "artificial intelligence in the Union.  \n",
      "1.4.2.  Specific objective(s)  \n",
      "Specific objective No 1  \n",
      "To set requirements specific to AI systems and obligations on all value chain \n",
      "participants in order to ensure that AI systems placed on the market and used are safe \n",
      "and respect existing law on fundamental rights and Union values;  \n",
      "Specific ob jective No 2  \n",
      "To ensure legal certainty to facilitate investment and innovation in AI by making it \n",
      "clear what essential requirements, obligations, as well as conformity and compliance \n",
      "procedures must be followed to place or use an AI system in the Union mar ket; \n",
      "Specific objective No 3  \n",
      "To enhance governance and effective enforcement of existing law on fundamental \n",
      "rights and safety requirements applicable to AI systems by providing new powers, \n",
      "resources and clear rules for relevant authorities on conformity as sessment and ex \n",
      "                                                 \n",
      "64 As referred to in Article 54(2)(a) or (b) of the Financial  Regulation   EN 92  EN post monitoring procedures and the division of governance and supervision tasks \n",
      "between national and EU levels;  \n",
      "Specific objective No 4  \n",
      "To facilitate the development of a single market for lawful, safe and trustworthy AI \n",
      "applications and prevent market fragmentation by taking EU action to set minimum \n",
      "requirement for AI systems to be placed and used in the Union market in compliance \n",
      "with existing law on fundamental rights and safety.  EN 93  EN 1.4.3.  Expected result(s) and impact  \n",
      "Specify the effects  which the proposal/initiative should have on the beneficiaries/groups targeted.  \n",
      "AI suppliers should benefit from a minimal but clear set of requirements, creating \n",
      "legal certainty and ensuring access to the entire single market.  \n",
      "AI users should benefit fro m legal certainty that the high -risk AI systems they buy \n",
      "comply with European laws and values.  \n",
      "Consumers should benefit by reducing the risk of violations of their safety or \n",
      "fundamental rights.  \n",
      "1.4.4.  Indicators of performance  \n",
      "Specify the indicators for mo nitoring implementation of the proposal/initiative.  \n",
      "Indicator 1  \n",
      "Number of serious incidents or AI performances which constitute a serious incident \n",
      "or a breach of fundamental rights obligations (semi -annual) by fields of applications \n",
      "and calculated a) in ab solute terms, b) as share of applications deployed and c) as \n",
      "share of citizens concerned.  \n",
      "Indicator 2  \n",
      "a) Total AI investment in the EU (annual)  \n",
      "b) Total AI investment by Member State (annual)  \n",
      "c) Share of companies using AI (annual)  \n",
      "d) Share of SMEs using AI (annual)  \n",
      "a) and b) will be calculated based on official sources and benchmarked against \n",
      "private estimates  \n",
      "c) and d) will be collected by regular company surveys  \n",
      "1.5. Grounds for the proposal/initiative  \n",
      "1.5.1.  Requirement(s) to be met in the short or lo ng term including a detailed timeline for \n",
      "roll-out of the implementation of the initiative  \n",
      "The Regulation should be fully applicable one year and a half after its adoption. \n",
      "However, elements of the governance structure should be in place before then. In \n",
      "particular, Member States shall have appointed existing authorities and/or established \n",
      "new authorities performing the tasks set out in the legislation earlier, and the EU AI \n",
      "Board should be set -up and effective. By the time of applicability, the European \n",
      "database of AI systems should be fully operative. In parallel to the adoption process, \n",
      "it is therefore necessary to develop the database, so that its development has come to \n",
      "an end when the regulation enters into force.  \n",
      "1.5.2.  Added value of Union involvement  (it may result from different factors, e.g. \n",
      "coordination gains, legal certainty, greater effectiveness or complementarities). For \n",
      "the purposes of this point 'added value of Union involvement' is the value resulting \n",
      "from Union intervention which is additio nal to the value that would have been \n",
      "otherwise created by Member States alone.  \n",
      "An emerging patchy framework of potentially divergent national rules will hamper \n",
      "the seamless provision of AI systems across the EU and is ineffective in ensuring the EN 94  EN safety a nd protection of fundamental rights and Union values across the different \n",
      "Member States. A common EU legislative action on AI could boost the internal \n",
      "market and has great potential to provide European industry with a competitive edge \n",
      "at the global scene a nd economies of scale that cannot be achieved by individual \n",
      "Member States alone.  \n",
      "1.5.3.  Lessons learned from similar experiences in the past  \n",
      "The E -commerce Directive 2000/31/EC provides the core framework for the \n",
      "functioning of the single market and the su pervision of digital services and sets a \n",
      "basic structure for a general cooperation mechanism among Member States, covering \n",
      "in principle all requirements applicable to digital services. The evaluation of the \n",
      "Directive pointed to shortcomings in several aspe cts of this cooperation mechanism, \n",
      "including important procedural aspects such as the lack of clear timeframes for \n",
      "response from Member States coupled with a general lack of responsiveness to \n",
      "requests from their counterparts. This has led over the years to  a lack of trust \n",
      "between Member States in addressing concerns about providers offering digital \n",
      "services cross -border. The evaluation of the Directive showed the need to define a \n",
      "differentiated set of rules and requirements at European level. For this reaso n, the \n",
      "implementation of the specific obligations laid down in this Regulation would \n",
      "require a specific cooperation mechanism at EU level, with a governance structure \n",
      "ensuring coordination of specific responsible bodies at EU level.  \n",
      "1.5.4.  Compatibility wi th the Multiannual Financial Framework and possible synergies \n",
      "with other appropriate instruments  \n",
      "The Regulation Laying Down Harmonised Rules on Artificial Intelligence and \n",
      "Amending Certain Union Legislative Acts defines a new common framework of \n",
      "requiremen ts applicable to AI systems, which goes well beyond the framework \n",
      "provided by existing legislation. For this reason, a new national and European \n",
      "regulatory and coordination function needs to be established with this proposal.  \n",
      "As regards possible synergies with other appropriate instruments, the role of \n",
      "notifying authorities at national level can be performed by national authorities \n",
      "fulfilling similar functions sunder other EU regulations.  \n",
      "Moreover, by increasing trust in AI and thus encouraging investment i n development \n",
      "and adoption of AI, it complements Digital Europe, for which promoting the \n",
      "diffusion of AI is one of five priorities.  \n",
      "1.5.5.  Assessment of the different available financing options, including scope for \n",
      "redeployment  \n",
      "The staff will be redeploye d. The other costs will be supported from the DEP. \n",
      "envelope, given that the objective of this regulation – ensuring trustworthy AI – \n",
      "contributes directly to one key objective of Digital Europe – accelerating AI \n",
      "development and deployment in Europe.  EN 95  EN 1.6. Duration and financial impact of the proposal/initiative  \n",
      " limited duration  \n",
      "–  in effect from [DD/MM]YYYY to [DD/MM]YYYY  \n",
      "–  Financial impact from YYYY to YYYY for commitment appropriations and \n",
      "from YYYY to YYYY for payment appropriations.  \n",
      "X unlimited durat ion \n",
      "– Implementation with a start -up period from one/two (tbc) year,  \n",
      "– followed by full -scale operation.  \n",
      "1.7. Management mode(s) planned65  \n",
      "X Direct management  by the Commission  \n",
      "–  by its departments, including by its staff in the Union delegations;  \n",
      "–  by the executive agencies  \n",
      " Shared management  with the Member States  \n",
      " Indirect management  by entrusting budget implementation tasks to:  \n",
      "–  third countries or the bodies they have designated;  \n",
      "–  international organisations and their agencies (to be specifi ed); \n",
      "–  the EIB and the European Investment Fund;  \n",
      "–  bodies referred to in Articles 70 and 71 of the Financial Regulation;  \n",
      "–  public law bodies;  \n",
      "–  bodies governed by private law with a public service mission to the extent that \n",
      "they provide adequate financial guarantees;  \n",
      "–  bodies governed by the private law of a Member State that are entrusted with \n",
      "the implementation of a public -private partnership and that provide adequate \n",
      "financial guarantees;  \n",
      "–  persons entrusted with the implementation of specific actions in  the CFSP \n",
      "pursuant to Title V of the TEU, and identified in the relevant basic act.  \n",
      "– If more than one management mode is indicated, please provide details in the ‘Comments’ section.  \n",
      "Comments  \n",
      " \n",
      " \n",
      "                                                 \n",
      "65 Details of management modes and references to the Financial Regulation may be found on the \n",
      "BudgWeb site: http://www.cc.cec/budg/man/budgmanag/budgmanag_en.html  EN 96  EN 2. MANAGEMENT  MEASURES   \n",
      "2.1. Monitoring and reporting rules  \n",
      "Specify frequency and conditions.  \n",
      "The Regulation will be reviewed and evaluated five years from the entry into force of \n",
      "the regulation. The Commission will report on the findings of the evaluation to the \n",
      "European Parliament, the Council and the European Economic and Social \n",
      "Committee.  \n",
      "2.2. Management and control system(s)  \n",
      "2.2.1.  Justification of the management mode(s), the funding implementation mechanism(s), \n",
      "the payment modalities and the control strategy proposed  \n",
      "The Regulation establishes a new policy with regard to harmonised rules for the \n",
      "provision of artificial intelligence systems in the internal market while ensuring the \n",
      "respect of safety and fundamental rights. These new rules require a consistency \n",
      "mechanism for the cross -border application of the obligations under this Regulation \n",
      "in the form of a new advisory group coordinating the activities of national \n",
      "authorities.  \n",
      "In order to face these new tasks, it is necessary to appropriately resource the \n",
      "Commission’s services. The enforcement of the new Regulation is estimated to \n",
      "require 10 FTE à regime (5 FTE for the support to the activities of the Board and 5 \n",
      "FTE for the European Data Protection Supervisor acting as a notifying body for AI \n",
      "systems deployed by a body of the European Union).  \n",
      "2.2.2.  Information concerning the risks identified  and the internal control system(s) set up \n",
      "to mitigate them  \n",
      "In order to ensure that the members of the Board have the possibility to make \n",
      "informed analysis on the basis of factual evidence, it is foreseen that the Board \n",
      "should be supported by the administr ative structure of the Commission and that an \n",
      "expert group be created to provide additional expertise where required.  \n",
      "2.2.3.  Estimate and justification of the cost -effectiveness of the controls (ratio of \"control \n",
      "costs ÷ value of the related funds managed\" ), and assessment of the expected levels \n",
      "of risk of error (at payment & at closure)  \n",
      "For the meeting expenditure, given the low value per transaction (e.g. refunding \n",
      "travel costs for a delegate for a meeting), standard control procedures seem \n",
      "sufficient. Re garding the development of the database, contract attribution has a \n",
      "strong internal control system in place in DG CNECT through centralised \n",
      "procurement activities.  \n",
      "2.3. Measures to prevent fraud and irregularities  \n",
      "Specify existing or envisaged prevention and protection measures, e.g. from the Anti -Fraud Strategy.  \n",
      "The existing fraud prevention measures applicable to the Commission will cover the \n",
      "additional appropriations necessary for this Regulation.   \n",
      "EN 97  EN 3. ESTIMATED  FINANCIAL  IMPACT  OF THE  PROPOSAL/INITIATIVE   \n",
      "3.1. Heading(s) of the multiannual financial framework and expenditure budget line(s) affected  \n",
      " Existing budget lines  \n",
      "In order  of multiannual financial framework headings and budget lines.  \n",
      "Heading of \n",
      "multiannual \n",
      "financial \n",
      "framewo rk Budget line  Type of  \n",
      "expenditure  Contribution  \n",
      "Number  \n",
      " Diff./Non -\n",
      "diff.66 from \n",
      "EFTA \n",
      "countries\n",
      "67 \n",
      " from \n",
      "candidate \n",
      "countries68 \n",
      " from third \n",
      "countries  within the \n",
      "meaning of \n",
      "Article  21(2)(b) of \n",
      "the Financial \n",
      "Regulation  \n",
      "7 20 02 06 Administrative expenditure  Non-diff. NO NO NO NO \n",
      "1 02 04 03 DEP Artificial Intelligence  Diff.  YES  NO NO NO \n",
      "1 02 01 30 01 Support expenditure for \n",
      "the Digital Europe programme  Non-diff. YES  NO NO NO \n",
      "3.2. Estimated financial impact of the proposal on appropriations  \n",
      "3.2.1.  Summary of  estimated impact on expenditure on operational appropriations  \n",
      "–  The proposal/initiative does not require the use of operational appropriations  \n",
      "– X The proposal/initiative requires the use of operational appropriations, as explained below:  \n",
      "EUR million (to t hree decimal places)  \n",
      "                                                 \n",
      "66 Diff. = Differentiated appropriations / Non -diff. = Non -differentiated appropriations.  \n",
      "67 EFTA: European Free Trade Association.  \n",
      "68 Candidate countries and, where applicable, potential candidate countries from the Western Balkans.   \n",
      "EN 98  EN Heading of multiannual financial  \n",
      "framework  1  \n",
      " \n",
      "DG: CNECT     Year \n",
      "2022  Year \n",
      "2023  Year \n",
      "2024  Year \n",
      "2025  Year \n",
      "2026  Year \n",
      "202769 TOTAL  \n",
      " Operational appropriations          \n",
      "Budget line70 02 04 03  Commitments  (1a)  1.000       1.000  \n",
      "Payments  (2a)  0.600  0.100  0.100  0.100  0.100   1.000  \n",
      "Budget line  Commitments  (1b)         \n",
      "Payments  (2b)         \n",
      "Appropriations of an administrative nature financed from the envelope of specific \n",
      "programmes71  \n",
      "         \n",
      "Budget line 02 01 30 01   (3)  0.240  0.240  0.240  0.240  0.240   1.200  \n",
      "TOTAL appropriations  \n",
      "for DG CNECT  Commitments  =1a+1b +3   1.240   0.240  0.240  0.240   2.200  \n",
      " Payments  =2a+2b  \n",
      "+3  0.840  0.340  0.340  0.340  0.340   2.200  \n",
      " \n",
      " \n",
      " \n",
      "                                                 \n",
      "69 Indicative and dependent on budg et availability.  \n",
      "70 According to the official budget nomenclature.  \n",
      "71 Technical and/or administrative assistance and expenditure in support of the implementation of EU programmes and/or actions ( former ‘BA’ lines), indirect research, \n",
      "direct research.   \n",
      "EN 99  EN  TOTAL operational appropriations  Commitments  (4)  1.000       1.000  \n",
      "Payments  (5)  0.600  0.100  0.100  0.100  0.100   1.000  \n",
      " TOTAL appropriations of an administrative nature financed from the \n",
      "envelope for specific programmes  (6)  0.240  0.240  0.240  0.240  0.240   1.200  \n",
      "TOTAL appropriations  \n",
      "under HEADING 1  \n",
      "of the multiannual  financial framework  Commitments  =4+ 6   1.240  0.240  0.240  .0.240  0.240   2.200  \n",
      "Payments  =5+ 6   0.840  0.340  0.340  0.340  0.340   2.200  \n",
      "If more than one heading is affected by the proposal / initiative, repeat the section above:  \n",
      " TOTAL operational appropriations (all \n",
      "operational headings)  Commitments  (4)         \n",
      "Payments  (5)         \n",
      " TOTAL appropriations of an administrative nature \n",
      "financed from the envelope for specific programmes (all \n",
      "operational headings)  (6)         \n",
      "TOTAL appropriations  \n",
      "under HEADINGS 1 to 6  \n",
      "of the multiannual financial framework  \n",
      "(Reference amount)  Commitments  =4+ 6          \n",
      "Payments  =5+ 6          \n",
      " \n",
      "   \n",
      "EN 100  EN Heading of multiannual financial  \n",
      "framework  7 ‘Administrative expenditure’  \n",
      "This section should be filled in using the 'budget data of an administrative nature' to be firstly introduced in the Annex  to the Legislative \n",
      "Financial Statement  (Annex V to the internal rules), which is uploaded to DECIDE for interservice consultation purposes.  \n",
      "EUR million (to three decimal places)  \n",
      "   Year \n",
      "2023  Year \n",
      "2024  Year \n",
      "2025  Year \n",
      "2026  Year 2027  After \n",
      "202772 TOTAL  \n",
      "DG: CNECT  \n",
      " Human resources  0.760  0.760  0.760  0.760  0.760  0.760  3.800  \n",
      " Other administrative expenditure  0.010  0.010  0.010  0.010  0.010  0.010  0.050  \n",
      "TOTAL DG CNECT  Appropriations  0.760  0.760  0.760  0.760  0.760  0.760  3.850  \n",
      "European Data Protection Supervisor   \n",
      " Human resources  0.760  0.760  0.760  0.760  0.760  0.760  3.800  \n",
      " Other administrative expenditure         \n",
      "TOTAL EDPS  Appropriations  0.760  0.760  0.760  0.760  0.760  0.760  3.800  \n",
      "TOTAL appropriations  \n",
      "under HEADING  7 \n",
      "of the multiannual financial framework   (Total commitments = Total payments)  1.530  1.530  1.530  1.530  1.530  1.530  7.650  \n",
      "EUR million (to three decimal places)  \n",
      "   Year \n",
      "2022  Year \n",
      "2023  Year \n",
      "2024  Year \n",
      "2025  Year 2026  Year 2027   TOTAL  \n",
      "TOTAL appropriations  Commitments   2.770  1.770  1.770  1.770  1.770   9.850  \n",
      "                                                 \n",
      "72 All figures in this column are indicative and subject to the continuation of the programmes and availability of appropriations   \n",
      "EN 101  EN under HEADINGS 1 to 7  \n",
      "of the multiannual financial framework   Payments   2.370  1.870  1.870  1.870  1.870  9.850   \n",
      "EN 102  EN 3.2.2.  Estimated output funded with operational appropriations  \n",
      "Commitment appropriations in EUR million (to three decimal places)  \n",
      "Indicate objectives \n",
      "and outputs  \n",
      " \n",
      "   Year \n",
      "2022  Year \n",
      "2023  Year \n",
      "2024  Year \n",
      "2025  Year \n",
      "2026  Year \n",
      "2027  After \n",
      "202773 TOTAL  \n",
      " OUTPUTS  \n",
      " Type  \n",
      " Average \n",
      "cost \n",
      "No \n",
      "Cost \n",
      "No \n",
      "Cost \n",
      "No \n",
      "Cost \n",
      "No \n",
      "Cost \n",
      "No \n",
      "Cost \n",
      "No \n",
      "Cost \n",
      "No \n",
      "Cost Tota\n",
      "l No Total \n",
      "cost \n",
      "SPECIFIC OBJECTIVE No 174…                 \n",
      "Database      1 1.000  1  1  1  1  1 0.100  1 1.000  \n",
      "Meetings - Output      10 0.200  10 0.200  10 0.200  10 0.200  10 0.200  10 0.200  50 1.000  \n",
      "Communication \n",
      "activities      2 0.040  2 0.040  2 0.040  2 0.040  2 0.040  2 0.040  10 0.040  \n",
      "Subtotal for specific objective No 1                  \n",
      "SPECIFIC OBJECTIVE No 2 ...                  \n",
      "- Output                    \n",
      "Subtotal for specific objective No 2                  \n",
      "TOTALS    13 0.240  13 0.240  13 0.240  13 0.240  13 0.240  13 0.100  65 2.200  \n",
      "                                                 \n",
      "73 All figures in this column are indicative and subject to the continuation of the programmes and availability of appropriation s \n",
      "74 As described in point 1.4.2. ‘Specific objective(s)…’   \n",
      "EN 103  EN 3.2.3.  Summary of estimated impact on administrative appropriations   \n",
      "–  The proposal/initiative does not require the use of appropriations of an \n",
      "administrative nature  \n",
      "– X The proposal/initiative requires the use of appropriations of an administrative \n",
      "nature, as explained below:  \n",
      "EUR million (to three decimal places)  \n",
      " Year \n",
      "2022  Year \n",
      "2023  Year \n",
      "2024  Year \n",
      "2025  Year \n",
      "2026  Year \n",
      "2027  Yearly after  \n",
      "202775 TOTAL  \n",
      " \n",
      "HEADING 7  \n",
      "of the multiannual \n",
      "financial framework          \n",
      "Human resources   1.520  1.520  1.520  1.520  1.520  1.520  7.600  \n",
      "Other administrative \n",
      "expenditure   0.010  0.010  0.010  0.010  0.010  0.010  0.050  \n",
      "Subtotal HEADING 7  \n",
      "of the multiannual \n",
      "financial framework   1.530  1.530  1.530  1.530  1.530  1.530  7.650  \n",
      " \n",
      "Outside HEADING 776 \n",
      "of the multiannual \n",
      "financial framework  \n",
      "         \n",
      "Human resources          \n",
      "Other expenditure  \n",
      "of an administrative \n",
      "nature   0.240  0.240  0.240  0.240  0.240  0.240  1.20 \n",
      "Subtotal  \n",
      "outside HEADING 7  \n",
      "of the multiannual \n",
      "financial framework   0.240  0.240  0.240  0.240  0.240  0.240  1.20 \n",
      " \n",
      "TOTAL   1.770  1.770  1.770  1.770  1.770  1.770  8.850  \n",
      "The appropriations required for human resources and other expenditure of an administrative nature will be met by \n",
      "appropriations from the DG that are already assigned to management of the action and/or have been redeployed within the \n",
      "DG, together if necessary with any additional alloc ation which may be granted to the managing DG under the annual \n",
      "allocation procedure and in the light of budgetary constraints.  \n",
      "                                                 \n",
      "75 All figures in this column are indicative and subject to the continuation of the programmes and availability of \n",
      "appropriations.  \n",
      "76 Technical and/or administrative assistance and expenditure in support of  the implementation of \n",
      "EU programmes and/or actions (former ‘BA’ lines), indirect research, direct research.   \n",
      "EN 104  EN 3.2.3.1.  Estimated requirements of human resources  \n",
      "–  The proposal/initiative does not require the use of human resources.  \n",
      "– X The proposal/initiative requires the use of human resources, as explained \n",
      "below:  \n",
      "Estimate to be expressed in full time equivalent units  \n",
      " \n",
      ".  Year \n",
      "2023  Year \n",
      "2024  Year \n",
      "2025  2026  2027  After \n",
      "202777  \n",
      " Establishment plan posts (officials and temporary staff)  \n",
      "20 01 02 01  (Headquarters and Commission’s Representation \n",
      "Offices)  10 10 10 10 10 10  \n",
      "20 01 02 03 (Delegations)         \n",
      "01 01 01 01   (Indirect research)         \n",
      " 01 01 01 11 (Direct research)         \n",
      "Other budget lines (specify)         \n",
      " External staff (in Full Time Equivalent unit: FTE)78 \n",
      " \n",
      "20 02 01  (AC, END, INT from the ‘global envelope’)         \n",
      "20 02 03 (AC, AL, END, INT and JPD in the delegations)         \n",
      "XX 01  xx yy zz  79 \n",
      " - at Headquarters  \n",
      "        \n",
      "- in Delegations         \n",
      "01 01 01 02  (AC, END, INT - Indirect research)         \n",
      " 01 01 01 12 (AC, END, INT - Direct research)         \n",
      "Other budget lines (specify)         \n",
      "TOTAL   10 10 10 10 10 10  \n",
      "XX is the policy area or budget title concerned.  \n",
      "The human resources required will be met by staff from the DG who are already assigned to management of the \n",
      "action and/or have been redeployed within the DG, together if necessary with any additional allocatio n which \n",
      "may be granted to the managing DG under the annual allocation procedure and in the light of budgetary \n",
      "constraints.  \n",
      "EDPS is expected to provide half of the resources required.  \n",
      " \n",
      "Description of tasks to be carried out:  \n",
      "Officials and temporary staff  To prepare a total of 13 -16 meetings, draft reports, continue policy work, e.g. \n",
      "regarding future amendments of the list of high -risk AI applications, and maintain \n",
      "relations with Member States’ authorities will require four AD FTE and 1 AST FTE.  \n",
      "For AI system s developed by the EU institutions, the European Data Protection \n",
      "Supervisor is responsible. Based on past experience, it can be estimated that 5 AD FTE \n",
      "are reuqired to fulfill the EDPS responsibilites under the draft legislation.  \n",
      "                                                 \n",
      "77  All figures in this column are indicative and subject to the continuation of the programmes and \n",
      "availability of appropriations.  \n",
      "78 AC = Contract Staff; AL = Local Staff; END = Seconded National Expert; INT = agency staff; JPD \n",
      "= Junior Professionals in Delegations.  \n",
      "79 Sub-ceiling for external staff covered by operational appropriations (former ‘BA’ lines).   \n",
      "EN 105  EN External staff    \n",
      "EN 106  EN 3.2.4.  Compatibility with the current multiannual financial framework  \n",
      "The proposal/initiative:  \n",
      "– X can be fully financed through redeployment within the relevant heading of the \n",
      "Multiannual Financial Framework (MFF).  \n",
      "No reporgramming is needed.   \n",
      "–  requires use of the unallocated margin under the relevant heading of the MFF \n",
      "and/or use of the special instruments as defined in the MFF Regulation.  \n",
      "Explain what is required, specifying the headings and budget lines concerned, the corresponding \n",
      "amounts, and the instruments proposed to be used.   \n",
      "–  requires a revision of the MFF.  \n",
      "Explain what is required, specifying the headings and budget lines concerned and the corresponding \n",
      "amounts.  \n",
      "3.2.5.  Third -party contributions  \n",
      "The proposal/in itiative:  \n",
      "– X does not provide for co -financing by third parties  \n",
      "–  provides for the co -financing by third parties estimated below:  \n",
      "Appropriations in EUR million (to three decimal places)  \n",
      " Year \n",
      "N80 Year \n",
      "N+1 Year \n",
      "N+2 Year \n",
      "N+3 Enter as many years as necessary \n",
      "to show the duration of the \n",
      "impact (see point 1.6)  Total  \n",
      "Specify the co -financing \n",
      "body           \n",
      "TOTAL appropriations \n",
      "co-financed          \n",
      " \n",
      " \n",
      "                                                 \n",
      "80 Year N is the year in which implementatio n of the proposal/initiative starts. Please replace \"N\" by the \n",
      "expected first year of implementation (for instance: 2021). The same for the following years.   \n",
      "EN 107  EN 3.3. Estimated impact on revenue  \n",
      "–  The proposal/initiative has the following financial impact:  \n",
      "–  The proposal/initiative has the following financial impact:  \n",
      "–  on other revenue  \n",
      "–  on other revenue  \n",
      "– Please indicate, if the revenue is assigned to expenditure lines  \n",
      "EUR million (to three decimal places)  \n",
      "Budget revenue line:  Appropriation\n",
      "s available for \n",
      "the current \n",
      "financial year  Impact of the proposal/initiative81 \n",
      "Year \n",
      "N Year \n",
      "N+1 Year \n",
      "N+2 Year \n",
      "N+3 Enter as many years as necessary to show \n",
      "the duration of the impact (see point 1.6)  \n",
      "Article ………….          \n",
      "For assigned revenue, specify the budget expenditure line(s) affected.  \n",
      "  \n",
      "Other remarks (e.g. method/formula used for calculating the impact on revenue or any other \n",
      "information).  \n",
      " \n",
      "                                                 \n",
      "81 As regards traditional own resources (customs duties, sugar levies), the amounts indicated must be net \n",
      "amounts, i.e. gross amounts after deduction of 20  % for collection costs.  \n",
      "Table chunks: [[[None, 'anonymisation or'], ['pseudonymisation of judicial decisions, documents or data, communication between', None], ['personnel,', None]], [['Where a high-risk AI system related to products to which the legal acts listed in Annex II,'], ['section A, apply, is placed on the market or put into service together with the product'], ['manufactured in accordance with those legal acts and under the name of the product']], [['manufacturer, the manufacturer of the product shall take the responsibility of the compliance'], ['of the AI system with this Regulation and, as far as the AI system is concerned, have the same'], ['obligations imposed by the present Regulation on the provider.']], [[None, 'or it is necessary for the'], ['exercise of the right to freedom of expression and the right to freedom of the arts and', None], ['sciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to', None]], [['appropriate safeguards for the rights and freedoms of third parties.', None]], [['Heading of\\nmultiannual\\nfinancial\\nframework', 'Budget line', 'Type of\\nexpenditure', 'Contribution', None, None, None], [None, 'Number', 'Diff./Non-\\n66\\ndiff.', 'from\\nEFTA\\ncountries\\n67', 'from\\ncandidate\\n68\\ncountries', 'from third\\ncountries', 'within the\\nmeaning of\\nArticle 21(2)(b) of\\nthe Financial\\nRegulation'], ['7', '20 02 06 Administrative expenditure', 'Non-diff.', 'NO', 'NO', 'NO', 'NO']], [['1', '02 04 03 DEP Artificial Intelligence', 'Diff.', 'YES', 'NO', 'NO', 'NO'], ['1', '02 01 30 01 Support expenditure for\\nthe Digital Europe programme', 'Non-diff.', 'YES', 'NO', 'NO', 'NO']], [['DG: CNECT', '', '', '', 'Year\\n2022', 'Year\\n2023', 'Year\\n2024', 'Year\\n2025', 'Year\\n2026', 'Year\\n69\\n2027', 'TOTAL', None], ['\\uf09f Operational appropriations', None, None, None, '', '', '', '', '', '', '', ''], ['70\\nBudget line 02 04 03', None, 'Commitments', '(1a)', '', '1.000', '', '', '', '', '', '1.000']], [[None, None, 'Payments', '(2a)', '', '0.600', '0.100', '0.100', '0.100', '0.100', '', '1.000'], ['Budget line', None, 'Commitments', '(1b)', '', '', '', '', '', '', '', ''], [None, None, 'Payments', '(2b)', '', '', '', '', '', '', '', '']], [['Appropriations of an administrative nature financed from the envelope of specific\\n71\\nprogrammes', None, None, None, '', '', '', '', '', '', '', ''], ['Budget line 02 01 30 01', None, '', '(3)', '', '0.240', '0.240', '0.240', '0.240', '0.240', '', '1.200'], ['TOTAL appropriations\\nfor DG CNECT', None, 'Commitments', '=1a+1b +3', '', '1.240', '', '0.240', '0.240', '0.240', '', '2.200']], [['', None, 'Payments', '=2a+2b\\n+3', '', '0.840', '0.340', '0.340', '0.340', '0.340', '', '2.200']], [['\\uf09f TOTAL operational appropriations (all\\noperational headings)', None, None, 'Commitments', '(4)', '', '', '', '', '', '', '', ''], [None, None, None, 'Payments', '(5)', '', '', '', '', '', '', '', ''], ['\\uf09f TOTAL appropriations of an administrative nature\\nfinanced from the envelope for specific programmes (all\\noperational headings)', None, None, None, '(6)', '', '', '', '', '', '', '', '']], [['', 'TOTAL appropriations', '', 'Commitments', '=4+ 6', '', '', '', '', '', '', '', ''], [None, 'under HEADINGS 1 to 6', None, None, None, None, None, None, None, None, None, None, None], [None, None, None, 'Payments', '=5+ 6', '', '', '', '', '', '', '', '']], [[None, 'of the multiannual financial framework', None, None, None, None, None, None, None, None, None, None, None], [None, '(Reference amount)', None, None, None, None, None, None, None, None, None, None, None]], [['\\uf09f Human resources', None, None, None, None, '0.760', '0.760', '0.760', '0.760', '0.760', '0.760'], ['\\uf09f Other administrative expenditure', None, None, None, None, '0.010', '0.010', '0.010', '0.010', '0.010', '0.010'], ['TOTAL DG CNECT', None, None, None, 'Appropriations', '0.760', '0.760', '0.760', '0.760', '0.760', '0.760']], [['European Data Protection Supervisor', None, None, None, None, None, None, None, None, None, None], ['\\uf09f Human resources', None, None, None, None, '0.760', '0.760', '0.760', '0.760', '0.760', '0.760'], ['\\uf09f Other administrative expenditure', None, None, None, None, '', '', '', '', '', '']], [['TOTAL EDPS', None, None, None, 'Appropriations', '0.760', '0.760', '0.760', '0.760', '0.760', '0.760'], ['', 'TOTAL appropriations', '', '(Total commitments = Total payments)', None, '1.530', '1.530', '1.530', '1.530', '1.530', '1.530'], [None, 'under HEADING 7', None, None, None, None, None, None, None, None, None]], [[None, 'of the multiannual financial framework', None, None, None, None, None, None, None, None, None]], [['', 'under HEADINGS 1 to 7', '', 'Payments', '', '2.370', '1.870', '1.870', '1.870', '1.870'], [None, 'of the multiannual financial framework', None, None, None, None, None, None, None, None]], [['Indicate objectives\\nand outputs\\n\\uf0f2', '', '', 'Year\\n2022', None, 'Year\\n2023', None, 'Year\\n2024', None, 'Year\\n2025', None, None, None, 'Year\\n2026', None, 'Year\\n2027', None, 'After\\n73\\n2027', None, 'TOTAL', None], ['', 'OUTPUTS', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], ['', 'Type', 'Average\\ncost', 'o\\nN', 'Cost', 'o\\nN', 'Cost', 'o\\nN', 'Cost', '', 'o\\nN', '', 'Cost', 'o\\nN', 'Cost', 'o\\nN', 'Cost', 'o\\nN', 'Cost', 'Tota\\nl No', 'Total\\ncost']], [['74\\nSPECIFIC OBJECTIVE No 1 …', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], ['Database', '', '', '', None, '1 1.000', None, '1', None, '1', None, None, None, '1', None, '1', None, '1 0.100', None, '1', '1.000'], ['Meetings- Output', '', '', '', None, '10 0.200', None, '10 0.200', None, '10 0.200', None, None, None, '10 0.200', None, '10 0.200', None, '10 0.200', None, '50', '1.000']], [['Communication\\nactivities', '', '', '', '', '2', '0.040', '2', '0.040', '2', None, None, '0.040', '2', '0.040', '2', '0.040', '2', '0.040', '10', '0.040'], ['Subtotal for specific objective No 1', None, None, '', '', '', '', '', '', '', None, None, '', '', '', '', '', '', '', '', ''], ['SPECIFIC OBJECTIVE No 2 ...', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]], [['- Output', '', '', '', '', '', '', '', '', '', None, None, '', '', '', '', '', '', '', '', ''], ['Subtotal for specific objective No 2', None, None, '', '', '', '', '', '', '', None, None, '', '', '', '', '', '', '', '', ''], ['TOTALS', None, None, '', '', '13', '0.240', '13', '0.240', '13', None, None, '0.240', '13', '0.240', '13', '0.240', '13', '0.100', '65', '2.200']], [['', '76\\nOutside HEADING 7', '', '', '', '', '', '', '', '', ''], [None, 'of the multiannual', None, None, None, None, None, None, None, None, None], [None, 'financial framework', None, None, None, None, None, None, None, None, None]], [['Human resources', None, None, '', '', '', '', '', '', '', ''], ['Other expenditure\\nof an administrative\\nnature', None, None, '', '0.240', '0.240', '0.240', '0.240', '0.240', '0.240', '1.20'], ['', 'Subtotal', '', '', '0.240', '0.240', '0.240', '0.240', '0.240', '0.240', '1.20']], [[None, 'outside HEADING 7', None, None, None, None, None, None, None, None, None], [None, 'of the multiannual', None, None, None, None, None, None, None, None, None], [None, 'financial framework', None, None, None, None, None, None, None, None, None]], [['.', None, '', 'Year\\n2023', 'Year\\n2024', 'Year\\n2025', '2026', '2027', 'After\\n202777', ''], ['\\uf09f Establishment plan posts (officials and temporary staff)', None, None, None, None, None, None, None, None, None], ['20 01 02 01 (Headquarters and Commission’s Representation\\nOffices)', None, None, '10', '10', '10', '10', '10', '10', '']], [['20 01 02 03 (Delegations)', None, None, '', '', '', '', '', '', ''], ['01 01 01 01 (Indirect research)', None, None, '', '', '', '', '', '', ''], ['01 01 01 11 (Direct research)', None, None, '', '', '', '', '', '', '']], [['Other budget lines (specify)', None, None, '', '', '', '', '', '', ''], ['78\\n\\uf09f External staff (in Full Time Equivalent unit: FTE)', None, None, None, None, None, None, None, None, None], ['20 02 01 (AC, END, INT from the ‘global envelope’)', None, None, '', '', '', '', '', '', '']], [['20 02 03 (AC, AL, END, INT and JPD in the delegations)', None, None, '', '', '', '', '', '', ''], ['79\\nXX 01 xx yy zz', '- at Headquarters', None, '', '', '', '', '', '', ''], [None, '- in Delegations', None, '', '', '', '', '', '', '']], [['01 01 01 02 (AC, END, INT - Indirect research)', None, None, '', '', '', '', '', '', ''], ['01 01 01 12 (AC, END, INT - Direct research)', None, None, '', '', '', '', '', '', ''], ['Other budget lines (specify)', None, None, '', '', '', '', '', '', '']], [['TOTAL', None, '', '10', '10', '10', '10', '10', '10', '']], [['External staff', '']], [['', 'Year\\n80\\nN', 'Year\\nN+1', 'Year\\nN+2', 'Year\\nN+3', 'Enter as many years as necessary\\nto show the duration of the\\nimpact (see point 1.6)', None, None, 'Total'], ['Specify the co-financing\\nbody', '', '', '', '', '', '', '', ''], ['TOTAL appropriations\\nco-financed', '', '', '', '', '', '', '', '']], [['Budget revenue line:', 'Appropriation\\ns available for\\nthe current\\nfinancial year', '81\\nImpact of the proposal/initiative', None, None, None, None, None, None], [None, None, 'Year\\nN', 'Year\\nN+1', 'Year\\nN+2', 'Year\\nN+3', 'Enter as many years as necessary to show\\nthe duration of the impact (see point 1.6)', None, None], ['Article ………….', '', '', '', '', '', '', '', '']]]\n"
     ]
    }
   ],
   "source": [
    "# Call chunk_text_and_save\n",
    "text_chunks = chunk_text_and_save(text, 'chunked_text.txt', chunk_size=500)\n",
    "print(\"Text chunks:\", text_chunks)\n",
    "\n",
    "# Call chunk_table_rows_and_save_all\n",
    "table_chunks = chunk_table_rows_and_save_all(tables, 'chunked_tables.csv', chunk_size=3)\n",
    "print(\"Table chunks:\", table_chunks)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T10:13:58.763804800Z",
     "start_time": "2024-05-17T10:13:58.604910100Z"
    }
   },
   "id": "dda6ff99876edf0f"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[52], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BertTokenizer, BertModel\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msqlite3\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Bureau\\stage_code\\.venv\\lib\\site-packages\\transformers\\__init__.py:26\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dependency_versions_check\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     28\u001B[0m     OptionalDependencyNotAvailable,\n\u001B[0;32m     29\u001B[0m     _LazyModule,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     48\u001B[0m     logging,\n\u001B[0;32m     49\u001B[0m )\n\u001B[0;32m     52\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mget_logger(\u001B[38;5;18m__name__\u001B[39m)  \u001B[38;5;66;03m# pylint: disable=invalid-name\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Bureau\\stage_code\\.venv\\lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdependency_versions_table\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deps\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m require_version, require_version_core\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# define which module versions we always want to check at run time\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# order specific notes:\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# - tqdm must be checked before tokenizers\u001B[39;00m\n\u001B[0;32m     25\u001B[0m pkgs_to_check_at_runtime \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpython\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtqdm\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpyyaml\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     38\u001B[0m ]\n",
      "File \u001B[1;32m~\\OneDrive\\Bureau\\stage_code\\.venv\\lib\\site-packages\\transformers\\utils\\__init__.py:33\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconstants\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdoc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     26\u001B[0m     add_code_sample_docstrings,\n\u001B[0;32m     27\u001B[0m     add_end_docstrings,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     31\u001B[0m     replace_return_docstrings,\n\u001B[0;32m     32\u001B[0m )\n\u001B[1;32m---> 33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgeneric\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     34\u001B[0m     ContextManagers,\n\u001B[0;32m     35\u001B[0m     ExplicitEnum,\n\u001B[0;32m     36\u001B[0m     ModelOutput,\n\u001B[0;32m     37\u001B[0m     PaddingStrategy,\n\u001B[0;32m     38\u001B[0m     TensorType,\n\u001B[0;32m     39\u001B[0m     add_model_info_to_auto_map,\n\u001B[0;32m     40\u001B[0m     cached_property,\n\u001B[0;32m     41\u001B[0m     can_return_loss,\n\u001B[0;32m     42\u001B[0m     expand_dims,\n\u001B[0;32m     43\u001B[0m     find_labels,\n\u001B[0;32m     44\u001B[0m     flatten_dict,\n\u001B[0;32m     45\u001B[0m     infer_framework,\n\u001B[0;32m     46\u001B[0m     is_jax_tensor,\n\u001B[0;32m     47\u001B[0m     is_numpy_array,\n\u001B[0;32m     48\u001B[0m     is_tensor,\n\u001B[0;32m     49\u001B[0m     is_tf_symbolic_tensor,\n\u001B[0;32m     50\u001B[0m     is_tf_tensor,\n\u001B[0;32m     51\u001B[0m     is_torch_device,\n\u001B[0;32m     52\u001B[0m     is_torch_dtype,\n\u001B[0;32m     53\u001B[0m     is_torch_tensor,\n\u001B[0;32m     54\u001B[0m     reshape,\n\u001B[0;32m     55\u001B[0m     squeeze,\n\u001B[0;32m     56\u001B[0m     strtobool,\n\u001B[0;32m     57\u001B[0m     tensor_size,\n\u001B[0;32m     58\u001B[0m     to_numpy,\n\u001B[0;32m     59\u001B[0m     to_py_obj,\n\u001B[0;32m     60\u001B[0m     transpose,\n\u001B[0;32m     61\u001B[0m     working_or_temp_dir,\n\u001B[0;32m     62\u001B[0m )\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhub\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     64\u001B[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001B[0;32m     65\u001B[0m     HF_MODULES_CACHE,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     91\u001B[0m     try_to_load_from_cache,\n\u001B[0;32m     92\u001B[0m )\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimport_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     94\u001B[0m     ACCELERATE_MIN_VERSION,\n\u001B[0;32m     95\u001B[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    207\u001B[0m     torch_only_method,\n\u001B[0;32m    208\u001B[0m )\n",
      "File \u001B[1;32m~\\OneDrive\\Bureau\\stage_code\\.venv\\lib\\site-packages\\transformers\\utils\\generic.py:465\u001B[0m\n\u001B[0;32m    461\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;28mself\u001B[39m[k] \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkeys())\n\u001B[0;32m    464\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torch_available():\n\u001B[1;32m--> 465\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_pytree\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_torch_pytree\u001B[39;00m\n\u001B[0;32m    467\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_model_output_flatten\u001B[39m(output: ModelOutput) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[List[Any], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_torch_pytree.Context\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    468\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output\u001B[38;5;241m.\u001B[39mvalues()), \u001B[38;5;28mlist\u001B[39m(output\u001B[38;5;241m.\u001B[39mkeys())\n",
      "File \u001B[1;32m~\\OneDrive\\Bureau\\stage_code\\.venv\\lib\\site-packages\\torch\\__init__.py:560\u001B[0m\n\u001B[0;32m    546\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(textwrap\u001B[38;5;241m.\u001B[39mdedent(\u001B[38;5;124m'''\u001B[39m\n\u001B[0;32m    547\u001B[0m \u001B[38;5;124m            Failed to load PyTorch C extensions:\u001B[39m\n\u001B[0;32m    548\u001B[0m \u001B[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    556\u001B[0m \u001B[38;5;124m                or by running Python from a different directory.\u001B[39m\n\u001B[0;32m    557\u001B[0m \u001B[38;5;124m            \u001B[39m\u001B[38;5;124m'''\u001B[39m)\u001B[38;5;241m.\u001B[39mstrip()) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    558\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m  \u001B[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001B[39;00m\n\u001B[1;32m--> 560\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mdir\u001B[39m(\u001B[43m_C\u001B[49m):\n\u001B[0;32m    561\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m name\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBase\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m    562\u001B[0m         __all__\u001B[38;5;241m.\u001B[39mappend(name)\n",
      "\u001B[1;31mNameError\u001B[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "#embeding using BERT - First Try\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_word_embedding(word):\n",
    "    inputs = tokenizer(word, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    # Use the [CLS] token representation\n",
    "    embeddings = outputs.last_hidden_state[0][0].detach().numpy()\n",
    "    return embeddings\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T10:38:10.304121300Z",
     "start_time": "2024-05-17T10:38:04.158495Z"
    }
   },
   "id": "837ccd2dfea2ec1d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e54500e5e3178c80"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

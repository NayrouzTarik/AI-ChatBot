{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this code i'm aiming to process text from PDF documents & chunk it with an overlap, create embeddings for the words in the text, store these embeddings in a database, and then retrieve and use these embeddings to answer queries."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "961aad39bfbaaaa4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "MAIN STEPS :\n",
    "-Fetching from the pdf using Pdfminer\n",
    "-Lowercase, cleaning the text removing ponctuation\n",
    "-The cleaned text is divided into chunks that have specified size with overlap.\n",
    "-Saving into a file\n",
    "-Embed For each word and stored into a Database\n",
    "-Embeding reading back from the database to respond to queries\n",
    "-Clean Process & embed the query\n",
    "-The queries are entred by the user"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38a1da80bb0876c7"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (C:\\Users\\21264\\OneDrive\\Bureau\\stage_code\\.venv\\lib\\site-packages\\scipy\\linalg\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenize\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m word_tokenize\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m stopwords\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Word2Vec\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mstring\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msqlite3\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Bureau\\stage_code\\.venv\\lib\\site-packages\\gensim\\__init__.py:11\u001B[0m\n\u001B[0;32m      7\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m4.3.2\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001B[38;5;66;03m# noqa:F401\u001B[39;00m\n\u001B[0;32m     14\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgensim\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m logger\u001B[38;5;241m.\u001B[39mhandlers:  \u001B[38;5;66;03m# To ensure reload() doesn't add another one\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Bureau\\stage_code\\.venv\\lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mindexedcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m IndexedCorpus  \u001B[38;5;66;03m# noqa:F401 must appear before the other classes\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmmcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MmCorpus  \u001B[38;5;66;03m# noqa:F401\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbleicorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BleiCorpus  \u001B[38;5;66;03m# noqa:F401\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Bureau\\stage_code\\.venv\\lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m interfaces, utils\n\u001B[0;32m     16\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mIndexedCorpus\u001B[39;00m(interfaces\u001B[38;5;241m.\u001B[39mCorpusABC):\n",
      "File \u001B[1;32m~\\OneDrive\\Bureau\\stage_code\\.venv\\lib\\site-packages\\gensim\\interfaces.py:19\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \n\u001B[0;32m      9\u001B[0m \u001B[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     14\u001B[0m \n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils, matutils\n\u001B[0;32m     22\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mCorpusABC\u001B[39;00m(utils\u001B[38;5;241m.\u001B[39mSaveLoad):\n",
      "File \u001B[1;32m~\\OneDrive\\Bureau\\stage_code\\.venv\\lib\\site-packages\\gensim\\matutils.py:20\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparse\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstats\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m entropy\n\u001B[1;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_blas_funcs, triu\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlapack\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_lapack_funcs\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspecial\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m psi  \u001B[38;5;66;03m# gamma function utils\u001B[39;00m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'triu' from 'scipy.linalg' (C:\\Users\\21264\\OneDrive\\Bureau\\stage_code\\.venv\\lib\\site-packages\\scipy\\linalg\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "import sqlite3\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Download resources for tokenizing and stopword\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def fetch_text_from_pdf(pdf_path):\n",
    "    text = extract_text(pdf_path)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # Tokenize text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    # Define the stopword to be removed \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    # Remove stopwords and punctuation from tokens\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in punctuation]\n",
    "    # Return tokens instead of joining back into string for Word2Vec training\n",
    "    return tokens\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def save_chunks_to_file(chunks, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for chunk in chunks:\n",
    "            file.write(chunk + '\\n')\n",
    "\n",
    "def save_embeddings_to_database(embeddings, database_path):\n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    c = conn.cursor()\n",
    "    # Create a table for the embeddings if it doesn't exist\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS embeddings (word TEXT, embedding BLOB)''')\n",
    "    for word, embedding in embeddings.items():\n",
    "        # Convert the embedding to a bytes-like object\n",
    "        embedding_blob = np.array(embedding).tobytes()\n",
    "        # Insert the word and its embedding into the database\n",
    "        c.execute(\"INSERT INTO embeddings (word, embedding) VALUES (?, ?)\", (word, embedding_blob))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def generate_word2vec_embeddings(tokenized_text):\n",
    "    # Train Word2Vec model on tokenized text\n",
    "    model = Word2Vec(sentences=[tokenized_text], vector_size=100, window=5, min_count=1, workers=4)\n",
    "    embeddings = {word: model.wv[word] for word in model.wv.key_to_index}\n",
    "    return embeddings\n",
    "\n",
    "def process_pdfs():\n",
    "    books = [\n",
    "        {\"title\": \"PDF1\", \"pdf_path\": \"infos/pdf1.pdf\"},\n",
    "        {\"title\": \"PDF2\", \"pdf_path\": \"infos/pdf2.pdf\"},\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    all_tokens = []\n",
    "    for book in books:\n",
    "        pdf_path = book[\"pdf_path\"]\n",
    "        text = fetch_text_from_pdf(pdf_path)\n",
    "        tokens = preprocess_text(text)\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    # Use the list of all tokens for Word2Vec training\n",
    "    embeddings = generate_word2vec_embeddings(all_tokens)\n",
    "    save_embeddings_to_database(embeddings, \"embeddings.db\")\n",
    "    print(\"Embeddings saved to embeddings.db\")\n",
    "\n",
    "process_pdfs()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T22:39:13.246267100Z",
     "start_time": "2024-05-17T22:39:05.668881900Z"
    }
   },
   "id": "f05ef203a0f93301"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\21264\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\21264\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "no such table: embeddings",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOperationalError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 41\u001B[0m\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m embeddings\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# Load embeddings from database\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mread_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# Function to calculate embedding for the query\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_embedding_for_query\u001B[39m(query, embeddings):\n",
      "Cell \u001B[1;32mIn[7], line 36\u001B[0m, in \u001B[0;36mread_embeddings\u001B[1;34m()\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_embeddings\u001B[39m():\n\u001B[1;32m---> 36\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mload_embeddings_from_database\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43membeddings.db\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of embeddings loaded:\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mlen\u001B[39m(embeddings))\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m embeddings\n",
      "Cell \u001B[1;32mIn[7], line 24\u001B[0m, in \u001B[0;36mload_embeddings_from_database\u001B[1;34m(database_path)\u001B[0m\n\u001B[0;32m     22\u001B[0m conn \u001B[38;5;241m=\u001B[39m sqlite3\u001B[38;5;241m.\u001B[39mconnect(database_path)\n\u001B[0;32m     23\u001B[0m c \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mcursor()\n\u001B[1;32m---> 24\u001B[0m \u001B[43mc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mSELECT word, embedding FROM embeddings\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m c\u001B[38;5;241m.\u001B[39mfetchall():\n",
      "\u001B[1;31mOperationalError\u001B[0m: no such table: embeddings"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to preprocess the query\n",
    "def preprocess_query(query):\n",
    "    query = query.lower()\n",
    "    tokens = word_tokenize(query)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in punctuation]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def load_embeddings_from_database(database_path):\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT word, embedding FROM embeddings\")\n",
    "    embeddings = {}\n",
    "    for row in c.fetchall():\n",
    "        word, embedding_bytes = row\n",
    "        if embedding_bytes is not None:\n",
    "            embedding = np.frombuffer(embedding_bytes, dtype=np.float64)  \n",
    "            embeddings[word] = embedding\n",
    "    conn.close()\n",
    "    return embeddings\n",
    "\n",
    "# Function to read embeddings\n",
    "def read_embeddings():\n",
    "    embeddings = load_embeddings_from_database(\"embeddings.db\")\n",
    "    print(\"Number of embeddings loaded:\", len(embeddings))\n",
    "    return embeddings\n",
    "\n",
    "# Load embeddings from database\n",
    "embeddings = read_embeddings()\n",
    "\n",
    "# Function to calculate embedding for the query\n",
    "def get_embedding_for_query(query, embeddings):\n",
    "    processed_query = preprocess_query(query)\n",
    "    query_words = processed_query.split()\n",
    "    query_embedding = np.zeros(next(iter(embeddings.values())).shape)\n",
    "    for word in query_words:\n",
    "        word_embedding = embeddings.get(word)\n",
    "        if word_embedding is not None:\n",
    "            query_embedding += word_embedding\n",
    "    return query_embedding\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    query = input(\"Enter your query: \")\n",
    "    query_embedding = get_embedding_for_query(query, embeddings)\n",
    "    if np.any(query_embedding):\n",
    "        print(\"Embedding for query:\", query_embedding)\n",
    "        print(\"The answer is:\")\n",
    "        print(\"The query was:\", query)\n",
    "        print(\"The query embedding was:\", query_embedding)\n",
    "    else:\n",
    "        print(\"Embedding not found for the query.\")\n",
    "\n",
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T22:46:32.937944500Z",
     "start_time": "2024-05-17T22:46:31.369346700Z"
    }
   },
   "id": "4e4b02acd4c97408"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
